{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp39-cp39-win_amd64.whl (17.9 MB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from datasets) (2021.10.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from datasets) (1.3.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mayte\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: dill, xxhash, responses, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed datasets-2.2.1 dill-0.3.4 huggingface-hub-0.6.0 multiprocess-0.70.12.2 pyarrow-8.0.0 responses-0.18.0 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting demoji\n",
      "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: demoji\n",
      "Successfully installed demoji-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayte\\AppData\\Local\\Temp/ipykernel_3920/753331155.py:19: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from xmlrpc.client import Boolean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.utils import resample\n",
    "import demoji\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "demoji.download_codes()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def setup(rem_stop=True, do_stem=True, do_lem=False, split=True, split_on='preprocessed', upsample=True, do_emojis=True):\n",
    "    df = load_data();\n",
    "    df['preprocessed'] = preprocess(\n",
    "        df['tweet'], rem_stop=rem_stop, do_stem=do_stem, do_lem=do_lem, do_emojis=do_emojis)\n",
    "\n",
    "    tfidf = train_tfidf(df['preprocessed'])\n",
    "\n",
    "    if split is True:\n",
    "        df_train, df_test = split_data(df, split_on)\n",
    "        if upsample is True:\n",
    "            df_train = upsampling(df_train)\n",
    "        return tfidf, df_train, df_test\n",
    "    else:\n",
    "        return tfidf, df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    dataset = load_dataset(\"tweets_hate_speech_detection\")\n",
    "    df = pd.DataFrame.from_dict(dataset['train'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(data, rem_stop=True, do_stem=True, do_lem=False, do_emojis=True):\n",
    "\n",
    "    preprocessed = []\n",
    "    for tweet in data:\n",
    "        if do_emojis is True:\n",
    "            tweet = convert_emoji(tweet)\n",
    "        tokens = tokenization(remove_punctuation(tweet))\n",
    "        if rem_stop is True:\n",
    "            tokens = remove_stopwords(tokens)\n",
    "        if do_stem is True and do_lem is False:\n",
    "            tokens = stemming(tokens)\n",
    "        if do_lem is True and do_stem is False:\n",
    "            tokens = lemmatization(tokens)\n",
    "        preprocessed.append(np.array(tokens))\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "def train_tfidf(data):\n",
    "    def dummy(text):\n",
    "        return text\n",
    "\n",
    "    tf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        token_pattern=None)\n",
    "\n",
    "    return tf.fit(data)\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame, split_on='tweet', test_size=0.2, random_state=17):\n",
    "    y = df['label']\n",
    "    X = df[split_on]\n",
    "    (X_train, X_test, y_train, y_test) = ms.train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    df_train = pd.concat([y_train, X_train], axis=1)\n",
    "    df_test = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def upsampling(df: pd.DataFrame, replace=True, n_samples=23775, random_state=55):\n",
    "    data_minority = df[df.label == 1]\n",
    "    data_majority = df[df.label == 0]\n",
    "    data_minority = resample(\n",
    "        data_minority, replace=replace, n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "    return pd.concat([data_majority, data_minority])\n",
    "\n",
    "\n",
    "def tokenization(text: str):\n",
    "    return pd.Series(nltk.word_tokenize(text.lower()))\n",
    "\n",
    "\n",
    "def remove_punctuation(tokens: pd.Series):\n",
    "    return \"\".join([i for i in tokens if i not in punctuation])\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: pd.Series):\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    return tokens.apply(lambda token: token if token not in stopwords_list and token != '' else None).dropna()\n",
    "\n",
    "\n",
    "def stemming(tokens: pd.Series):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return tokens.apply(lambda token: stemmer.stem(token))\n",
    "\n",
    "\n",
    "def lemmatization(tokens: pd.Series):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return tokens.apply(lambda token: lemmatizer.lemmatize(token))\n",
    "\n",
    "\n",
    "def convert_emoji(text: str) -> str:\n",
    "    # convert string to binary representation\n",
    "    binary = ' '.join(format(ord(x), 'b') for x in text)\n",
    "\n",
    "    # convert binary representation to utf8 representation\n",
    "    listRes = list(binary.split(\" \"))\n",
    "    try:\n",
    "        text_with_emoji = bytes([int(x, 2) for x in listRes]).decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return text\n",
    "\n",
    "    # get all emojis\n",
    "    dictionary = demoji.findall(text_with_emoji)\n",
    "\n",
    "    # replace emojis with text representation\n",
    "    for key in dictionary.keys():\n",
    "        text_with_emoji = text_with_emoji.replace(key, dictionary[key] + \" \")\n",
    "\n",
    "    return text_with_emoji\n",
    "\n",
    "\n",
    "def get_features(df: pd.DataFrame):\n",
    "    df[\"n_mentions\"] = df[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
    "    df[\"hashtags\"] = df[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "def count_user_mentions(text:str) ->int:\n",
    "    return text.count(\"@user\")\n",
    "\n",
    "def identify_hashtags(text:str) -> list:\n",
    "    pattern = re.compile(r\"#(\\w+)\")\n",
    "    return pattern.findall(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayte\\Documents\\Uni\\Data Mining\\Group Project\\src\\data\\preprocessing.py:18: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname((os.path.abspath(''))))\n",
    "from src.data.preprocessing import load_data, preprocess, train_tfidf, split_data, upsampling, get_features, setup\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>tfidf_stemmed_tokens</th>\n",
       "      <th>tfidf_stemmed_hashtags</th>\n",
       "      <th>tfidf_lemmatized_tokens</th>\n",
       "      <th>tfidf_lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24090</td>\n",
       "      <td>0</td>\n",
       "      <td>best #lawofattraction #resources for #healing!...</td>\n",
       "      <td>0</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>['lawofattraction', 'for', 'altwaystoheal', 'is']</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>['lawofattract', 'altwaystoh']</td>\n",
       "      <td>['lawofattract', 'resourc', 'heal', 'altwaysto...</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resource', 'healing', 'al...</td>\n",
       "      <td>[-54.797821044921875, 19.326093673706055]</td>\n",
       "      <td>[50.279502868652344, -0.368119478225708]</td>\n",
       "      <td>[30.39127540588379, -49.107627868652344]</td>\n",
       "      <td>[8.619098663330078, -19.02434539794922]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15264</td>\n",
       "      <td>0</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>0</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['to', 'on', 'simplest', 'moments', 'life', 'b...</td>\n",
       "      <td>['simplest', 'moments', 'life', 'blogger', 'li...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>[12.769611358642578, 38.40631866455078]</td>\n",
       "      <td>[-0.20247356593608856, -12.449955940246582]</td>\n",
       "      <td>[-25.304161071777344, -11.912176132202148]</td>\n",
       "      <td>[-5.459761619567871, -11.88219928741455]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19310</td>\n",
       "      <td>0</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['you', 'as', 'as', 'boyfriend', 'be', 'with',...</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>[15.18979263305664, -9.596672058105469]</td>\n",
       "      <td>[10.67345905303955, -4.716570854187012]</td>\n",
       "      <td>[80.47900390625, 3.222534656524658]</td>\n",
       "      <td>[-2.534292459487915, -6.132740020751953]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27244</td>\n",
       "      <td>0</td>\n",
       "      <td>why do you always try to make me happy?  i don...</td>\n",
       "      <td>0</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>['do', 'always', 'to', 'me', 'i', 'know', 'to'...</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>['alway', 'know', 'love']</td>\n",
       "      <td>['love', 'devot']</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>[-46.13848876953125, -18.032955169677734]</td>\n",
       "      <td>[11.31847858428955, -16.98657989501953]</td>\n",
       "      <td>[40.34811019897461, -24.527305603027344]</td>\n",
       "      <td>[8.227179527282715, -13.818502426147461]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6633</td>\n",
       "      <td>0</td>\n",
       "      <td>omg is finally here!!! #ps4 #farcry4 #gtav #un...</td>\n",
       "      <td>0</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>['is', 'here', 'farcry4', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>[3.866750478744507, -5.09706449508667]</td>\n",
       "      <td>[21.51004409790039, 15.360187530517578]</td>\n",
       "      <td>[8.554491996765137, -20.749971389770508]</td>\n",
       "      <td>[-6.128843307495117, -11.832077980041504]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0  24090      0  best #lawofattraction #resources for #healing!...   \n",
       "1  15264      0  remembering to focus on the simplest happy mom...   \n",
       "2  19310      0  when you get as happy as your boyfriend to be ...   \n",
       "3  27244      0  why do you always try to make me happy?  i don...   \n",
       "4   6633      0  omg is finally here!!! #ps4 #farcry4 #gtav #un...   \n",
       "\n",
       "   n_mentions                                           hashtags  \\\n",
       "0           0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1           0                        ['blogger', 'blog', 'life']   \n",
       "2           0                                         ['silvia']   \n",
       "3           0                               ['love', 'devotion']   \n",
       "4           0             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  ['lawofattraction', 'for', 'altwaystoheal', 'is']   \n",
       "1  ['to', 'on', 'simplest', 'moments', 'life', 'b...   \n",
       "2  ['you', 'as', 'as', 'boyfriend', 'be', 'with',...   \n",
       "3  ['do', 'always', 'to', 'me', 'i', 'know', 'to'...   \n",
       "4              ['is', 'here', 'farcry4', 'unchaed4']   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moments', 'life', 'blogger', 'li...   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0                     ['lawofattract', 'altwaystoh']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                          ['alway', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  ['lawofattract', 'resourc', 'heal', 'altwaysto...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                                  ['love', 'devot']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                 lemmatized_hashtags  \\\n",
       "0  ['lawofattraction', 'resource', 'healing', 'al...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                        tfidf_stemmed_tokens  \\\n",
       "0  [-54.797821044921875, 19.326093673706055]   \n",
       "1    [12.769611358642578, 38.40631866455078]   \n",
       "2    [15.18979263305664, -9.596672058105469]   \n",
       "3  [-46.13848876953125, -18.032955169677734]   \n",
       "4     [3.866750478744507, -5.09706449508667]   \n",
       "\n",
       "                        tfidf_stemmed_hashtags  \\\n",
       "0     [50.279502868652344, -0.368119478225708]   \n",
       "1  [-0.20247356593608856, -12.449955940246582]   \n",
       "2      [10.67345905303955, -4.716570854187012]   \n",
       "3      [11.31847858428955, -16.98657989501953]   \n",
       "4      [21.51004409790039, 15.360187530517578]   \n",
       "\n",
       "                      tfidf_lemmatized_tokens  \\\n",
       "0    [30.39127540588379, -49.107627868652344]   \n",
       "1  [-25.304161071777344, -11.912176132202148]   \n",
       "2         [80.47900390625, 3.222534656524658]   \n",
       "3    [40.34811019897461, -24.527305603027344]   \n",
       "4    [8.554491996765137, -20.749971389770508]   \n",
       "\n",
       "                   tfidf_lemmatized_hashtags  \n",
       "0    [8.619098663330078, -19.02434539794922]  \n",
       "1   [-5.459761619567871, -11.88219928741455]  \n",
       "2   [-2.534292459487915, -6.132740020751953]  \n",
       "3   [8.227179527282715, -13.818502426147461]  \n",
       "4  [-6.128843307495117, -11.832077980041504]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires datasets library (use pip)\n",
    "#df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>tfidf_stemmed_tokens</th>\n",
       "      <th>tfidf_stemmed_hashtags</th>\n",
       "      <th>tfidf_lemmatized_tokens</th>\n",
       "      <th>tfidf_lemmatized_hashtags</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24090</td>\n",
       "      <td>0</td>\n",
       "      <td>best #lawofattraction #resources for #healing!...</td>\n",
       "      <td>0</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>['lawofattraction', 'for', 'altwaystoheal', 'is']</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>['lawofattract', 'altwaystoh']</td>\n",
       "      <td>['lawofattract', 'resourc', 'heal', 'altwaysto...</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resource', 'healing', 'al...</td>\n",
       "      <td>[-54.797821044921875, 19.326093673706055]</td>\n",
       "      <td>[50.279502868652344, -0.368119478225708]</td>\n",
       "      <td>[30.39127540588379, -49.107627868652344]</td>\n",
       "      <td>[8.619098663330078, -19.02434539794922]</td>\n",
       "      <td>[best, lawofattract, resourc, heal, altwaystoh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15264</td>\n",
       "      <td>0</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>0</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['to', 'on', 'simplest', 'moments', 'life', 'b...</td>\n",
       "      <td>['simplest', 'moments', 'life', 'blogger', 'li...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>[12.769611358642578, 38.40631866455078]</td>\n",
       "      <td>[-0.20247356593608856, -12.449955940246582]</td>\n",
       "      <td>[-25.304161071777344, -11.912176132202148]</td>\n",
       "      <td>[-5.459761619567871, -11.88219928741455]</td>\n",
       "      <td>[rememb, focu, simplest, happi, moment, life, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19310</td>\n",
       "      <td>0</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['you', 'as', 'as', 'boyfriend', 'be', 'with',...</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>[15.18979263305664, -9.596672058105469]</td>\n",
       "      <td>[10.67345905303955, -4.716570854187012]</td>\n",
       "      <td>[80.47900390625, 3.222534656524658]</td>\n",
       "      <td>[-2.534292459487915, -6.132740020751953]</td>\n",
       "      <td>[get, happi, boyfriend, reunit, car, silvia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27244</td>\n",
       "      <td>0</td>\n",
       "      <td>why do you always try to make me happy?  i don...</td>\n",
       "      <td>0</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>['do', 'always', 'to', 'me', 'i', 'know', 'to'...</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>['alway', 'know', 'love']</td>\n",
       "      <td>['love', 'devot']</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>[-46.13848876953125, -18.032955169677734]</td>\n",
       "      <td>[11.31847858428955, -16.98657989501953]</td>\n",
       "      <td>[40.34811019897461, -24.527305603027344]</td>\n",
       "      <td>[8.227179527282715, -13.818502426147461]</td>\n",
       "      <td>[alway, tri, make, happi, dont, know, make, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6633</td>\n",
       "      <td>0</td>\n",
       "      <td>omg is finally here!!! #ps4 #farcry4 #gtav #un...</td>\n",
       "      <td>0</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>['is', 'here', 'farcry4', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>[3.866750478744507, -5.09706449508667]</td>\n",
       "      <td>[21.51004409790039, 15.360187530517578]</td>\n",
       "      <td>[8.554491996765137, -20.749971389770508]</td>\n",
       "      <td>[-6.128843307495117, -11.832077980041504]</td>\n",
       "      <td>[omg, final, ps4, farcry4, gtav, unchaed4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0  24090      0  best #lawofattraction #resources for #healing!...   \n",
       "1  15264      0  remembering to focus on the simplest happy mom...   \n",
       "2  19310      0  when you get as happy as your boyfriend to be ...   \n",
       "3  27244      0  why do you always try to make me happy?  i don...   \n",
       "4   6633      0  omg is finally here!!! #ps4 #farcry4 #gtav #un...   \n",
       "\n",
       "   n_mentions                                           hashtags  \\\n",
       "0           0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1           0                        ['blogger', 'blog', 'life']   \n",
       "2           0                                         ['silvia']   \n",
       "3           0                               ['love', 'devotion']   \n",
       "4           0             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  ['lawofattraction', 'for', 'altwaystoheal', 'is']   \n",
       "1  ['to', 'on', 'simplest', 'moments', 'life', 'b...   \n",
       "2  ['you', 'as', 'as', 'boyfriend', 'be', 'with',...   \n",
       "3  ['do', 'always', 'to', 'me', 'i', 'know', 'to'...   \n",
       "4              ['is', 'here', 'farcry4', 'unchaed4']   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moments', 'life', 'blogger', 'li...   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0                     ['lawofattract', 'altwaystoh']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                          ['alway', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  ['lawofattract', 'resourc', 'heal', 'altwaysto...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                                  ['love', 'devot']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                 lemmatized_hashtags  \\\n",
       "0  ['lawofattraction', 'resource', 'healing', 'al...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                        tfidf_stemmed_tokens  \\\n",
       "0  [-54.797821044921875, 19.326093673706055]   \n",
       "1    [12.769611358642578, 38.40631866455078]   \n",
       "2    [15.18979263305664, -9.596672058105469]   \n",
       "3  [-46.13848876953125, -18.032955169677734]   \n",
       "4     [3.866750478744507, -5.09706449508667]   \n",
       "\n",
       "                        tfidf_stemmed_hashtags  \\\n",
       "0     [50.279502868652344, -0.368119478225708]   \n",
       "1  [-0.20247356593608856, -12.449955940246582]   \n",
       "2      [10.67345905303955, -4.716570854187012]   \n",
       "3      [11.31847858428955, -16.98657989501953]   \n",
       "4      [21.51004409790039, 15.360187530517578]   \n",
       "\n",
       "                      tfidf_lemmatized_tokens  \\\n",
       "0    [30.39127540588379, -49.107627868652344]   \n",
       "1  [-25.304161071777344, -11.912176132202148]   \n",
       "2         [80.47900390625, 3.222534656524658]   \n",
       "3    [40.34811019897461, -24.527305603027344]   \n",
       "4    [8.554491996765137, -20.749971389770508]   \n",
       "\n",
       "                   tfidf_lemmatized_hashtags  \\\n",
       "0    [8.619098663330078, -19.02434539794922]   \n",
       "1   [-5.459761619567871, -11.88219928741455]   \n",
       "2   [-2.534292459487915, -6.132740020751953]   \n",
       "3   [8.227179527282715, -13.818502426147461]   \n",
       "4  [-6.128843307495117, -11.832077980041504]   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [best, lawofattract, resourc, heal, altwaystoh...  \n",
       "1  [rememb, focu, simplest, happi, moment, life, ...  \n",
       "2       [get, happi, boyfriend, reunit, car, silvia]  \n",
       "3  [alway, tri, make, happi, dont, know, make, sa...  \n",
       "4         [omg, final, ps4, farcry4, gtav, unchaed4]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create preprossesed data\n",
    "df['preprocessed'] = preprocess(df['tweet'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>tfidf_stemmed_tokens</th>\n",
       "      <th>tfidf_stemmed_hashtags</th>\n",
       "      <th>tfidf_lemmatized_tokens</th>\n",
       "      <th>tfidf_lemmatized_hashtags</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24090</td>\n",
       "      <td>0</td>\n",
       "      <td>best #lawofattraction #resources for #healing!...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lawofattraction, resources, healing, altwayst...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>['lawofattraction', 'for', 'altwaystoheal', 'is']</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>['lawofattract', 'altwaystoh']</td>\n",
       "      <td>['lawofattract', 'resourc', 'heal', 'altwaysto...</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resource', 'healing', 'al...</td>\n",
       "      <td>[-54.797821044921875, 19.326093673706055]</td>\n",
       "      <td>[50.279502868652344, -0.368119478225708]</td>\n",
       "      <td>[30.39127540588379, -49.107627868652344]</td>\n",
       "      <td>[8.619098663330078, -19.02434539794922]</td>\n",
       "      <td>[best, lawofattract, resourc, heal, altwaystoh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15264</td>\n",
       "      <td>0</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>0</td>\n",
       "      <td>[blogger, blog, life]</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['to', 'on', 'simplest', 'moments', 'life', 'b...</td>\n",
       "      <td>['simplest', 'moments', 'life', 'blogger', 'li...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>[12.769611358642578, 38.40631866455078]</td>\n",
       "      <td>[-0.20247356593608856, -12.449955940246582]</td>\n",
       "      <td>[-25.304161071777344, -11.912176132202148]</td>\n",
       "      <td>[-5.459761619567871, -11.88219928741455]</td>\n",
       "      <td>[rememb, focu, simplest, happi, moment, life, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19310</td>\n",
       "      <td>0</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[silvia]</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['you', 'as', 'as', 'boyfriend', 'be', 'with',...</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>[15.18979263305664, -9.596672058105469]</td>\n",
       "      <td>[10.67345905303955, -4.716570854187012]</td>\n",
       "      <td>[80.47900390625, 3.222534656524658]</td>\n",
       "      <td>[-2.534292459487915, -6.132740020751953]</td>\n",
       "      <td>[get, happi, boyfriend, reunit, car, silvia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27244</td>\n",
       "      <td>0</td>\n",
       "      <td>why do you always try to make me happy?  i don...</td>\n",
       "      <td>0</td>\n",
       "      <td>[love, devotion]</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>['do', 'always', 'to', 'me', 'i', 'know', 'to'...</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>['alway', 'know', 'love']</td>\n",
       "      <td>['love', 'devot']</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>[-46.13848876953125, -18.032955169677734]</td>\n",
       "      <td>[11.31847858428955, -16.98657989501953]</td>\n",
       "      <td>[40.34811019897461, -24.527305603027344]</td>\n",
       "      <td>[8.227179527282715, -13.818502426147461]</td>\n",
       "      <td>[alway, tri, make, happi, dont, know, make, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6633</td>\n",
       "      <td>0</td>\n",
       "      <td>omg is finally here!!! #ps4 #farcry4 #gtav #un...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ps4, farcry4, gtav, unchaed4]</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>['is', 'here', 'farcry4', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>[3.866750478744507, -5.09706449508667]</td>\n",
       "      <td>[21.51004409790039, 15.360187530517578]</td>\n",
       "      <td>[8.554491996765137, -20.749971389770508]</td>\n",
       "      <td>[-6.128843307495117, -11.832077980041504]</td>\n",
       "      <td>[omg, final, ps4, farcry4, gtav, unchaed4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0  24090      0  best #lawofattraction #resources for #healing!...   \n",
       "1  15264      0  remembering to focus on the simplest happy mom...   \n",
       "2  19310      0  when you get as happy as your boyfriend to be ...   \n",
       "3  27244      0  why do you always try to make me happy?  i don...   \n",
       "4   6633      0  omg is finally here!!! #ps4 #farcry4 #gtav #un...   \n",
       "\n",
       "   n_mentions                                           hashtags  \\\n",
       "0           0  [lawofattraction, resources, healing, altwayst...   \n",
       "1           0                              [blogger, blog, life]   \n",
       "2           0                                           [silvia]   \n",
       "3           0                                   [love, devotion]   \n",
       "4           0                     [ps4, farcry4, gtav, unchaed4]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  ['lawofattraction', 'for', 'altwaystoheal', 'is']   \n",
       "1  ['to', 'on', 'simplest', 'moments', 'life', 'b...   \n",
       "2  ['you', 'as', 'as', 'boyfriend', 'be', 'with',...   \n",
       "3  ['do', 'always', 'to', 'me', 'i', 'know', 'to'...   \n",
       "4              ['is', 'here', 'farcry4', 'unchaed4']   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moments', 'life', 'blogger', 'li...   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0                     ['lawofattract', 'altwaystoh']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                          ['alway', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  ['lawofattract', 'resourc', 'heal', 'altwaysto...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                                  ['love', 'devot']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                 lemmatized_hashtags  \\\n",
       "0  ['lawofattraction', 'resource', 'healing', 'al...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                        tfidf_stemmed_tokens  \\\n",
       "0  [-54.797821044921875, 19.326093673706055]   \n",
       "1    [12.769611358642578, 38.40631866455078]   \n",
       "2    [15.18979263305664, -9.596672058105469]   \n",
       "3  [-46.13848876953125, -18.032955169677734]   \n",
       "4     [3.866750478744507, -5.09706449508667]   \n",
       "\n",
       "                        tfidf_stemmed_hashtags  \\\n",
       "0     [50.279502868652344, -0.368119478225708]   \n",
       "1  [-0.20247356593608856, -12.449955940246582]   \n",
       "2      [10.67345905303955, -4.716570854187012]   \n",
       "3      [11.31847858428955, -16.98657989501953]   \n",
       "4      [21.51004409790039, 15.360187530517578]   \n",
       "\n",
       "                      tfidf_lemmatized_tokens  \\\n",
       "0    [30.39127540588379, -49.107627868652344]   \n",
       "1  [-25.304161071777344, -11.912176132202148]   \n",
       "2         [80.47900390625, 3.222534656524658]   \n",
       "3    [40.34811019897461, -24.527305603027344]   \n",
       "4    [8.554491996765137, -20.749971389770508]   \n",
       "\n",
       "                   tfidf_lemmatized_hashtags  \\\n",
       "0    [8.619098663330078, -19.02434539794922]   \n",
       "1   [-5.459761619567871, -11.88219928741455]   \n",
       "2   [-2.534292459487915, -6.132740020751953]   \n",
       "3   [8.227179527282715, -13.818502426147461]   \n",
       "4  [-6.128843307495117, -11.832077980041504]   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [best, lawofattract, resourc, heal, altwaystoh...  \n",
       "1  [rememb, focu, simplest, happi, moment, life, ...  \n",
       "2       [get, happi, boyfriend, reunit, car, silvia]  \n",
       "3  [alway, tri, make, happi, dont, know, make, sa...  \n",
       "4         [omg, final, ps4, farcry4, gtav, unchaed4]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get n_usermentions and get seperate hashtags\n",
    "df = get_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trained tfidf vectorizer\n",
    "tfidf = train_tfidf(df['preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data on specified column\n",
    "df_train , df_test = split_data(df, 'preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>0</td>\n",
       "      <td>[cant, nice, everyon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>0</td>\n",
       "      <td>[recycl, make, green, natur, wastewarrior]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>0</td>\n",
       "      <td>[10, premium, staer, kit, today, happi, oiler,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>0</td>\n",
       "      <td>[tag, someon, quot, beauti, beach, lifestyl, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>0</td>\n",
       "      <td>[anyonebuttrump, go, backfir, user, lot, us, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                       preprocessed\n",
       "1823      0                              [cant, nice, everyon]\n",
       "2723      0         [recycl, make, green, natur, wastewarrior]\n",
       "3024      0  [10, premium, staer, kit, today, happi, oiler,...\n",
       "3793      0  [tag, someon, quot, beauti, beach, lifestyl, b...\n",
       "2283      0  [anyonebuttrump, go, backfir, user, lot, us, m..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsample data\n",
    "df_train_up = upsampling(df_train)\n",
    "df_train_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f9d8644790497f82f4b67c192095f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9441f274e17a407b8a714a6f88d659fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/881 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tweets_hate_speech_detection/default (download: 2.96 MiB, generated: 3.04 MiB, post-processed: Unknown size, total: 6.00 MiB) to C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1531d660361e417d8e3934805119400e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/31962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tweets_hate_speech_detection downloaded and prepared to C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d132f0bd1de4f6c96a2c950245b2517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Does all of the above (wip)\n",
    "tfidf, df_train, df_test = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 47550 training data, of which 50.0% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "print('There is {} training data, of which {}% is hate speech '.format(df_train['label'].count(), round(df_train['label'].sum()/df_train['label'].count()*100,2)))\n",
    "print('There is {} test data, of which {}% is hate speech '.format(df_test['label'].count(), round(df_test['label'].sum()/df_test['label'].count()*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.dirname((os.path.abspath(''))))\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from src.data.preprocessing import load_data, preprocess, train_tfidf, split_data, upsampling, get_features, setup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm_cv = pd.DataFrame(['precision', 'recall', 'accuracy', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4c8e1bbc9b478dadfda155d2aaeb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=False, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(df_train: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "\n",
    "    X_train = tfidf.transform(df_train['preprocessed'])\n",
    "    y_train = df_train['label']\n",
    "\n",
    "    # C-Support  Support Vector Machine\n",
    "    svm_grid = ms.GridSearchCV(svm.SVC(), param_grid={'C': [1], 'kernel': [\"linear\"]})\n",
    "    svm_grid.fit(X_train, y_train);\n",
    "    \n",
    "    return svm_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#idea for final finetuning\\nparam_grid={\\'C\\': [0.001,0.01,0.1,1,10,100,1000],  \\n            \\'gamma\\': [1, 0.1, 0.01, 0.001, 0.0001],\\n            \\'kernel\\': [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\\n            \\'class_weight\\': [None,\"balanced\"]}\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#idea for final finetuning\n",
    "param_grid={'C': [0.001,0.01,0.1,1,10,100,1000],  \n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            'class_weight': [None,\"balanced\"]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, df_test: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "    \n",
    "    X_test = tfidf.transform(df_test['preprocessed'])\n",
    "    y_test = df_test['label']\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    predictions.append(precision_score(y_test, y_pred))\n",
    "    predictions.append(recall_score(y_test, y_pred))\n",
    "    predictions.append(accuracy_score(y_test, y_pred))\n",
    "    predictions.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba52d5e85b2406787726e2033b76e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=False, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=False)\n",
    "svm_model = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Only Tokenization'] = test_model(svm_model, df_test, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.915254\n",
      "1    0.482143\n",
      "2    0.960582\n",
      "3    0.631579\n",
      "Name: Only Tokenization, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#print(results_svm_cv['Only Tokenization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843f11d5cf244025a5b13d5dbd389fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=False)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Remove Stopwords'] = test_model(svm_cv, df_test, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e69dcc93c64d6a96b5c6da163d2a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Emojis'] = test_model(svm_cv, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9426ba7644144a5ba2e77996c5a666d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, split_on='preprocessed', upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Upsampling'] = test_model(svm_cv, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d98c0c92674c4f952c7fc318737046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['All_but_stemming'] = test_model(svm_cv, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Only Tokenization</th>\n",
       "      <th>Remove Stopwords</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Upsampling</th>\n",
       "      <th>All_but_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.917012</td>\n",
       "      <td>0.921811</td>\n",
       "      <td>0.720358</td>\n",
       "      <td>0.740099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.667411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.960582</td>\n",
       "      <td>0.961364</td>\n",
       "      <td>0.961990</td>\n",
       "      <td>0.960738</td>\n",
       "      <td>0.960269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.648336</td>\n",
       "      <td>0.719553</td>\n",
       "      <td>0.701878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  Only Tokenization  Remove Stopwords    Emojis  Upsampling  \\\n",
       "0  precision           0.915254          0.917012  0.921811    0.720358   \n",
       "1     recall           0.482143          0.493304  0.500000    0.718750   \n",
       "2   accuracy           0.960582          0.961364  0.961990    0.960738   \n",
       "3         F1           0.631579          0.641509  0.648336    0.719553   \n",
       "\n",
       "   All_but_stemming  \n",
       "0          0.740099  \n",
       "1          0.667411  \n",
       "2          0.960269  \n",
       "3          0.701878  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_svm_cv"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cca499a840d662a33e8301a94c1b3730e7c5db68b1a61aac955d5a843456daa4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
