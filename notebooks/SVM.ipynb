{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install demoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Just in case the file structure does not make the preprocessing available\\nimport string\\nfrom xmlrpc.client import Boolean\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\nfrom nltk.stem.porter import PorterStemmer\\nimport pandas as pd\\nimport numpy as np\\nfrom datasets import load_dataset\\nfrom string import punctuation\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport sklearn.model_selection as ms\\nfrom sklearn.utils import resample\\nimport demoji\\nimport re\\n\\n\\n\\ndemoji.download_codes()\\n\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\n\\n\\ndef setup(rem_stop=True, do_stem=True, do_lem=False, split=True, split_on=\\'preprocessed\\', upsample=True, do_emojis=True):\\n    df = load_data();\\n    df[\\'preprocessed\\'] = preprocess(\\n        df[\\'tweet\\'], rem_stop=rem_stop, do_stem=do_stem, do_lem=do_lem, do_emojis=do_emojis)\\n\\n    tfidf = train_tfidf(df[\\'preprocessed\\'])\\n\\n    if split is True:\\n        df_train, df_test = split_data(df, split_on)\\n        if upsample is True:\\n            df_train = upsampling(df_train)\\n        return tfidf, df_train, df_test\\n    else:\\n        return tfidf, df\\n\\n\\ndef load_data():\\n    dataset = load_dataset(\"tweets_hate_speech_detection\")\\n    df = pd.DataFrame.from_dict(dataset[\\'train\\'])\\n    return df\\n\\n\\ndef preprocess(data, rem_stop=True, do_stem=True, do_lem=False, do_emojis=True):\\n\\n    preprocessed = []\\n    for tweet in data:\\n        if do_emojis is True:\\n            tweet = convert_emoji(tweet)\\n        tokens = tokenization(remove_punctuation(tweet))\\n        if rem_stop is True:\\n            tokens = remove_stopwords(tokens)\\n        if do_stem is True and do_lem is False:\\n            tokens = stemming(tokens)\\n        if do_lem is True and do_stem is False:\\n            tokens = lemmatization(tokens)\\n        preprocessed.append(np.array(tokens))\\n\\n    return preprocessed\\n\\n\\ndef train_tfidf(data):\\n    def dummy(text):\\n        return text\\n\\n    tf = TfidfVectorizer(\\n        analyzer=\\'word\\',\\n        tokenizer=dummy,\\n        preprocessor=dummy,\\n        token_pattern=None)\\n\\n    return tf.fit(data)\\n\\n\\ndef split_data(df: pd.DataFrame, split_on=\\'tweet\\', test_size=0.2, random_state=17):\\n    y = df[\\'label\\']\\n    X = df[split_on]\\n    (X_train, X_test, y_train, y_test) = ms.train_test_split(\\n        X, y, test_size=test_size, random_state=random_state, stratify=y)\\n\\n    df_train = pd.concat([y_train, X_train], axis=1)\\n    df_test = pd.concat([y_test, X_test], axis=1)\\n\\n    return df_train, df_test\\n\\n\\ndef upsampling(df: pd.DataFrame, replace=True, n_samples=23775, random_state=55):\\n    data_minority = df[df.label == 1]\\n    data_majority = df[df.label == 0]\\n    data_minority = resample(\\n        data_minority, replace=replace, n_samples=n_samples, random_state=random_state)\\n\\n    return pd.concat([data_majority, data_minority])\\n\\n\\ndef tokenization(text: str):\\n    return pd.Series(nltk.word_tokenize(text.lower()))\\n\\n\\ndef remove_punctuation(tokens: pd.Series):\\n    return \"\".join([i for i in tokens if i not in punctuation])\\n\\n\\ndef remove_stopwords(tokens: pd.Series):\\n    stopwords_list = stopwords.words(\"english\")\\n    return tokens.apply(lambda token: token if token not in stopwords_list and token != \\'\\' else None).dropna()\\n\\n\\ndef stemming(tokens: pd.Series):\\n    stemmer = PorterStemmer()\\n\\n    return tokens.apply(lambda token: stemmer.stem(token))\\n\\n\\ndef lemmatization(tokens: pd.Series):\\n    lemmatizer = WordNetLemmatizer()\\n\\n    return tokens.apply(lambda token: lemmatizer.lemmatize(token))\\n\\n\\ndef convert_emoji(text: str) -> str:\\n    # convert string to binary representation\\n    binary = \\' \\'.join(format(ord(x), \\'b\\') for x in text)\\n\\n    # convert binary representation to utf8 representation\\n    listRes = list(binary.split(\" \"))\\n    try:\\n        text_with_emoji = bytes([int(x, 2) for x in listRes]).decode(\\'utf-8\\')\\n    except UnicodeDecodeError:\\n        return text\\n\\n    # get all emojis\\n    dictionary = demoji.findall(text_with_emoji)\\n\\n    # replace emojis with text representation\\n    for key in dictionary.keys():\\n        text_with_emoji = text_with_emoji.replace(key, dictionary[key] + \" \")\\n\\n    return text_with_emoji\\n\\n\\ndef get_features(df: pd.DataFrame):\\n    df[\"n_mentions\"] = df[\"tweet\"].apply(lambda x: count_user_mentions(x))\\n    df[\"hashtags\"] = df[\"tweet\"].apply(lambda x: identify_hashtags(x))\\n\\n    return df\\n\\ndef count_user_mentions(text:str) ->int:\\n    return text.count(\"@user\")\\n\\ndef identify_hashtags(text:str) -> list:\\n    pattern = re.compile(r\"#(\\\\w+)\")\\n    return pattern.findall(text)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Just in case the file structure does not make the preprocessing available\n",
    "import string\n",
    "from xmlrpc.client import Boolean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.utils import resample\n",
    "import demoji\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "demoji.download_codes()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def setup(rem_stop=True, do_stem=True, do_lem=False, split=True, split_on='preprocessed', upsample=True, do_emojis=True):\n",
    "    df = load_data();\n",
    "    df['preprocessed'] = preprocess(\n",
    "        df['tweet'], rem_stop=rem_stop, do_stem=do_stem, do_lem=do_lem, do_emojis=do_emojis)\n",
    "\n",
    "    tfidf = train_tfidf(df['preprocessed'])\n",
    "\n",
    "    if split is True:\n",
    "        df_train, df_test = split_data(df, split_on)\n",
    "        if upsample is True:\n",
    "            df_train = upsampling(df_train)\n",
    "        return tfidf, df_train, df_test\n",
    "    else:\n",
    "        return tfidf, df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    dataset = load_dataset(\"tweets_hate_speech_detection\")\n",
    "    df = pd.DataFrame.from_dict(dataset['train'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(data, rem_stop=True, do_stem=True, do_lem=False, do_emojis=True):\n",
    "\n",
    "    preprocessed = []\n",
    "    for tweet in data:\n",
    "        if do_emojis is True:\n",
    "            tweet = convert_emoji(tweet)\n",
    "        tokens = tokenization(remove_punctuation(tweet))\n",
    "        if rem_stop is True:\n",
    "            tokens = remove_stopwords(tokens)\n",
    "        if do_stem is True and do_lem is False:\n",
    "            tokens = stemming(tokens)\n",
    "        if do_lem is True and do_stem is False:\n",
    "            tokens = lemmatization(tokens)\n",
    "        preprocessed.append(np.array(tokens))\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "def train_tfidf(data):\n",
    "    def dummy(text):\n",
    "        return text\n",
    "\n",
    "    tf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        token_pattern=None)\n",
    "\n",
    "    return tf.fit(data)\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame, split_on='tweet', test_size=0.2, random_state=17):\n",
    "    y = df['label']\n",
    "    X = df[split_on]\n",
    "    (X_train, X_test, y_train, y_test) = ms.train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    df_train = pd.concat([y_train, X_train], axis=1)\n",
    "    df_test = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def upsampling(df: pd.DataFrame, replace=True, n_samples=23775, random_state=55):\n",
    "    data_minority = df[df.label == 1]\n",
    "    data_majority = df[df.label == 0]\n",
    "    data_minority = resample(\n",
    "        data_minority, replace=replace, n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "    return pd.concat([data_majority, data_minority])\n",
    "\n",
    "\n",
    "def tokenization(text: str):\n",
    "    return pd.Series(nltk.word_tokenize(text.lower()))\n",
    "\n",
    "\n",
    "def remove_punctuation(tokens: pd.Series):\n",
    "    return \"\".join([i for i in tokens if i not in punctuation])\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: pd.Series):\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    return tokens.apply(lambda token: token if token not in stopwords_list and token != '' else None).dropna()\n",
    "\n",
    "\n",
    "def stemming(tokens: pd.Series):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return tokens.apply(lambda token: stemmer.stem(token))\n",
    "\n",
    "\n",
    "def lemmatization(tokens: pd.Series):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return tokens.apply(lambda token: lemmatizer.lemmatize(token))\n",
    "\n",
    "\n",
    "def convert_emoji(text: str) -> str:\n",
    "    # convert string to binary representation\n",
    "    binary = ' '.join(format(ord(x), 'b') for x in text)\n",
    "\n",
    "    # convert binary representation to utf8 representation\n",
    "    listRes = list(binary.split(\" \"))\n",
    "    try:\n",
    "        text_with_emoji = bytes([int(x, 2) for x in listRes]).decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return text\n",
    "\n",
    "    # get all emojis\n",
    "    dictionary = demoji.findall(text_with_emoji)\n",
    "\n",
    "    # replace emojis with text representation\n",
    "    for key in dictionary.keys():\n",
    "        text_with_emoji = text_with_emoji.replace(key, dictionary[key] + \" \")\n",
    "\n",
    "    return text_with_emoji\n",
    "\n",
    "\n",
    "def get_features(df: pd.DataFrame):\n",
    "    df[\"n_mentions\"] = df[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
    "    df[\"hashtags\"] = df[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "def count_user_mentions(text:str) ->int:\n",
    "    return text.count(\"@user\")\n",
    "\n",
    "def identify_hashtags(text:str) -> list:\n",
    "    pattern = re.compile(r\"#(\\w+)\")\n",
    "    return pattern.findall(text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayte\\Documents\\GitHub\\Data_mining\\src\\data\\preprocessing.py:18: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname((os.path.abspath(''))))\n",
    "from src.data.preprocessing import load_data, preprocess, train_tfidf, split_data, upsampling, get_features, setup\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba90e12f6364fc1bfe5f0cae0737eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocessing (wip)\n",
    "tfidf, df_train, df_test = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 47550 training data, of which 50.0% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "print('There is {} training data, of which {}% is hate speech '.format(df_train['label'].count(), round(df_train['label'].sum()/df_train['label'].count()*100,2)))\n",
    "print('There is {} test data, of which {}% is hate speech '.format(df_test['label'].count(), round(df_test['label'].sum()/df_test['label'].count()*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.dirname((os.path.abspath(''))))\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from src.data.preprocessing import load_data, preprocess, train_tfidf, split_data, upsampling, get_features, setup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm_cv = pd.DataFrame([\"param\", 'precision', 'recall', 'accuracy', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399d36f1f17d4a9a935a6a0596a12ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=False, do_stem=True, do_lem=False, split=True, upsample=False, do_emojis=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid={\\'C\\': [x for x in range (1, 202, 25)],  \\n            \\'gamma\\': [1, 0.1, 0.01, 0.001, 0.0001],\\n            \\'kernel\\': [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\\n           \\'class_weight\\': [\"balanced\", None]\\n           }\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#idea for final finetuning\n",
    "\"\"\"\n",
    "param_grid={'C': [x for x in range (1, 202, 25)],  \n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "           'class_weight': [\"balanced\", None]\n",
    "           }\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller grid for old laptops:\n",
    "param_grid={'C': [1],\n",
    "            'kernel': [\"linear\"],\n",
    "           'class_weight': [None]\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(df_train: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "\n",
    "    X_train = tfidf.transform(df_train['preprocessed'])\n",
    "    y_train = df_train['label']\n",
    "\n",
    "    # C-Support  Support Vector Machine\n",
    "    svm_grid = GridSearchCV(svm.SVC(random_state=55), param_grid=param_grid, verbose=10, n_jobs=-1, scoring='f1', cv=5)\n",
    "    svm_grid.fit(X_train, y_train);\n",
    "    \n",
    "    return svm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter \"class_weight = 'balanced' \" has proven most useful, however it does have same/similar effect so upsamling; therefore left out of analysis for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, df_test: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "    \n",
    "    X_test = tfidf.transform(df_test['preprocessed'])\n",
    "    y_test = df_test['label']\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    predictions.append(model.get_params())\n",
    "    predictions.append(precision_score(y_test, y_pred))\n",
    "    predictions.append(recall_score(y_test, y_pred))\n",
    "    predictions.append(accuracy_score(y_test, y_pred))\n",
    "    predictions.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa52870f4334ac798725715d0063922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Don't Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=False, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=False)\n",
    "svm_model = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Only Tokenization'] = test_model(svm_model, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'C': 1, 'break_ties': False, 'cache_size': 20...\n",
      "1                                             0.915254\n",
      "2                                             0.482143\n",
      "3                                             0.960582\n",
      "4                                             0.631579\n",
      "Name: Only Tokenization, dtype: object\n"
     ]
    }
   ],
   "source": [
    "results_svm_cv.to_csv(r'result_svm_1.csv', header=True, index=None, sep=';', mode='a')\n",
    "print(results_svm_cv['Only Tokenization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8980f40fad8b40c6bd552a7e37ce31b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=False)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Remove Stopwords'] = test_model(svm_cv, df_test, tfidf)\n",
    "results_svm_cv.to_csv(r'result_svm_2.csv', header=True, index=None, sep=';', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10a29a983f44feb829b9555243ffd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Emojis'] = test_model(svm_cv, df_test, tfidf)\n",
    "results_svm_cv.to_csv(r'result_svm_3.csv', header=True, index=None, sep=';', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ec85935bb14b8eb2488868ffc15a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, upsample=False, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Stemming'] = test_model(svm_cv, df_test, tfidf)\n",
    "results_svm_cv.to_csv(r'result_svm_4.csv', header=True, index=None, sep=';', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac98f76fa236452d8085a348a5c6a821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['Upsampling'] = test_model(svm_cv, df_test, tfidf)\n",
    "results_svm_cv.to_csv(r'result_svm_5.csv', header=True, index=None, sep=';', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37cf6eb2a6e4694821ee77b0ba3d7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_svm_cv['All_but_stemming'] = test_model(svm_cv, df_test, tfidf)\n",
    "results_svm_cv.to_csv(r'result_svm_6.csv', header=True, index=None, sep=';', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm_cv\n",
    "results_svm_cv.to_csv(r'result_svm.csv', header=True, index=None, sep=';', mode='a')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cca499a840d662a33e8301a94c1b3730e7c5db68b1a61aac955d5a843456daa4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
