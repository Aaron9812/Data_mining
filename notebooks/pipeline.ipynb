{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname((os.path.abspath(''))))\n",
    "from src.data.preprocessing import load_data, preprocess, train_tfidf, split_data, upsampling, get_features, setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires datasets library (use pip)\n",
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprossesed data\n",
    "df['preprocessed'] = preprocess(df['tweet'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n_usermentions and get seperate hashtags\n",
    "df = get_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trained tfidf vectorizer\n",
    "tfidf = train_tfidf(df['preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data on specified column\n",
    "df_train , df_test = split_data(df, 'preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample data\n",
    "df_train_up = upsampling(df_train)\n",
    "df_train_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does all of the above (wip)\n",
    "tfidf, df_train, df_test = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There is {} training data, of which {}% is hate speech '.format(df_train['label'].count(), round(df_train['label'].sum()/df_train['label'].count()*100,2)))\n",
    "print('There is {} test data, of which {}% is hate speech '.format(df_test['label'].count(), round(df_test['label'].sum()/df_test['label'].count()*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programme\\Miniconda\\envs\\DataMining\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jonas\\OneDrive - bwedu\\Studium_Master\\1_FSS_2022\\Data Mining [IE 500]\\Projekt\\Code\\Git\\Data_mining\\src\\data\\preprocessing.py:18: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.dirname((os.path.abspath(''))))\n",
    "from src.models.Naive_Bayes import train_mvb_bayes, train_mn_bayes, test_model\n",
    "from src.data.preprocessing import load_data, preprocess, train_tfidf, split_data, upsampling, get_features, setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mvb = pd.DataFrame(['precision', 'recall', 'accuracy', 'F1'])\n",
    "results_mn = pd.DataFrame(['precision', 'recall', 'accuracy', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\jonas\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 334.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Don't Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=False, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=False)\n",
    "mvb = train_mvb_bayes(df_train, tfidf)\n",
    "mn = train_mn_bayes(df_train, tfidf)\n",
    "results_mvb['Only Tokenization'] = test_model(mvb, df_test, tfidf)\n",
    "results_mn['Only Tokenization'] = test_model(mn, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\jonas\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 503.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=False)\n",
    "mvb = train_mvb_bayes(df_train, tfidf)\n",
    "mn = train_mn_bayes(df_train, tfidf)\n",
    "results_mvb['Remove Stopwords'] = test_model(mvb, df_test, tfidf)\n",
    "results_mn['Remove Stopwords'] = test_model(mn, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\jonas\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=True)\n",
    "mvb = train_mvb_bayes(df_train, tfidf)\n",
    "mn = train_mn_bayes(df_train, tfidf)\n",
    "results_mvb['Emojis'] = test_model(mvb, df_test, tfidf)\n",
    "results_mn['Emojis'] = test_model(mn, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\jonas\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 495.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, split_on='preprocessed', upsample=False, do_emojis=True)\n",
    "mvb = train_mvb_bayes(df_train, tfidf)\n",
    "mn = train_mn_bayes(df_train, tfidf)\n",
    "results_mvb['Stemming'] = test_model(mvb, df_test, tfidf)\n",
    "results_mn['Stemming'] = test_model(mn, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\jonas\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 330.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, split_on='preprocessed', upsample=True, do_emojis=True)\n",
    "mvb = train_mvb_bayes(df_train, tfidf)\n",
    "mn = train_mn_bayes(df_train, tfidf)\n",
    "results_mvb['Upsampling'] = test_model(mvb, df_test, tfidf)\n",
    "results_mn['Upsampling'] = test_model(mn, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\jonas\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 334.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, split_on='preprocessed', upsample=True, do_emojis=True)\n",
    "mvb = train_mvb_bayes(df_train, tfidf)\n",
    "mn = train_mn_bayes(df_train, tfidf)\n",
    "results_mvb['All_but_stemming'] = test_model(mvb, df_test, tfidf)\n",
    "results_mn['All_but_stemming'] = test_model(mn, df_test, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Only Tokenization</th>\n",
       "      <th>Remove Stopwords</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Stemming</th>\n",
       "      <th>Upsampling</th>\n",
       "      <th>All_but_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.676136</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.647959</td>\n",
       "      <td>0.695793</td>\n",
       "      <td>0.507576</td>\n",
       "      <td>0.536379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>0.566964</td>\n",
       "      <td>0.479911</td>\n",
       "      <td>0.747768</td>\n",
       "      <td>0.707589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.949320</td>\n",
       "      <td>0.948694</td>\n",
       "      <td>0.948068</td>\n",
       "      <td>0.948850</td>\n",
       "      <td>0.931488</td>\n",
       "      <td>0.936649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.581633</td>\n",
       "      <td>0.604762</td>\n",
       "      <td>0.568032</td>\n",
       "      <td>0.604693</td>\n",
       "      <td>0.610202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  Only Tokenization  Remove Stopwords    Emojis  Stemming  \\\n",
       "0  precision           0.676136          0.678571  0.647959  0.695793   \n",
       "1     recall           0.531250          0.508929  0.566964  0.479911   \n",
       "2   accuracy           0.949320          0.948694  0.948068  0.948850   \n",
       "3         F1           0.595000          0.581633  0.604762  0.568032   \n",
       "\n",
       "   Upsampling  All_but_stemming  \n",
       "0    0.507576          0.536379  \n",
       "1    0.747768          0.707589  \n",
       "2    0.931488          0.936649  \n",
       "3    0.604693          0.610202  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mvb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Only Tokenization</th>\n",
       "      <th>Remove Stopwords</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Stemming</th>\n",
       "      <th>Upsampling</th>\n",
       "      <th>All_but_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.664615</td>\n",
       "      <td>0.744526</td>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.515773</td>\n",
       "      <td>0.549565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.477679</td>\n",
       "      <td>0.477679</td>\n",
       "      <td>0.729911</td>\n",
       "      <td>0.705357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.946660</td>\n",
       "      <td>0.950884</td>\n",
       "      <td>0.954325</td>\n",
       "      <td>0.951822</td>\n",
       "      <td>0.933052</td>\n",
       "      <td>0.938839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.558862</td>\n",
       "      <td>0.565097</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.581522</td>\n",
       "      <td>0.604436</td>\n",
       "      <td>0.617791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  Only Tokenization  Remove Stopwords    Emojis  Stemming  \\\n",
       "0  precision           0.664615          0.744526  0.786765  0.743056   \n",
       "1     recall           0.482143          0.455357  0.477679  0.477679   \n",
       "2   accuracy           0.946660          0.950884  0.954325  0.951822   \n",
       "3         F1           0.558862          0.565097  0.594444  0.581522   \n",
       "\n",
       "   Upsampling  All_but_stemming  \n",
       "0    0.515773          0.549565  \n",
       "1    0.729911          0.705357  \n",
       "2    0.933052          0.938839  \n",
       "3    0.604436          0.617791  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mn"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cca499a840d662a33e8301a94c1b3730e7c5db68b1a61aac955d5a843456daa4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DataMining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
