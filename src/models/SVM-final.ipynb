{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Hate Speech in Tweets using C-Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayte\\Documents\\GitHub\\Data_mining\\src\\models\\../data\\preprocessing.py:19: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mayte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.append(os.path.dirname((os.path.abspath(''))))\n",
    "sys.path.append(\"../data\")\n",
    "from preprocessing import load_data, preprocess, train_tfidf, split_data, upsampling, get_features, setup\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e4a81591b34a80a4c727c7c9278e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "# pre- processing\n",
    "tfidf, df_train, df_test = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 47550 training data, of which 50.0% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "print('There is {} training data, of which {}% is hate speech '.format(df_train['label'].count(), round(df_train['label'].sum()/df_train['label'].count()*100,2)))\n",
    "print('There is {} test data, of which {}% is hate speech '.format(df_test['label'].count(), round(df_test['label'].sum()/df_test['label'].count()*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Function (with GridSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter \"class_weight = 'balanced' \" has proven very useful, however it does have a similar effect so upsamling; therefore left out of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'C': [i for i in range (1, 3000, 50)],\n",
    "            'kernel': [\"linear\", \"rbf\"],\n",
    "            'class_weight': [None],\n",
    "            'gamma': ['scale', 0.00003, 0.0003, 0.003, 0.03, 0.3, 1, 3, 8]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing notebook\n",
    "# REMOVE\n",
    "param_grid={'C': [1],\n",
    "            'kernel': [\"linear\"],\n",
    "            'class_weight': [None],\n",
    "            'gamma': ['scale']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(df_train: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "\n",
    "    X_train = tfidf.transform(df_train['preprocessed'])\n",
    "    y_train = df_train['label']\n",
    "\n",
    "    # C-Support  Support Vector Machine\n",
    "    svm_grid = GridSearchCV(svm.SVC(random_state=55), param_grid=param_grid, verbose=10, n_jobs=-1, scoring='f1', cv=5)\n",
    "    svm_grid.fit(X_train, y_train);\n",
    "    \n",
    "    return svm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, df_test: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "    \n",
    "    X_test = tfidf.transform(df_test['preprocessed'])\n",
    "    y_test = df_test['label']\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    predictions.append(model.get_params())\n",
    "    predictions.append(precision_score(y_test, y_pred))\n",
    "    predictions.append(recall_score(y_test, y_pred))\n",
    "    predictions.append(accuracy_score(y_test, y_pred))\n",
    "    predictions.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Result List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b598adce056426b9bbae70a73b9749c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Don't Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=False, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=False)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results.append(\"Only Tokenization \\n\")\n",
    "results.append(test_model(svm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (C:\\Users\\Mayte\\.cache\\huggingface\\datasets\\tweets_hate_speech_detection\\default\\0.0.0\\c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe8d25cd85a43f28d26ac786f1f8657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=False)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results.append(\"\\n\\nRemove Stopwords \\n\")\n",
    "results.append(test_model(svm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results.append(\"\\n\\nEmojis \\n\")\n",
    "results.append(test_model(scm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, upsample=False, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results.append(\"\\n\\nStemming \\n\")\n",
    "results.append(test_model(svm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results.append(\"\\n\\nUpsampling \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-but-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results.append(\"\\n\\nAll-but-Stemming \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Function (with RandomSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'C': [i for i in range (1, 3000)],\n",
    "            'kernel': [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            'class_weight': [None, \"balanced\"],\n",
    "            'gamma': ['scale', 0.00003, 0.0003, 0.003, 0.03, 0.3, 1, 3, 8]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing notebook\n",
    "# REMOVE\n",
    "param_grid={'C': [1],\n",
    "            'kernel': [\"linear\"],\n",
    "            'class_weight': [None],\n",
    "            'gamma': ['scale']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(df_train: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "\n",
    "    X_train = tfidf.transform(df_train['preprocessed'])\n",
    "    y_train = df_train['label']\n",
    "\n",
    "    # C-Support  Support Vector Machine (RandomizedSearchCV)\n",
    "    svm_grid = RandomizedSearchCV(svm.SVC(random_state=55), param_grid, verbose=10, n_jobs=-1, scoring='f1', cv=5)\n",
    "    svm_grid.fit(X_train, y_train);\n",
    "    \n",
    "    return svm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Result List (RandomSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_random = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=False, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=False)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_random.append(\"Only Tokenization \\n\")\n",
    "results_random.append(test_model(svm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, No Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=False)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_random.append(\"\\n\\nRemove Stopwords \\n\")\n",
    "results_random.append(test_model(svm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=False, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_random.append(\"\\n\\nEmojis \\n\")\n",
    "results_random.append(test_model(scm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, No Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, upsample=False, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_random.append(\"\\n\\nStemming \\n\")\n",
    "results_random.append(test_model(svm_cv, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=True, do_lem=False, split=True, upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_random.append(\"\\n\\nUpsampling \\n\")\n",
    "results_random.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-but-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords, Emojis, No Stemming, Upsampling \n",
    "tfidf, df_train, df_test = setup(rem_stop=True, do_stem=False, do_lem=False, split=True, upsample=True, do_emojis=True)\n",
    "svm_cv = train_svm(df_train, tfidf)\n",
    "results_random.append(\"\\n\\nAll-but-Stemming \\n\")\n",
    "results_random.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "textfile = open(\"results_random_svm.txt\", \"w\")\n",
    "for element in results:\n",
    "    if not isinstance(element, str):\n",
    "        for subelement in element:\n",
    "            textfile.write(str(subelement) + \"\\n\")\n",
    "        continue\n",
    "    textfile.write(str(element) + \"\\n\")\n",
    "textfile.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(kernel= \"rbf\", C=2496, class_weight = \"balanced\", gamma = 0.3)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "prediction = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: \", accuracy_score(y_test, prediction))\n",
    "print(\"precision: \",(precision_score(y_test, y_pred))\n",
    "print(\"recall: \", recall_score(y_test, y_pred))\n",
    "print(\"f1: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion-Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cca499a840d662a33e8301a94c1b3730e7c5db68b1a61aac955d5a843456daa4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
