{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_converted_emojis</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_punctuation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['run']</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>['user', 'when', 'a', 'father', 'is', 'dysfunc...</td>\n",
       "      <td>['user', 'father', 'dysfunctional', 'selfish',...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>['user', 'father', 'dysfunct', 'selfish', 'dra...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>['user', 'father', 'dysfunctional', 'selfish',...</td>\n",
       "      <td>['run']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>2</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>['user', 'user', 'thanks', 'for', 'lyft', 'cre...</td>\n",
       "      <td>['user', 'user', 'thanks', 'lyft', 'credit', '...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>['user', 'user', 'thank', 'lyft', 'credit', 'c...</td>\n",
       "      <td>['lyft', 'disapoint', 'getthank']</td>\n",
       "      <td>['user', 'user', 'thanks', 'lyft', 'credit', '...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>['bihday', 'your', 'majesty']</td>\n",
       "      <td>['bihday', 'majesty']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['bihday', 'majesti']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['bihday', 'majesty']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['model']</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>['model', 'i', 'love', 'u', 'take', 'with', 'u...</td>\n",
       "      <td>['model', 'love', 'u', 'take', 'u', 'time', 'u...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['model', 'love', 'u', 'take', 'u', 'time', 'u...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['model', 'love', 'u', 'take', 'u', 'time', 'u...</td>\n",
       "      <td>['model']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>['factsguide', 'society', 'now', 'motivation']</td>\n",
       "      <td>['factsguide', 'society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>['factsguid', 'societi', 'motiv']</td>\n",
       "      <td>['motiv']</td>\n",
       "      <td>['factsguide', 'society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                              tweet_converted_emojis  n_mentions  \\\n",
       "0   @user when a father is dysfunctional and is s...           1   \n",
       "1  @user @user thanks for #lyft credit i can't us...           2   \n",
       "2                                bihday your majesty           0   \n",
       "3  #model   i love u take with u all the time in ...           0   \n",
       "4             factsguide: society now    #motivation           0   \n",
       "\n",
       "                                hashtags  \\\n",
       "0                                ['run']   \n",
       "1  ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                     []   \n",
       "3                              ['model']   \n",
       "4                         ['motivation']   \n",
       "\n",
       "                                 without_punctuation  \\\n",
       "0   user when a father is dysfunctional and is so...   \n",
       "1  user user thanks for lyft credit i cant use ca...   \n",
       "2                                bihday your majesty   \n",
       "3  model   i love u take with u all the time in u...   \n",
       "4               factsguide society now    motivation   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0   user when a father is dysfunctional and is so...   \n",
       "1  user user thanks for lyft credit i cant use ca...   \n",
       "2                                bihday your majesty   \n",
       "3  model   i love u take with u all the time in u...   \n",
       "4               factsguide society now    motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  ['user', 'when', 'a', 'father', 'is', 'dysfunc...   \n",
       "1  ['user', 'user', 'thanks', 'for', 'lyft', 'cre...   \n",
       "2                      ['bihday', 'your', 'majesty']   \n",
       "3  ['model', 'i', 'love', 'u', 'take', 'with', 'u...   \n",
       "4     ['factsguide', 'society', 'now', 'motivation']   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  ['user', 'father', 'dysfunctional', 'selfish',...   \n",
       "1  ['user', 'user', 'thanks', 'lyft', 'credit', '...   \n",
       "2                              ['bihday', 'majesty']   \n",
       "3  ['model', 'love', 'u', 'take', 'u', 'time', 'u...   \n",
       "4            ['factsguide', 'society', 'motivation']   \n",
       "\n",
       "                          clean_hashtags  \\\n",
       "0                                ['run']   \n",
       "1  ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                     []   \n",
       "3                              ['model']   \n",
       "4                         ['motivation']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  ['user', 'father', 'dysfunct', 'selfish', 'dra...   \n",
       "1  ['user', 'user', 'thank', 'lyft', 'credit', 'c...   \n",
       "2                              ['bihday', 'majesti']   \n",
       "3  ['model', 'love', 'u', 'take', 'u', 'time', 'u...   \n",
       "4                  ['factsguid', 'societi', 'motiv']   \n",
       "\n",
       "                    stemmed_hashtags  \\\n",
       "0                            ['run']   \n",
       "1  ['lyft', 'disapoint', 'getthank']   \n",
       "2                                 []   \n",
       "3                          ['model']   \n",
       "4                          ['motiv']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  ['user', 'father', 'dysfunctional', 'selfish',...   \n",
       "1  ['user', 'user', 'thanks', 'lyft', 'credit', '...   \n",
       "2                              ['bihday', 'majesty']   \n",
       "3  ['model', 'love', 'u', 'take', 'u', 'time', 'u...   \n",
       "4            ['factsguide', 'society', 'motivation']   \n",
       "\n",
       "                     lemmatized_hashtags  \n",
       "0                                ['run']  \n",
       "1  ['lyft', 'disapointed', 'getthanked']  \n",
       "2                                     []  \n",
       "3                              ['model']  \n",
       "4                         ['motivation']  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../../data/220510_train_data_preprocessed.csv\", sep=';')\n",
    "test_data = pd.read_csv(\"../../data/220510_test_data_preprocessed.csv\", sep=\";\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features = [\"n_mentions\", \"lemmatized_hashtags\", \"lemmatized_tokens\"]\\nX = data[features]\\ny = data.label\\nX.head()'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"features = [\"n_mentions\", \"lemmatized_hashtags\", \"lemmatized_tokens\"]\n",
    "X = data[features]\n",
    "y = data.label\n",
    "X.head()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the bag of words from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import texthero as hero\\nfrom texthero import preprocessing\\nX[\"bow_tokens\"] = (hero.tfidf(X[\"lemmatized_tokens\"], max_features=15000))\\nX[\"bow_hashtags\"] = (hero.tfidf(X[\"lemmatized_hashtags\"], max_features=15000))\\nX'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import texthero as hero\n",
    "from texthero import preprocessing\n",
    "X[\"bow_tokens\"] = (hero.tfidf(X[\"lemmatized_tokens\"], max_features=15000))\n",
    "X[\"bow_hashtags\"] = (hero.tfidf(X[\"lemmatized_hashtags\"], max_features=15000))\n",
    "X\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimension of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X[\"bow_tokens\"] = (hero.tsne(X[\"bow_tokens\"]))\\nX[\"bow_hashtags\"] = (hero.tsne(X[\"bow_hashtags\"]))\\nX'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X[\"bow_tokens\"] = (hero.tsne(X[\"bow_tokens\"]))\n",
    "X[\"bow_hashtags\"] = (hero.tsne(X[\"bow_hashtags\"]))\n",
    "X\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split vector into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X[[\"bow_tokens_1\", \"bow_tokens_\"]] = pd.DataFrame(X.bow_tokens.tolist(), index= X.index)\\nX[[\"bow_hashtags_1\", \"bow_hashtags_2\"]] = pd.DataFrame(X.bow_hashtags.tolist(), index= X.index)\\nX'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X[[\"bow_tokens_1\", \"bow_tokens_\"]] = pd.DataFrame(X.bow_tokens.tolist(), index= X.index)\n",
    "X[[\"bow_hashtags_1\", \"bow_hashtags_2\"]] = pd.DataFrame(X.bow_hashtags.tolist(), index= X.index)\n",
    "X\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>n_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['user', 'father', 'dysfunctional', 'selfish',...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['user', 'user', 'thanks', 'lyft', 'credit', '...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['bihday', 'majesty']</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['model', 'love', 'u', 'take', 'u', 'time', 'u...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['factsguide', 'society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   lemmatized_tokens  \\\n",
       "0  ['user', 'father', 'dysfunctional', 'selfish',...   \n",
       "1  ['user', 'user', 'thanks', 'lyft', 'credit', '...   \n",
       "2                              ['bihday', 'majesty']   \n",
       "3  ['model', 'love', 'u', 'take', 'u', 'time', 'u...   \n",
       "4            ['factsguide', 'society', 'motivation']   \n",
       "\n",
       "                     lemmatized_hashtags  n_mentions  \n",
       "0                                ['run']           1  \n",
       "1  ['lyft', 'disapointed', 'getthanked']           2  \n",
       "2                                     []           0  \n",
       "3                              ['model']           0  \n",
       "4                         ['motivation']           0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"lemmatized_tokens\", \"lemmatized_hashtags\", \"n_mentions\"]\n",
    "\n",
    "X = train_data[features]\n",
    "y = train_data.label\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "X_vec = tf.fit(X[\"lemmatized_tokens\"])\n",
    "X_lem = X_vec.transform(X[\"lemmatized_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 40632)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lem, y, test_size=0.2, random_state = 17, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train (25569, 40632)\n",
      "Shape X_test (6393, 40632)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape X_train {X_train.shape}\")\n",
    "print(f\"Shape X_test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_precision_score = []\n",
    "result_recall_score = []\n",
    "result_accuracy_score = []\n",
    "result_f1_score = []\n",
    "max_depth = 100\n",
    "\n",
    "for i in range(5, max_depth):\n",
    "    classifier = DecisionTreeClassifier(random_state=55, max_depth=i)\n",
    "\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    result_precision_score.append(metrics.precision_score(y_test, y_pred))\n",
    "    result_recall_score.append(metrics.recall_score(y_test, y_pred))\n",
    "    result_accuracy_score.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    result_f1_score.append(metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "#print(len([i for i in range(5, max_depth)]), len(result_precision_score),len(result_recall_score),len(result_accuracy_score),len(result_f1_score))\n",
    "\n",
    "\n",
    "result_data = {\n",
    "    \"max_depth\": [i for i in range(5, max_depth)],\n",
    "    \"precision_score\": result_precision_score,\n",
    "    \"recall_score\": result_recall_score,\n",
    "    \"accuracy_score\": result_accuracy_score,\n",
    "    \"f1_score\": result_f1_score\n",
    "}\n",
    "df = pd.DataFrame(result_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/decision_tree.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/decision_tree.ipynb#ch0000026?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/decision_tree.ipynb#ch0000026?line=2'>3</a>\u001b[0m pio\u001b[39m.\u001b[39mrenderers\u001b[39m.\u001b[39mdefault \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnotebook_connected\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/decision_tree.ipynb#ch0000026?line=4'>5</a>\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mline(df, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maccuracy_score\u001b[39m\u001b[39m\"\u001b[39m], title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLemmatisation results\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/decision_tree.ipynb#ch0000026?line=5'>6</a>\u001b[0m fig\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "fig = px.line(df, x=\"max_depth\", y= [\"f1_score\", \"accuracy_score\"], title=\"Lemmatisation results\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[1, 0.9278339764313276], [2, 0.9278339764313276], [3, 0.9296068411721764], [4, 0.9297111273334029], [5, 0.9311711335905726], [6, 0.9316925643967046], [7, 0.9320054228803838], [8, 0.9322139952028365], [9, 0.931275419751799], [10, 0.9327354260089686], [11, 0.932318281364063], [12, 0.9316925643967046], [13, 0.9298154134946293], [14, 0.9300239858170821], [15, 0.9292939826884973], [16, 0.9285639795599124], [17, 0.9292939826884973], [18, 0.9271039733027427], [19, 0.9248096777557618], [20, 0.9244968192720826], [21, 0.9206382313067056], [22, 0.9203253728230264], [23, 0.9161539263739702], [24, 0.9144853477943476], [25, 0.9147982062780269], [26, 0.911982479924914], [27, 0.9087496089268954], [28, 0.9072896026697257], [29, 0.9055167379288769], [30, 0.9042653039941599], [31, 0.9036395870268016], [32, 0.9040567316717072], [33, 0.9028052977369903], [34, 0.9028052977369903], [35, 0.903222442381896], [36, 0.9017624361247263], [37, 0.900928146834915], [38, 0.900928146834915], [39, 0.9011367191573678], [40, 0.9008238606736886], [41, 0.8987381374491605], [42, 0.8992595682552925], [43, 0.8989467097716133], [44, 0.8973824173532172], [45, 0.8958181249348212], [46, 0.8966524142246324], [47, 0.8961309834185004], [48, 0.8964438419021796], [49, 0.8957138387735948]]\n",
    "\n",
    "First run\n",
    "Best result: max detp: 10, 0.9327354260089686 \n",
    "\n",
    "secound run\n",
    "\n",
    "Dataset split: 0.9278339764313276"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/8f0zskws27x_14w3qmfndjfw0000gn/T/ipykernel_97800/292283133.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_up2 = train_data.drop(\"label\", 1)\n"
     ]
    }
   ],
   "source": [
    "X_up2 = train_data.drop(\"label\", 1)\n",
    "y_up2 = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_up2, X_test_up2, y_train_up2, y_test_up2 = train_test_split(X_up2, y_up2, test_size=0.2, random_state = 17, stratify=y_up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train (25569, 14)\n",
      "Shape X_test (6393, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape X_train {X_train_up2.shape}\")\n",
    "print(f\"Shape X_test {X_test_up2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concat tain and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_converted_emojis</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_punctuation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30405</th>\n",
       "      <td>0</td>\n",
       "      <td>30406</td>\n",
       "      <td>when everyone's free when you're in exam mode ...</td>\n",
       "      <td>when everyone's free when you're in exam mode ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['wtf', 'badtiming', 'guys']</td>\n",
       "      <td>when everyones free when youre in exam mode wt...</td>\n",
       "      <td>when everyones free when youre in exam mode wt...</td>\n",
       "      <td>['when', 'everyones', 'free', 'when', 'youre',...</td>\n",
       "      <td>['everyones', 'free', 'youre', 'exam', 'mode',...</td>\n",
       "      <td>['wtf', 'badtiming', 'guys']</td>\n",
       "      <td>['everyon', 'free', 'your', 'exam', 'mode', 'w...</td>\n",
       "      <td>['wtf', 'badtim', 'guy']</td>\n",
       "      <td>['everyones', 'free', 'youre', 'exam', 'mode',...</td>\n",
       "      <td>['wtf', 'badtiming', 'guy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27807</th>\n",
       "      <td>0</td>\n",
       "      <td>27808</td>\n",
       "      <td>#jacksonville   rooster simulation: i want to ...</td>\n",
       "      <td>#jacksonville   rooster simulation: i want to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['jacksonville']</td>\n",
       "      <td>jacksonville   rooster simulation i want to cl...</td>\n",
       "      <td>jacksonville   rooster simulation i want to cl...</td>\n",
       "      <td>['jacksonville', 'rooster', 'simulation', 'i',...</td>\n",
       "      <td>['jacksonville', 'rooster', 'simulation', 'wan...</td>\n",
       "      <td>['jacksonville']</td>\n",
       "      <td>['jacksonvil', 'rooster', 'simul', 'want', 'cl...</td>\n",
       "      <td>['jacksonvil']</td>\n",
       "      <td>['jacksonville', 'rooster', 'simulation', 'wan...</td>\n",
       "      <td>['jacksonville']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>0</td>\n",
       "      <td>8661</td>\n",
       "      <td>@user just run 10kms for @user @user   #lovei...</td>\n",
       "      <td>@user just run 10kms for @user @user   #lovei...</td>\n",
       "      <td>3</td>\n",
       "      <td>['loveisall']</td>\n",
       "      <td>user just run 10kms for user user   loveisall...</td>\n",
       "      <td>user just run 10kms for user user   loveisall...</td>\n",
       "      <td>['user', 'just', 'run', '10kms', 'for', 'user'...</td>\n",
       "      <td>['user', 'run', '10kms', 'user', 'user', 'love...</td>\n",
       "      <td>['loveisall']</td>\n",
       "      <td>['user', 'run', '10km', 'user', 'user', 'lovei...</td>\n",
       "      <td>['loveisal']</td>\n",
       "      <td>['user', 'run', '10kms', 'user', 'user', 'love...</td>\n",
       "      <td>['loveisall']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19185</th>\n",
       "      <td>0</td>\n",
       "      <td>19186</td>\n",
       "      <td>@user got the prototype for our new usb today...</td>\n",
       "      <td>@user got the prototype for our new usb today...</td>\n",
       "      <td>1</td>\n",
       "      <td>['new', 'doncasterphotographer', 'special']</td>\n",
       "      <td>user got the prototype for our new usb today ...</td>\n",
       "      <td>user got the prototype for our new usb today ...</td>\n",
       "      <td>['user', 'got', 'the', 'prototype', 'for', 'ou...</td>\n",
       "      <td>['user', 'got', 'prototype', 'new', 'usb', 'to...</td>\n",
       "      <td>['new', 'doncasterphotographer', 'special']</td>\n",
       "      <td>['user', 'got', 'prototyp', 'new', 'usb', 'tod...</td>\n",
       "      <td>['new', 'doncasterphotograph', 'special']</td>\n",
       "      <td>['user', 'got', 'prototype', 'new', 'usb', 'to...</td>\n",
       "      <td>['new', 'doncasterphotographer', 'special']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>0</td>\n",
       "      <td>10356</td>\n",
       "      <td>have a   &amp;amp; #healthy #fathersday. #runnerda...</td>\n",
       "      <td>have a   &amp;amp; #healthy #fathersday. #runnerda...</td>\n",
       "      <td>0</td>\n",
       "      <td>['healthy', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "      <td>have a   amp healthy fathersday runnerdad eath...</td>\n",
       "      <td>have a   amp healthy fathersday runnerdad eath...</td>\n",
       "      <td>['have', 'a', 'amp', 'healthy', 'fathersday', ...</td>\n",
       "      <td>['amp', 'healthy', 'fathersday', 'runnerdad', ...</td>\n",
       "      <td>['healthy', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "      <td>['amp', 'healthi', 'fathersday', 'runnerdad', ...</td>\n",
       "      <td>['healthi', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "      <td>['amp', 'healthy', 'fathersday', 'runnerdad', ...</td>\n",
       "      <td>['healthy', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label     id                                              tweet  \\\n",
       "30405      0  30406  when everyone's free when you're in exam mode ...   \n",
       "27807      0  27808  #jacksonville   rooster simulation: i want to ...   \n",
       "8660       0   8661   @user just run 10kms for @user @user   #lovei...   \n",
       "19185      0  19186   @user got the prototype for our new usb today...   \n",
       "10355      0  10356  have a   &amp; #healthy #fathersday. #runnerda...   \n",
       "\n",
       "                                  tweet_converted_emojis  n_mentions  \\\n",
       "30405  when everyone's free when you're in exam mode ...           0   \n",
       "27807  #jacksonville   rooster simulation: i want to ...           0   \n",
       "8660    @user just run 10kms for @user @user   #lovei...           3   \n",
       "19185   @user got the prototype for our new usb today...           1   \n",
       "10355  have a   &amp; #healthy #fathersday. #runnerda...           0   \n",
       "\n",
       "                                                hashtags  \\\n",
       "30405                       ['wtf', 'badtiming', 'guys']   \n",
       "27807                                   ['jacksonville']   \n",
       "8660                                       ['loveisall']   \n",
       "19185        ['new', 'doncasterphotographer', 'special']   \n",
       "10355  ['healthy', 'fathersday', 'runnerdad', 'eathea...   \n",
       "\n",
       "                                     without_punctuation  \\\n",
       "30405  when everyones free when youre in exam mode wt...   \n",
       "27807  jacksonville   rooster simulation i want to cl...   \n",
       "8660    user just run 10kms for user user   loveisall...   \n",
       "19185   user got the prototype for our new usb today ...   \n",
       "10355  have a   amp healthy fathersday runnerdad eath...   \n",
       "\n",
       "                                             tweet_lower  \\\n",
       "30405  when everyones free when youre in exam mode wt...   \n",
       "27807  jacksonville   rooster simulation i want to cl...   \n",
       "8660    user just run 10kms for user user   loveisall...   \n",
       "19185   user got the prototype for our new usb today ...   \n",
       "10355  have a   amp healthy fathersday runnerdad eath...   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "30405  ['when', 'everyones', 'free', 'when', 'youre',...   \n",
       "27807  ['jacksonville', 'rooster', 'simulation', 'i',...   \n",
       "8660   ['user', 'just', 'run', '10kms', 'for', 'user'...   \n",
       "19185  ['user', 'got', 'the', 'prototype', 'for', 'ou...   \n",
       "10355  ['have', 'a', 'amp', 'healthy', 'fathersday', ...   \n",
       "\n",
       "                                             clean_token  \\\n",
       "30405  ['everyones', 'free', 'youre', 'exam', 'mode',...   \n",
       "27807  ['jacksonville', 'rooster', 'simulation', 'wan...   \n",
       "8660   ['user', 'run', '10kms', 'user', 'user', 'love...   \n",
       "19185  ['user', 'got', 'prototype', 'new', 'usb', 'to...   \n",
       "10355  ['amp', 'healthy', 'fathersday', 'runnerdad', ...   \n",
       "\n",
       "                                          clean_hashtags  \\\n",
       "30405                       ['wtf', 'badtiming', 'guys']   \n",
       "27807                                   ['jacksonville']   \n",
       "8660                                       ['loveisall']   \n",
       "19185        ['new', 'doncasterphotographer', 'special']   \n",
       "10355  ['healthy', 'fathersday', 'runnerdad', 'eathea...   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "30405  ['everyon', 'free', 'your', 'exam', 'mode', 'w...   \n",
       "27807  ['jacksonvil', 'rooster', 'simul', 'want', 'cl...   \n",
       "8660   ['user', 'run', '10km', 'user', 'user', 'lovei...   \n",
       "19185  ['user', 'got', 'prototyp', 'new', 'usb', 'tod...   \n",
       "10355  ['amp', 'healthi', 'fathersday', 'runnerdad', ...   \n",
       "\n",
       "                                        stemmed_hashtags  \\\n",
       "30405                           ['wtf', 'badtim', 'guy']   \n",
       "27807                                     ['jacksonvil']   \n",
       "8660                                        ['loveisal']   \n",
       "19185          ['new', 'doncasterphotograph', 'special']   \n",
       "10355  ['healthi', 'fathersday', 'runnerdad', 'eathea...   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "30405  ['everyones', 'free', 'youre', 'exam', 'mode',...   \n",
       "27807  ['jacksonville', 'rooster', 'simulation', 'wan...   \n",
       "8660   ['user', 'run', '10kms', 'user', 'user', 'love...   \n",
       "19185  ['user', 'got', 'prototype', 'new', 'usb', 'to...   \n",
       "10355  ['amp', 'healthy', 'fathersday', 'runnerdad', ...   \n",
       "\n",
       "                                     lemmatized_hashtags  \n",
       "30405                        ['wtf', 'badtiming', 'guy']  \n",
       "27807                                   ['jacksonville']  \n",
       "8660                                       ['loveisall']  \n",
       "19185        ['new', 'doncasterphotographer', 'special']  \n",
       "10355  ['healthy', 'fathersday', 'runnerdad', 'eathea...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_up2 = pd.concat([y_train_up2,X_train_up2], axis=1)\n",
    "df_test_up2 = pd.concat([y_test_up2,X_test_up2], axis = 1)\n",
    "df_train_up2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate Minortity Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length majority 23775\n",
      "length minority 1794\n"
     ]
    }
   ],
   "source": [
    "data_minority = df_train_up2[df_train_up2.label == 1]\n",
    "data_majority = df_train_up2[df_train_up2.label == 0]\n",
    "print(\"length majority\", len(data_majority))\n",
    "print(\"length minority\", len(data_minority))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsample minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_minority = resample(data_minority, replace = True, n_samples=23775, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23775\n",
       "1    23775\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tarin_up2 = pd.concat([data_majority, data_minority])\n",
    "df_tarin_up2.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_converted_emojis</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_punctuation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30405</th>\n",
       "      <td>0</td>\n",
       "      <td>30406</td>\n",
       "      <td>when everyone's free when you're in exam mode ...</td>\n",
       "      <td>when everyone's free when you're in exam mode ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['wtf', 'badtiming', 'guys']</td>\n",
       "      <td>when everyones free when youre in exam mode wt...</td>\n",
       "      <td>when everyones free when youre in exam mode wt...</td>\n",
       "      <td>['when', 'everyones', 'free', 'when', 'youre',...</td>\n",
       "      <td>['everyones', 'free', 'youre', 'exam', 'mode',...</td>\n",
       "      <td>['wtf', 'badtiming', 'guys']</td>\n",
       "      <td>['everyon', 'free', 'your', 'exam', 'mode', 'w...</td>\n",
       "      <td>['wtf', 'badtim', 'guy']</td>\n",
       "      <td>['everyones', 'free', 'youre', 'exam', 'mode',...</td>\n",
       "      <td>['wtf', 'badtiming', 'guy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27807</th>\n",
       "      <td>0</td>\n",
       "      <td>27808</td>\n",
       "      <td>#jacksonville   rooster simulation: i want to ...</td>\n",
       "      <td>#jacksonville   rooster simulation: i want to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['jacksonville']</td>\n",
       "      <td>jacksonville   rooster simulation i want to cl...</td>\n",
       "      <td>jacksonville   rooster simulation i want to cl...</td>\n",
       "      <td>['jacksonville', 'rooster', 'simulation', 'i',...</td>\n",
       "      <td>['jacksonville', 'rooster', 'simulation', 'wan...</td>\n",
       "      <td>['jacksonville']</td>\n",
       "      <td>['jacksonvil', 'rooster', 'simul', 'want', 'cl...</td>\n",
       "      <td>['jacksonvil']</td>\n",
       "      <td>['jacksonville', 'rooster', 'simulation', 'wan...</td>\n",
       "      <td>['jacksonville']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>0</td>\n",
       "      <td>8661</td>\n",
       "      <td>@user just run 10kms for @user @user   #lovei...</td>\n",
       "      <td>@user just run 10kms for @user @user   #lovei...</td>\n",
       "      <td>3</td>\n",
       "      <td>['loveisall']</td>\n",
       "      <td>user just run 10kms for user user   loveisall...</td>\n",
       "      <td>user just run 10kms for user user   loveisall...</td>\n",
       "      <td>['user', 'just', 'run', '10kms', 'for', 'user'...</td>\n",
       "      <td>['user', 'run', '10kms', 'user', 'user', 'love...</td>\n",
       "      <td>['loveisall']</td>\n",
       "      <td>['user', 'run', '10km', 'user', 'user', 'lovei...</td>\n",
       "      <td>['loveisal']</td>\n",
       "      <td>['user', 'run', '10kms', 'user', 'user', 'love...</td>\n",
       "      <td>['loveisall']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19185</th>\n",
       "      <td>0</td>\n",
       "      <td>19186</td>\n",
       "      <td>@user got the prototype for our new usb today...</td>\n",
       "      <td>@user got the prototype for our new usb today...</td>\n",
       "      <td>1</td>\n",
       "      <td>['new', 'doncasterphotographer', 'special']</td>\n",
       "      <td>user got the prototype for our new usb today ...</td>\n",
       "      <td>user got the prototype for our new usb today ...</td>\n",
       "      <td>['user', 'got', 'the', 'prototype', 'for', 'ou...</td>\n",
       "      <td>['user', 'got', 'prototype', 'new', 'usb', 'to...</td>\n",
       "      <td>['new', 'doncasterphotographer', 'special']</td>\n",
       "      <td>['user', 'got', 'prototyp', 'new', 'usb', 'tod...</td>\n",
       "      <td>['new', 'doncasterphotograph', 'special']</td>\n",
       "      <td>['user', 'got', 'prototype', 'new', 'usb', 'to...</td>\n",
       "      <td>['new', 'doncasterphotographer', 'special']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>0</td>\n",
       "      <td>10356</td>\n",
       "      <td>have a   &amp;amp; #healthy #fathersday. #runnerda...</td>\n",
       "      <td>have a   &amp;amp; #healthy #fathersday. #runnerda...</td>\n",
       "      <td>0</td>\n",
       "      <td>['healthy', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "      <td>have a   amp healthy fathersday runnerdad eath...</td>\n",
       "      <td>have a   amp healthy fathersday runnerdad eath...</td>\n",
       "      <td>['have', 'a', 'amp', 'healthy', 'fathersday', ...</td>\n",
       "      <td>['amp', 'healthy', 'fathersday', 'runnerdad', ...</td>\n",
       "      <td>['healthy', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "      <td>['amp', 'healthi', 'fathersday', 'runnerdad', ...</td>\n",
       "      <td>['healthi', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "      <td>['amp', 'healthy', 'fathersday', 'runnerdad', ...</td>\n",
       "      <td>['healthy', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label     id                                              tweet  \\\n",
       "30405      0  30406  when everyone's free when you're in exam mode ...   \n",
       "27807      0  27808  #jacksonville   rooster simulation: i want to ...   \n",
       "8660       0   8661   @user just run 10kms for @user @user   #lovei...   \n",
       "19185      0  19186   @user got the prototype for our new usb today...   \n",
       "10355      0  10356  have a   &amp; #healthy #fathersday. #runnerda...   \n",
       "\n",
       "                                  tweet_converted_emojis  n_mentions  \\\n",
       "30405  when everyone's free when you're in exam mode ...           0   \n",
       "27807  #jacksonville   rooster simulation: i want to ...           0   \n",
       "8660    @user just run 10kms for @user @user   #lovei...           3   \n",
       "19185   @user got the prototype for our new usb today...           1   \n",
       "10355  have a   &amp; #healthy #fathersday. #runnerda...           0   \n",
       "\n",
       "                                                hashtags  \\\n",
       "30405                       ['wtf', 'badtiming', 'guys']   \n",
       "27807                                   ['jacksonville']   \n",
       "8660                                       ['loveisall']   \n",
       "19185        ['new', 'doncasterphotographer', 'special']   \n",
       "10355  ['healthy', 'fathersday', 'runnerdad', 'eathea...   \n",
       "\n",
       "                                     without_punctuation  \\\n",
       "30405  when everyones free when youre in exam mode wt...   \n",
       "27807  jacksonville   rooster simulation i want to cl...   \n",
       "8660    user just run 10kms for user user   loveisall...   \n",
       "19185   user got the prototype for our new usb today ...   \n",
       "10355  have a   amp healthy fathersday runnerdad eath...   \n",
       "\n",
       "                                             tweet_lower  \\\n",
       "30405  when everyones free when youre in exam mode wt...   \n",
       "27807  jacksonville   rooster simulation i want to cl...   \n",
       "8660    user just run 10kms for user user   loveisall...   \n",
       "19185   user got the prototype for our new usb today ...   \n",
       "10355  have a   amp healthy fathersday runnerdad eath...   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "30405  ['when', 'everyones', 'free', 'when', 'youre',...   \n",
       "27807  ['jacksonville', 'rooster', 'simulation', 'i',...   \n",
       "8660   ['user', 'just', 'run', '10kms', 'for', 'user'...   \n",
       "19185  ['user', 'got', 'the', 'prototype', 'for', 'ou...   \n",
       "10355  ['have', 'a', 'amp', 'healthy', 'fathersday', ...   \n",
       "\n",
       "                                             clean_token  \\\n",
       "30405  ['everyones', 'free', 'youre', 'exam', 'mode',...   \n",
       "27807  ['jacksonville', 'rooster', 'simulation', 'wan...   \n",
       "8660   ['user', 'run', '10kms', 'user', 'user', 'love...   \n",
       "19185  ['user', 'got', 'prototype', 'new', 'usb', 'to...   \n",
       "10355  ['amp', 'healthy', 'fathersday', 'runnerdad', ...   \n",
       "\n",
       "                                          clean_hashtags  \\\n",
       "30405                       ['wtf', 'badtiming', 'guys']   \n",
       "27807                                   ['jacksonville']   \n",
       "8660                                       ['loveisall']   \n",
       "19185        ['new', 'doncasterphotographer', 'special']   \n",
       "10355  ['healthy', 'fathersday', 'runnerdad', 'eathea...   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "30405  ['everyon', 'free', 'your', 'exam', 'mode', 'w...   \n",
       "27807  ['jacksonvil', 'rooster', 'simul', 'want', 'cl...   \n",
       "8660   ['user', 'run', '10km', 'user', 'user', 'lovei...   \n",
       "19185  ['user', 'got', 'prototyp', 'new', 'usb', 'tod...   \n",
       "10355  ['amp', 'healthi', 'fathersday', 'runnerdad', ...   \n",
       "\n",
       "                                        stemmed_hashtags  \\\n",
       "30405                           ['wtf', 'badtim', 'guy']   \n",
       "27807                                     ['jacksonvil']   \n",
       "8660                                        ['loveisal']   \n",
       "19185          ['new', 'doncasterphotograph', 'special']   \n",
       "10355  ['healthi', 'fathersday', 'runnerdad', 'eathea...   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "30405  ['everyones', 'free', 'youre', 'exam', 'mode',...   \n",
       "27807  ['jacksonville', 'rooster', 'simulation', 'wan...   \n",
       "8660   ['user', 'run', '10kms', 'user', 'user', 'love...   \n",
       "19185  ['user', 'got', 'prototype', 'new', 'usb', 'to...   \n",
       "10355  ['amp', 'healthy', 'fathersday', 'runnerdad', ...   \n",
       "\n",
       "                                     lemmatized_hashtags  \n",
       "30405                        ['wtf', 'badtiming', 'guy']  \n",
       "27807                                   ['jacksonville']  \n",
       "8660                                       ['loveisall']  \n",
       "19185        ['new', 'doncasterphotographer', 'special']  \n",
       "10355  ['healthy', 'fathersday', 'runnerdad', 'eathea...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tarin_up2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/8f0zskws27x_14w3qmfndjfw0000gn/T/ipykernel_97800/986097789.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train_up2 = df_tarin_up2.drop(\"label\", 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_up2 = df_tarin_up2.drop(\"label\", 1)\n",
    "y_train_up2 = df_tarin_up2.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>n_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30405</th>\n",
       "      <td>['everyones', 'free', 'youre', 'exam', 'mode',...</td>\n",
       "      <td>['wtf', 'badtiming', 'guy']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27807</th>\n",
       "      <td>['jacksonville', 'rooster', 'simulation', 'wan...</td>\n",
       "      <td>['jacksonville']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>['user', 'run', '10kms', 'user', 'user', 'love...</td>\n",
       "      <td>['loveisall']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19185</th>\n",
       "      <td>['user', 'got', 'prototype', 'new', 'usb', 'to...</td>\n",
       "      <td>['new', 'doncasterphotographer', 'special']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>['amp', 'healthy', 'fathersday', 'runnerdad', ...</td>\n",
       "      <td>['healthy', 'fathersday', 'runnerdad', 'eathea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       lemmatized_tokens  \\\n",
       "30405  ['everyones', 'free', 'youre', 'exam', 'mode',...   \n",
       "27807  ['jacksonville', 'rooster', 'simulation', 'wan...   \n",
       "8660   ['user', 'run', '10kms', 'user', 'user', 'love...   \n",
       "19185  ['user', 'got', 'prototype', 'new', 'usb', 'to...   \n",
       "10355  ['amp', 'healthy', 'fathersday', 'runnerdad', ...   \n",
       "\n",
       "                                     lemmatized_hashtags  n_mentions  \n",
       "30405                        ['wtf', 'badtiming', 'guy']           0  \n",
       "27807                                   ['jacksonville']           0  \n",
       "8660                                       ['loveisall']           3  \n",
       "19185        ['new', 'doncasterphotographer', 'special']           1  \n",
       "10355  ['healthy', 'fathersday', 'runnerdad', 'eathea...           0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"lemmatized_tokens\", \"lemmatized_hashtags\", \"n_mentions\"]\n",
    "\n",
    "X_train_up2 = X_train_up2[features]\n",
    "X_test_up2 = X_test_up2[features]\n",
    "\n",
    "X_train_up2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47550, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_up2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "X_vec_2 = tf.fit(X_train_up2[\"lemmatized_tokens\"])\n",
    "X_tarin_up2 = X_vec_2.transform(X_train_up2[\"lemmatized_tokens\"])\n",
    "X_test_up2 = X_vec_2.transform(X_test_up2[\"lemmatized_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47550, 35036)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tarin_up2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.130505</td>\n",
       "      <td>0.542411</td>\n",
       "      <td>0.714688</td>\n",
       "      <td>0.210390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.138979</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.721883</td>\n",
       "      <td>0.223581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.140513</td>\n",
       "      <td>0.611607</td>\n",
       "      <td>0.710621</td>\n",
       "      <td>0.228524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.145982</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.712185</td>\n",
       "      <td>0.237780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.152613</td>\n",
       "      <td>0.658482</td>\n",
       "      <td>0.719850</td>\n",
       "      <td>0.247795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>95</td>\n",
       "      <td>0.354430</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.893947</td>\n",
       "      <td>0.452342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>96</td>\n",
       "      <td>0.352273</td>\n",
       "      <td>0.622768</td>\n",
       "      <td>0.893321</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>97</td>\n",
       "      <td>0.353827</td>\n",
       "      <td>0.629464</td>\n",
       "      <td>0.893477</td>\n",
       "      <td>0.453012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>98</td>\n",
       "      <td>0.378049</td>\n",
       "      <td>0.622768</td>\n",
       "      <td>0.901768</td>\n",
       "      <td>0.470489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>99</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.613839</td>\n",
       "      <td>0.902550</td>\n",
       "      <td>0.468883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  precision_score  recall_score  accuracy_score  f1_score\n",
       "0           5         0.130505      0.542411        0.714688  0.210390\n",
       "1           6         0.138979      0.571429        0.721883  0.223581\n",
       "2           7         0.140513      0.611607        0.710621  0.228524\n",
       "3           8         0.145982      0.640625        0.712185  0.237780\n",
       "4           9         0.152613      0.658482        0.719850  0.247795\n",
       "..        ...              ...           ...             ...       ...\n",
       "90         95         0.354430      0.625000        0.893947  0.452342\n",
       "91         96         0.352273      0.622768        0.893321  0.450000\n",
       "92         97         0.353827      0.629464        0.893477  0.453012\n",
       "93         98         0.378049      0.622768        0.901768  0.470489\n",
       "94         99         0.379310      0.613839        0.902550  0.468883\n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_precision_score = []\n",
    "result_recall_score = []\n",
    "result_accuracy_score = []\n",
    "result_f1_score = []\n",
    "max_depth = 100\n",
    "\n",
    "for i in range(5, max_depth):\n",
    "    classifier = DecisionTreeClassifier(random_state=55, max_depth=i)\n",
    "\n",
    "    model = classifier.fit(X_tarin_up2, y_train_up2)\n",
    "\n",
    "    y_pred = model.predict(X_test_up2)\n",
    "    \n",
    "    result_precision_score.append(metrics.precision_score(y_test_up2, y_pred))\n",
    "    result_recall_score.append(metrics.recall_score(y_test_up2, y_pred))\n",
    "    result_accuracy_score.append(metrics.accuracy_score(y_test_up2, y_pred))\n",
    "    result_f1_score.append(metrics.f1_score(y_test_up2, y_pred))\n",
    "\n",
    "\n",
    "#print(len([i for i in range(5, max_depth)]), len(result_precision_score),len(result_recall_score),len(result_accuracy_score),len(result_f1_score))\n",
    "\n",
    "\n",
    "result_data = {\n",
    "    \"max_depth\": [i for i in range(5, max_depth)],\n",
    "    \"precision_score\": result_precision_score,\n",
    "    \"recall_score\": result_recall_score,\n",
    "    \"accuracy_score\": result_accuracy_score,\n",
    "    \"f1_score\": result_f1_score\n",
    "}\n",
    "df_upsampled = pd.DataFrame(result_data)\n",
    "df_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.11.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"e39be2ff-2c20-4b40-9deb-95bd4b4d86b8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e39be2ff-2c20-4b40-9deb-95bd4b4d86b8\")) {                    Plotly.newPlot(                        \"e39be2ff-2c20-4b40-9deb-95bd4b4d86b8\",                        [{\"hovertemplate\":\"variable=f1_score<br>max_depth=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1_score\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"f1_score\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[0.2103896103896104,0.22358078602620088,0.2285237698081735,0.23777961888980945,0.24779504409911804,0.2601975096607986,0.2696824706394085,0.2743207712532866,0.27296587926509186,0.27486910994764396,0.2763157894736842,0.2826282628262826,0.2867546654528903,0.28649885583524026,0.28703703703703703,0.29106223678053345,0.2953904045155221,0.29873179896665103,0.2991533396048918,0.300376647834275,0.2993389990557129,0.3031175059952039,0.30563514804202485,0.30614203454894434,0.30702179176755445,0.31167562286272593,0.3112420225822287,0.3088954056695992,0.3103953147877013,0.30899155489319424,0.306562193927522,0.31370608609599204,0.3120638085742772,0.3182511438739197,0.32310838445807766,0.32388663967611336,0.3267175572519084,0.32168550873586843,0.3305439330543933,0.3308900523560209,0.3343717549325026,0.3322784810126582,0.3349488422186322,0.33885622661678244,0.3399677245831092,0.3388918773534158,0.33890987587695626,0.3427959292983396,0.3424878836833603,0.3458646616541354,0.34001082837033025,0.34505137912385075,0.342292923752057,0.3449401523394995,0.3485838779956427,0.3459051724137931,0.3463203463203463,0.34653465346534656,0.34863387978142074,0.3489130434782609,0.3774294670846395,0.3786220218931101,0.384020618556701,0.3784135240572172,0.3838120104438642,0.3930712858094604,0.3917662682602922,0.3917662682602922,0.38653198653198656,0.3927125506072875,0.389945652173913,0.3907103825136612,0.3986206896551724,0.3936022253129347,0.40111034004163776,0.40497581202487903,0.40083507306889354,0.4,0.4033496161898115,0.40638002773925097,0.40611961057023643,0.4045584045584046,0.4082798001427551,0.4117647058823529,0.4260089686098655,0.42615723732549593,0.42771982116244417,0.4465863453815261,0.44479495268138797,0.45856798069187443,0.45234248788368336,0.45,0.45301204819277113,0.47048903878583476,0.46888320545609546],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=accuracy_score<br>max_depth=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"accuracy_score\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"accuracy_score\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[0.7146879399343031,0.7218833098701705,0.7106209917096825,0.7121852025653058,0.7198498357578602,0.7304864695760989,0.7373689973408415,0.7409666823087753,0.7400281557954013,0.7400281557954013,0.7419052088221493,0.7506647896136399,0.7548881589238229,0.7561395276083216,0.759111528234006,0.7630220553730643,0.765681213827624,0.7664633192554356,0.7669325825121226,0.767558266854372,0.7678711090254966,0.772720162677929,0.7725637415923666,0.7738151102768653,0.7761614265603003,0.7796026904426717,0.7805412169560457,0.77882058501486,0.7789770061004223,0.7824182699827937,0.7785077428437354,0.783043954325043,0.7841389019239794,0.7902393242609104,0.79289848271547,0.791021429688722,0.7930549038010324,0.7935241670577193,0.7997810104802128,0.8000938526513374,0.7994681683090881,0.8019709056780854,0.8068199593305178,0.806507117159393,0.8080713280150165,0.8077584858438918,0.8083841701861411,0.8080713280150165,0.8090098545283905,0.8094791177850774,0.8093226966995151,0.8105740653840138,0.8124511184107618,0.8116690129829501,0.8129203816674487,0.8101048021273267,0.8110433286407007,0.8141717503519474,0.8135460660096981,0.8126075394963241,0.8446738620366026,0.8490536524323479,0.8504614422024089,0.8504614422024089,0.852338495229157,0.8575003910527139,0.8567182856249023,0.8567182856249023,0.8575003910527139,0.8592210229938996,0.8595338651650243,0.8604723916783983,0.8636008133896449,0.8636008133896449,0.865008603159706,0.8653214453308306,0.8653214453308306,0.8634443923040825,0.8662599718442046,0.8661035507586423,0.8664163929297669,0.8692319724698889,0.8703269200688253,0.8717347098388862,0.8798686062881277,0.8778351321758173,0.8798686062881277,0.892225872047552,0.889879555764117,0.8947286094165493,0.8939465039887377,0.8933208196464884,0.8934772407320507,0.9017675582668544,0.902549663694666],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"max_depth\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Lemmatisation results upsampeled\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e39be2ff-2c20-4b40-9deb-95bd4b4d86b8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(df_upsampled, x=\"max_depth\", y= [\"f1_score\", \"accuracy_score\"], title=\"Lemmatisation results upsampeled\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>n_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['user', 'father', 'dysfunctional', 'selfish',...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['user', 'user', 'thanks', 'lyft', 'credit', '...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['bihday', 'majesty']</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['model', 'love', 'u', 'take', 'u', 'time', 'u...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['factsguide', 'society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   lemmatized_tokens  \\\n",
       "0  ['user', 'father', 'dysfunctional', 'selfish',...   \n",
       "1  ['user', 'user', 'thanks', 'lyft', 'credit', '...   \n",
       "2                              ['bihday', 'majesty']   \n",
       "3  ['model', 'love', 'u', 'take', 'u', 'time', 'u...   \n",
       "4            ['factsguide', 'society', 'motivation']   \n",
       "\n",
       "                     lemmatized_hashtags  n_mentions  \n",
       "0                                ['run']           1  \n",
       "1  ['lyft', 'disapointed', 'getthanked']           2  \n",
       "2                                     []           0  \n",
       "3                              ['model']           0  \n",
       "4                         ['motivation']           0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 130\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [i for i in range(5, max_depth, 5)]\n",
    "\n",
    "#min_samples_split\n",
    "#min_samples_leaf\n",
    "#min_weight_fraction_leaf\n",
    "#max_featuresint, float or {“auto”, “sqrt”, “log2”}, default=None\n",
    "\n",
    "\n",
    "parameters = dict(dec_tree__criterion=criterion,\n",
    "                  dec_tree__max_depth=max_depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "pipe = Pipeline(steps=[('dec_tree', dec_tree)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Criterion: gini\n",
      "Best max_depth: 55\n",
      "DecisionTreeClassifier(max_depth=55, random_state=55)\n"
     ]
    }
   ],
   "source": [
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(X_train, y_train)\n",
    "\n",
    "print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print(clf_GS.best_estimator_.get_params()['dec_tree'])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947cd5ef72fa4485f7ecf5a654ef12bcb7ac0faec018370a70389fc4010d0179"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
