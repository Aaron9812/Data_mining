{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "  \n",
    "# setting path\n",
    "sys.path.append('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/../data/preprocessing.py:19: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aaronsteiner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aaronsteiner/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aaronsteiner/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import preprocessing as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:03:46.952073Z",
     "iopub.status.busy": "2022-05-17T15:03:46.951597Z",
     "iopub.status.idle": "2022-05-17T15:04:31.200885Z",
     "shell.execute_reply": "2022-05-17T15:04:31.200049Z",
     "shell.execute_reply.started": "2022-05-17T15:03:46.952034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=False, do_stem=False, do_lem=True, upsample=True, do_emojis=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:04:31.202249Z",
     "iopub.status.busy": "2022-05-17T15:04:31.202071Z",
     "iopub.status.idle": "2022-05-17T15:04:31.219187Z",
     "shell.execute_reply": "2022-05-17T15:04:31.218446Z",
     "shell.execute_reply.started": "2022-05-17T15:04:31.202231Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30405</th>\n",
       "      <td>0</td>\n",
       "      <td>when everyone's free when you're in exam mode ...</td>\n",
       "      <td>[when, everyones, free, when, youre, in, exam,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27807</th>\n",
       "      <td>0</td>\n",
       "      <td>#jacksonville   rooster simulation: i want to ...</td>\n",
       "      <td>[jacksonville, rooster, simulation, i, want, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>0</td>\n",
       "      <td>@user just run 10kms for @user @user   #loveis...</td>\n",
       "      <td>[user, just, run, 10kms, for, user, user, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19185</th>\n",
       "      <td>0</td>\n",
       "      <td>@user got the prototype for our new usb today!...</td>\n",
       "      <td>[user, got, the, prototype, for, our, new, usb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>0</td>\n",
       "      <td>have a   &amp;amp; #healthy #fathersday. #runnerda...</td>\n",
       "      <td>[have, a, amp, healthy, fathersday, runnerdad,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet  \\\n",
       "30405      0  when everyone's free when you're in exam mode ...   \n",
       "27807      0  #jacksonville   rooster simulation: i want to ...   \n",
       "8660       0  @user just run 10kms for @user @user   #loveis...   \n",
       "19185      0  @user got the prototype for our new usb today!...   \n",
       "10355      0  have a   &amp; #healthy #fathersday. #runnerda...   \n",
       "\n",
       "                                            preprocessed  \n",
       "30405  [when, everyones, free, when, youre, in, exam,...  \n",
       "27807  [jacksonville, rooster, simulation, i, want, t...  \n",
       "8660   [user, just, run, 10kms, for, user, user, love...  \n",
       "19185  [user, got, the, prototype, for, our, new, usb...  \n",
       "10355  [have, a, amp, healthy, fathersday, runnerdad,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(df_train: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "    tree = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "    pipe = Pipeline(steps=[('dec_tree', tree)])\n",
    "\n",
    "    Xt_train = tfidf.transform(df_train['preprocessed'])\n",
    "    y_train = df_train['label']\n",
    "    \n",
    "    criterion = ['gini', 'entropy']\n",
    "    max_depth = [i for i in range(200, 400, 20)] #-> tried 100 - 280 but not a single model used values bellow 200\n",
    "    #Some models used 280 therefore boost to 400 was tried \n",
    "    min_samples_split = [i for i in range(2, 20, 2)]\n",
    "    min_samples_leaf = [i for i in range(1, 5)]\n",
    "    #min_samples_leaf = [i for i in range(1, 10)] -> was tried but all models used 1 or 2\n",
    "    class_weight = [None] #-> balanced yields f1 bellow .50\n",
    "\n",
    "    parameters = dict(dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth, dec_tree__min_samples_split=min_samples_split,\n",
    "                      dec_tree__min_samples_leaf=min_samples_leaf, dec_tree__class_weight=class_weight)\n",
    "\n",
    "    dec_tree = GridSearchCV(pipe, param_grid=parameters, scoring='f1', n_jobs=-1)\n",
    "    dec_tree.fit(Xt_train, y_train)\n",
    "\n",
    "    return dec_tree.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model, df_test: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "    Xt_test = tfidf.transform(df_test['preprocessed'])\n",
    "    y_test = df_test['label']\n",
    "    y_pred = model.predict(Xt_test)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    predictions.append(model.get_params())\n",
    "    predictions.append(f\"Precision: {metrics.precision_score(y_test, y_pred)}\")\n",
    "    predictions.append(f\"Recall: {metrics.recall_score(y_test, y_pred)}\")\n",
    "    predictions.append(f\"Accuracy: {metrics.accuracy_score(y_test, y_pred)}\")\n",
    "    predictions.append(f\"F1: {metrics.f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 475.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000016?line=0'>1</a>\u001b[0m tfidf, df_train, df_test \u001b[39m=\u001b[39m pre\u001b[39m.\u001b[39msetup(rem_stop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, do_stem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, do_lem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, upsample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, do_emojis\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000016?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(df_train, tfidf)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000016?line=2'>3</a>\u001b[0m results\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mOnly Tokenization \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000016?line=3'>4</a>\u001b[0m results\u001b[39m.\u001b[39mappend(test_model(model, df_test, tfidf))\n",
      "\u001b[1;32m/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(df_train, tfidf)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000010?line=16'>17</a>\u001b[0m parameters \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dec_tree__criterion\u001b[39m=\u001b[39mcriterion,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000010?line=17'>18</a>\u001b[0m                   dec_tree__max_depth\u001b[39m=\u001b[39mmax_depth, dec_tree__min_samples_split\u001b[39m=\u001b[39mmin_samples_split,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000010?line=18'>19</a>\u001b[0m                   dec_tree__min_samples_leaf\u001b[39m=\u001b[39mmin_samples_leaf, dec_tree__class_weight\u001b[39m=\u001b[39mclass_weight)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000010?line=20'>21</a>\u001b[0m dec_tree \u001b[39m=\u001b[39m GridSearchCV(pipe, param_grid\u001b[39m=\u001b[39mparameters, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000010?line=21'>22</a>\u001b[0m dec_tree\u001b[39m.\u001b[39;49mfit(Xt_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000010?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dec_tree\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=884'>885</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=885'>886</a>\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=886'>887</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=888'>889</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=890'>891</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=892'>893</a>\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=893'>894</a>\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=894'>895</a>\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=1389'>1390</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=1390'>1391</a>\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=1391'>1392</a>\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=829'>830</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=830'>831</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=831'>832</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=832'>833</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=833'>834</a>\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=834'>835</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=835'>836</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=837'>838</a>\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=838'>839</a>\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=839'>840</a>\u001b[0m         clone(base_estimator),\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=840'>841</a>\u001b[0m         X,\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=841'>842</a>\u001b[0m         y,\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=842'>843</a>\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=843'>844</a>\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=844'>845</a>\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=845'>846</a>\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=846'>847</a>\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=847'>848</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=848'>849</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=849'>850</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=850'>851</a>\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=851'>852</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=852'>853</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=854'>855</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=855'>856</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=856'>857</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=857'>858</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=858'>859</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/sklearn/model_selection/_search.py?line=859'>860</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=1052'>1053</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=1054'>1055</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=1055'>1056</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=1056'>1057</a>\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=1057'>1058</a>\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=932'>933</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=933'>934</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=934'>935</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=935'>936</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/parallel.py?line=936'>937</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/_parallel_backends.py?line=538'>539</a>\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/_parallel_backends.py?line=539'>540</a>\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/_parallel_backends.py?line=540'>541</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/_parallel_backends.py?line=541'>542</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/_parallel_backends.py?line=542'>543</a>\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/joblib/_parallel_backends.py?line=543'>544</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py?line=437'>438</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py?line=438'>439</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py?line=440'>441</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py?line=442'>443</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py?line=443'>444</a>\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py?line=309'>310</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py?line=310'>311</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py?line=311'>312</a>\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py?line=312'>313</a>\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Cellar/python%403.9/3.9.13/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py?line=313'>314</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=False, do_stem=False, do_lem=False, upsample=False, do_emojis=False)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"Only Tokenization \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 283.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=False, do_lem=False, upsample=False, do_emojis=False)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nRemove Stopwords \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 358.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=False, do_lem=False, upsample=False, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nEmojis \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 467.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=True, do_lem=False, upsample=False, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nStemming \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 160.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n",
      "There is 47550 training data, of which 50.0% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000024?line=0'>1</a>\u001b[0m tfidf, df_train, df_test \u001b[39m=\u001b[39m pre\u001b[39m.\u001b[39msetup(rem_stop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, do_stem\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, do_lem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, upsample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, do_emojis\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000024?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(df_train, tfidf)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000024?line=2'>3</a>\u001b[0m results\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mUpsampling \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/final_decision_tree.ipynb#ch0000024?line=3'>4</a>\u001b[0m results\u001b[39m.\u001b[39mappend(test_model(model, df_test, tfidf))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=True, do_lem=False, upsample=True, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nUpsampling \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-but-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 307.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=False, do_lem=False, upsample=True, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nAll-but-Stemming \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"results_descision_tree.txt\", \"w\")\n",
    "for element in results:\n",
    "    if not isinstance(element, str):\n",
    "        for subelement in element:\n",
    "            textfile.write(str(subelement) + \"\\n\")\n",
    "        continue\n",
    "    textfile.write(str(element) + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Tokenization \n",
    "\n",
    "{'memory': None, 'steps': [('dec_tree', DecisionTreeClassifier(max_depth=240, min_samples_split=16, random_state=55))], 'verbose': False, 'dec_tree': DecisionTreeClassifier(max_depth=240, min_samples_split=16, random_state=55), 'dec_tree__ccp_alpha': 0.0, 'dec_tree__class_weight': None, 'dec_tree__criterion': 'gini', 'dec_tree__max_depth': 240, 'dec_tree__max_features': None, 'dec_tree__max_leaf_nodes': None, 'dec_tree__min_impurity_decrease': 0.0, 'dec_tree__min_samples_leaf': 1, 'dec_tree__min_samples_split': 16, 'dec_tree__min_weight_fraction_leaf': 0.0, 'dec_tree__random_state': 55, 'dec_tree__splitter': 'best'}\n",
    "Precision: 0.6869565217391305\n",
    "Recall: 0.5290178571428571\n",
    "Accuracy: 0.9501016737056155\n",
    "F1: 0.5977301387137451\n",
    "\n",
    "\n",
    "Remove Stopwords \n",
    "\n",
    "{'memory': None, 'steps': [('dec_tree', DecisionTreeClassifier(max_depth=300, min_samples_split=6, random_state=55))], 'verbose': False, 'dec_tree': DecisionTreeClassifier(max_depth=300, min_samples_split=6, random_state=55), 'dec_tree__ccp_alpha': 0.0, 'dec_tree__class_weight': None, 'dec_tree__criterion': 'gini', 'dec_tree__max_depth': 300, 'dec_tree__max_features': None, 'dec_tree__max_leaf_nodes': None, 'dec_tree__min_impurity_decrease': 0.0, 'dec_tree__min_samples_leaf': 1, 'dec_tree__min_samples_split': 6, 'dec_tree__min_weight_fraction_leaf': 0.0, 'dec_tree__random_state': 55, 'dec_tree__splitter': 'best'}\n",
    "Precision: 0.6565934065934066\n",
    "Recall: 0.5334821428571429\n",
    "Accuracy: 0.9477553574221805\n",
    "F1: 0.5886699507389163\n",
    "\n",
    "\n",
    "Emojis \n",
    "\n",
    "{'memory': None, 'steps': [('dec_tree', DecisionTreeClassifier(max_depth=260, random_state=55))], 'verbose': False, 'dec_tree': DecisionTreeClassifier(max_depth=260, random_state=55), 'dec_tree__ccp_alpha': 0.0, 'dec_tree__class_weight': None, 'dec_tree__criterion': 'gini', 'dec_tree__max_depth': 260, 'dec_tree__max_features': None, 'dec_tree__max_leaf_nodes': None, 'dec_tree__min_impurity_decrease': 0.0, 'dec_tree__min_samples_leaf': 1, 'dec_tree__min_samples_split': 2, 'dec_tree__min_weight_fraction_leaf': 0.0, 'dec_tree__random_state': 55, 'dec_tree__splitter': 'best'}\n",
    "Precision: 0.6565934065934066\n",
    "Recall: 0.5334821428571429\n",
    "Accuracy: 0.9477553574221805\n",
    "F1: 0.5886699507389163\n",
    "\n",
    "\n",
    "Stemming \n",
    "\n",
    "{'memory': None, 'steps': [('dec_tree', DecisionTreeClassifier(max_depth=240, min_samples_split=4, random_state=55))], 'verbose': False, 'dec_tree': DecisionTreeClassifier(max_depth=240, min_samples_split=4, random_state=55), 'dec_tree__ccp_alpha': 0.0, 'dec_tree__class_weight': None, 'dec_tree__criterion': 'gini', 'dec_tree__max_depth': 240, 'dec_tree__max_features': None, 'dec_tree__max_leaf_nodes': None, 'dec_tree__min_impurity_decrease': 0.0, 'dec_tree__min_samples_leaf': 1, 'dec_tree__min_samples_split': 4, 'dec_tree__min_weight_fraction_leaf': 0.0, 'dec_tree__random_state': 55, 'dec_tree__splitter': 'best'}\n",
    "Precision: 0.6887052341597796\n",
    "Recall: 0.5580357142857143\n",
    "Accuracy: 0.9513530423901142\n",
    "F1: 0.6165228113440198\n",
    "\n",
    "\n",
    "Upsampling \n",
    "\n",
    "{'memory': None, 'steps': [('dec_tree', DecisionTreeClassifier(criterion='entropy', max_depth=220, min_samples_leaf=2,\n",
    "                       min_samples_split=6, random_state=55))], 'verbose': False, 'dec_tree': DecisionTreeClassifier(criterion='entropy', max_depth=220, min_samples_leaf=2,\n",
    "                       min_samples_split=6, random_state=55), 'dec_tree__ccp_alpha': 0.0, 'dec_tree__class_weight': None, 'dec_tree__criterion': 'entropy', 'dec_tree__max_depth': 220, 'dec_tree__max_features': None, 'dec_tree__max_leaf_nodes': None, 'dec_tree__min_impurity_decrease': 0.0, 'dec_tree__min_samples_leaf': 2, 'dec_tree__min_samples_split': 6, 'dec_tree__min_weight_fraction_leaf': 0.0, 'dec_tree__random_state': 55, 'dec_tree__splitter': 'best'}\n",
    "Precision: 0.6130790190735694\n",
    "Recall: 0.5022321428571429\n",
    "Accuracy: 0.9429063037697482\n",
    "F1: 0.5521472392638037\n",
    "\n",
    "\n",
    "All-but-Stemming \n",
    "\n",
    "{'memory': None, 'steps': [('dec_tree', DecisionTreeClassifier(criterion='entropy', max_depth=220, min_samples_leaf=2,\n",
    "                       min_samples_split=6, random_state=55))], 'verbose': False, 'dec_tree': DecisionTreeClassifier(criterion='entropy', max_depth=220, min_samples_leaf=2,\n",
    "                       min_samples_split=6, random_state=55), 'dec_tree__ccp_alpha': 0.0, 'dec_tree__class_weight': None, 'dec_tree__criterion': 'entropy', 'dec_tree__max_depth': 220, 'dec_tree__max_features': None, 'dec_tree__max_leaf_nodes': None, 'dec_tree__min_impurity_decrease': 0.0, 'dec_tree__min_samples_leaf': 2, 'dec_tree__min_samples_split': 6, 'dec_tree__min_weight_fraction_leaf': 0.0, 'dec_tree__random_state': 55, 'dec_tree__splitter': 'best'}\n",
    "Precision: 0.6179104477611941\n",
    "Recall: 0.46205357142857145\n",
    "Accuracy: 0.9422806194274989\n",
    "F1: 0.5287356321839082\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947cd5ef72fa4485f7ecf5a654ef12bcb7ac0faec018370a70389fc4010d0179"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
