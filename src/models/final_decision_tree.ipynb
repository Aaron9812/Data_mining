{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "  \n",
    "# setting path\n",
    "sys.path.append('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/aaronsteiner/Documents/GitHub/Data_mining/src/models/../data/preprocessing.py:19: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aaronsteiner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aaronsteiner/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aaronsteiner/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import preprocessing as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:03:46.952073Z",
     "iopub.status.busy": "2022-05-17T15:03:46.951597Z",
     "iopub.status.idle": "2022-05-17T15:04:31.200885Z",
     "shell.execute_reply": "2022-05-17T15:04:31.200049Z",
     "shell.execute_reply.started": "2022-05-17T15:03:46.952034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/Users/aaronsteiner/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 484.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 25569 training data, of which 7.02% is hate speech \n",
      "There is 6393 test data, of which 7.01% is hate speech \n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=False, do_stem=False, do_lem=True, upsample=True, do_emojis=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-17T15:04:31.202249Z",
     "iopub.status.busy": "2022-05-17T15:04:31.202071Z",
     "iopub.status.idle": "2022-05-17T15:04:31.219187Z",
     "shell.execute_reply": "2022-05-17T15:04:31.218446Z",
     "shell.execute_reply.started": "2022-05-17T15:04:31.202231Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30405</th>\n",
       "      <td>0</td>\n",
       "      <td>when everyone's free when you're in exam mode ...</td>\n",
       "      <td>[when, everyones, free, when, youre, in, exam,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27807</th>\n",
       "      <td>0</td>\n",
       "      <td>#jacksonville   rooster simulation: i want to ...</td>\n",
       "      <td>[jacksonville, rooster, simulation, i, want, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>0</td>\n",
       "      <td>@user just run 10kms for @user @user   #loveis...</td>\n",
       "      <td>[user, just, run, 10kms, for, user, user, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19185</th>\n",
       "      <td>0</td>\n",
       "      <td>@user got the prototype for our new usb today!...</td>\n",
       "      <td>[user, got, the, prototype, for, our, new, usb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>0</td>\n",
       "      <td>have a   &amp;amp; #healthy #fathersday. #runnerda...</td>\n",
       "      <td>[have, a, amp, healthy, fathersday, runnerdad,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet  \\\n",
       "30405      0  when everyone's free when you're in exam mode ...   \n",
       "27807      0  #jacksonville   rooster simulation: i want to ...   \n",
       "8660       0  @user just run 10kms for @user @user   #loveis...   \n",
       "19185      0  @user got the prototype for our new usb today!...   \n",
       "10355      0  have a   &amp; #healthy #fathersday. #runnerda...   \n",
       "\n",
       "                                            preprocessed  \n",
       "30405  [when, everyones, free, when, youre, in, exam,...  \n",
       "27807  [jacksonville, rooster, simulation, i, want, t...  \n",
       "8660   [user, just, run, 10kms, for, user, user, love...  \n",
       "19185  [user, got, the, prototype, for, our, new, usb...  \n",
       "10355  [have, a, amp, healthy, fathersday, runnerdad,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(df_train: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "    tree = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "    pipe = Pipeline(steps=[('dec_tree', tree)])\n",
    "\n",
    "    Xt_train = tfidf.transform(df_train['preprocessed'])\n",
    "    y_train = df_train['label']\n",
    "    \n",
    "    criterion = ['gini', 'entropy']\n",
    "    max_depth = [i for i in range(200, 400, 20)] #-> tried 100 - 280 but not a single model used values bellow 200\n",
    "    #Some models used 280 therefore boost to 400 was tried \n",
    "    min_samples_split = [i for i in range(2, 20, 2)]\n",
    "    min_samples_leaf = [i for i in range(1, 3)]\n",
    "    #min_samples_leaf = [i for i in range(1, 10)] -> was tried but all models used 1 or 2\n",
    "    class_weight = [None] #-> balanced yields f1 bellow .50\n",
    "\n",
    "    parameters = dict(dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth, dec_tree__min_samples_split=min_samples_split,\n",
    "                      dec_tree__min_samples_leaf=min_samples_leaf, dec_tree__class_weight=class_weight)\n",
    "\n",
    "    dec_tree = GridSearchCV(pipe, param_grid=parameters, scoring='f1', n_jobs=-1)\n",
    "    dec_tree.fit(Xt_train, y_train)\n",
    "\n",
    "    return dec_tree.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model, df_test: pd.DataFrame, tfidf: TfidfVectorizer):\n",
    "    Xt_test = tfidf.transform(df_test['preprocessed'])\n",
    "    y_test = df_test['label']\n",
    "    y_pred = model.predict(Xt_test)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    predictions.append(model.get_params())\n",
    "    predictions.append(metrics.precision_score(y_test, y_pred))\n",
    "    predictions.append(metrics.recall_score(y_test, y_pred))\n",
    "    predictions.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    predictions.append(metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/jovyan/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 437.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=False, do_stem=False, do_lem=False, upsample=False, do_emojis=False)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"Only Tokenization \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))\n",
    "results.append(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/jovyan/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 668.52it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=False, do_lem=False, upsample=False, do_emojis=False)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nRemove Stopwords \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))\n",
    "results.append(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/jovyan/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 783.25it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=False, do_lem=False, upsample=False, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nEmojis \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))\n",
    "results.append(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/jovyan/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 714.65it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=True, do_lem=False, upsample=False, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nStemming \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))\n",
    "results.append(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/jovyan/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 711.86it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=True, do_lem=False, upsample=True, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nUpsampling \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))\n",
    "results.append(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-but-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/jovyan/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 718.33it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf, df_train, df_test = pre.setup(rem_stop=True, do_stem=False, do_lem=False, upsample=True, do_emojis=True)\n",
    "model = train_model(df_train, tfidf)\n",
    "results.append(\"\\n\\nAll-but-Stemming \\n\")\n",
    "results.append(test_model(model, df_test, tfidf))\n",
    "results.append(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"results_descision_tree.txt\", \"w\")\n",
    "for element in results:\n",
    "    if not isinstance(element, str):\n",
    "        for subelement in element:\n",
    "            textfile.write(str(subelement) + \"\\n\")\n",
    "        continue\n",
    "    textfile.write(str(element) + \"\\n\")\n",
    "textfile.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947cd5ef72fa4485f7ecf5a654ef12bcb7ac0faec018370a70389fc4010d0179"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
