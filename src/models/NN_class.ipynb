{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_class.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZLiVNl9hwO_",
        "outputId": "74191766-a121-44d0-8bce-6ad6d56ead6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-1.12.0-cp37-cp37m-manylinux2014_x86_64.whl (53.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 53.2 MB 160 kB/s \n",
            "\u001b[?25hCollecting virtualenv\n",
            "  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 24.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.6.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Collecting frozenlist\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.8 MB/s \n",
            "\u001b[?25hCollecting grpcio<=1.43.0,>=1.28.1\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 31.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 59.7 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: platformdirs, frozenlist, distlib, virtualenv, grpcio, aiosignal, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.44.0\n",
            "    Uninstalling grpcio-1.44.0:\n",
            "      Successfully uninstalled grpcio-1.44.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed aiosignal-1.2.0 distlib-0.3.4 frozenlist-1.3.0 grpcio-1.43.0 platformdirs-2.5.2 ray-1.12.0 virtualenv-20.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5maJBJvcdClB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from torch.optim import AdamW\n",
        "import itertools\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import csv\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import os.path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Basic Neural Network Model***"
      ],
      "metadata": {
        "id": "fH2Rz0y3dVWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, num_layers, max_layer_size, drop_out):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.num_layers = num_layers\n",
        "        self.max_layer_size = max_layer_size\n",
        "        in_size = max_layer_size\n",
        "\n",
        "        self.layers = [nn.Linear(input_size, in_size)]\n",
        "        for layer in range(1, num_layers-1):\n",
        "            self.layers.append(nn.Linear(in_size, int(in_size/2)))\n",
        "            in_size = int(in_size/2)\n",
        "        self.layers.append(nn.Linear(in_size, 1))\n",
        "        self.drop_out = nn.Dropout(drop_out)\n",
        "\n",
        "\n",
        "    def forward(self, x, num_layers):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(self.drop_out(layer(x)))\n",
        "        x = self.layers[-1]\n",
        "        return x"
      ],
      "metadata": {
        "id": "fm2qVSD2dJSQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***DataSet Loader***"
      ],
      "metadata": {
        "id": "4Po55NlbdbxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data\n",
        "class HatespeechDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        with open('filename', newline='') as csvfile:\n",
        "            data_reader = csv.reader(csvfile, delimiter=';')\n",
        "        \n",
        "            self.data = []      \n",
        "            for line in data_reader:\n",
        "                #need to know the format first\n",
        "                pass   \n",
        "                \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        ids = self.data[idx][0:3]\n",
        "        representations = self.data[idx][3:-1]\n",
        "        label = self.data[idx][-1]\n",
        "        return ids, representations, label\n"
      ],
      "metadata": {
        "id": "SkczlSvqdLsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Hyperparameter Optimisation***"
      ],
      "metadata": {
        "id": "akjmOmmZfQve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"lr\": tune.loguniform(1e-3, 1e-1),\n",
        "    \"batch_size\": tune.choice([4, 8, 12, 16]),\n",
        "    \"num_layers\": tune.choice([2, 3, 4]),\n",
        "    \"drop_out\" : tune.uniform(0.1, 0.8),\n",
        "    \"max_layer_size\": tune.uniform(80, 512),\n",
        "}\n",
        "\n",
        "scheduler = ASHAScheduler(\n",
        "    metric=\"accuracy\",\n",
        "    mode=\"min\",\n",
        "    max_t=10000, #No time restrictions\n",
        "    grace_period=2, \n",
        "    reduction_factor=2) \n",
        "\n",
        "reporter = CLIReporter(\n",
        "    parameter_columns=[\"lr\", \"batch_size\", \"num_layers\", \"drop_out\", \"max_layer_size\"],\n",
        "    metric_columns=[\"accuracy\", \"training_iteration\"])"
      ],
      "metadata": {
        "id": "gHfpY64bfQNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Train***"
      ],
      "metadata": {
        "id": "-QCe3iDXdf-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config, filename, epochs=2):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = Net(input_size, config[\"num_layers\"], config[\"max_layer_size\"], config[\"drop_out\"])\n",
        "\n",
        "    train_dataset = HatespeechDataset(\"220502_train_data_preprocessed.csv\")\n",
        "    dev_dataset = HatespeechDataset(\"220502_dev_data_preprocessed.csv\")\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    dev_dataloader = DataLoader(dev_dataset, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    vector_size = train_dataset.data[0][3].size()[-1] #Get size of representations\n",
        "    net = Net(vector_size).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss() \n",
        "    optimizer = AdamW(net.parameters())\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_dataloader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "                running_loss = 0.0\n",
        "        \n",
        "        net.eval()\n",
        "        preds = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for i, data in enumerate(dev_dataloader):\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                preds.extend([round(output.item()) for output in list(outputs)])\n",
        "                preds.extend([label.item() for label in list(labels)])\n",
        "\n",
        "            # sklearn.accuracy...\n",
        "            accuracy = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "\n",
        "            # log evaluation \n",
        "\n",
        "            with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "                path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "                torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "                        \n",
        "            tune.report(accuracy=...)\n",
        "    \n",
        "    torch.save(net.state_dict(), PATH=f'./nn_classification.model')\n",
        "\n"
      ],
      "metadata": {
        "id": "i4aZRuiOdNgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Test***"
      ],
      "metadata": {
        "id": "UGIXOVftdiTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(representation, modelfile):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_dataset = HatespeechDataset(\"220502_test_data_preprocessed.csv\")\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    vector_size = test_dataset.data[0][3].size()[-1] #Get size of representations\n",
        "    net = Net(vector_size)\n",
        "    net.load_state_dict(torch.load(modelfile))\n",
        "\n",
        "\n",
        "    net.eval()\n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_dataloader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            preds.extend([output.item() for output in list(outputs)])\n",
        "            preds.extend([label.item() for label in list(labels)])"
      ],
      "metadata": {
        "id": "4LLqyZLudQyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}