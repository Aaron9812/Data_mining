{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978e6c9a",
   "metadata": {},
   "source": [
    "# Classifing hate speech in tweets using Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cf6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dd58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'label', 'tweet', 'n_mentions', 'hashtags', 'without_puctioation',\n",
      "       'tweet_lower', 'tweet_token', 'clean_token', 'clean_hashtags',\n",
      "       'stemmed_tokens', 'stemmed_hashtags', 'lemmatized_tokens',\n",
      "       'lemmatized_hashtags', 'tfidf_stemmed_tokens', 'tfidf_stemmed_hashtags',\n",
      "       'tfidf_lemmatized_tokens', 'tfidf_lemmatized_hashtags'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Downloading the csv file from GitHub\n",
    "url = \"https://raw.githubusercontent.com/Aaron9812/Data_mining/main/data/220505_train_data_preprocessed.csv\"\n",
    "download = requests.get(url).content\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')), sep=\";\")\n",
    "\n",
    "# Printing out the first row of the dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a45a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "\"tweet\",\n",
    "\"hashtags\",\n",
    "\"without_puctioation\",\n",
    "\"tweet_lower\",\n",
    "\"tweet_token\",\n",
    "\"clean_token\",\n",
    "\"clean_hashtags\",\n",
    "\"stemmed_tokens\",\n",
    "\"stemmed_hashtags\",\n",
    "\"lemmatized_tokens\",\n",
    "\"lemmatized_hashtags\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade0cad",
   "metadata": {},
   "source": [
    "### only text features used (so far); no numerical features included!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00376894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best #lawofattraction #resources for #healing!...</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>['lawofattraction', 'for', 'altwaystoheal', 'is']</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>['lawofattract', 'altwaystoh']</td>\n",
       "      <td>['lawofattract', 'resourc', 'heal', 'altwaysto...</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resource', 'healing', 'al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['to', 'on', 'simplest', 'moments', 'life', 'b...</td>\n",
       "      <td>['simplest', 'moments', 'life', 'blogger', 'li...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['you', 'as', 'as', 'boyfriend', 'be', 'with',...</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why do you always try to make me happy?  i don...</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>['do', 'always', 'to', 'me', 'i', 'know', 'to'...</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>['alway', 'know', 'love']</td>\n",
       "      <td>['love', 'devot']</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg is finally here!!! #ps4 #farcry4 #gtav #un...</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>['is', 'here', 'farcry4', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  best #lawofattraction #resources for #healing!...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy?  i don...   \n",
       "4  omg is finally here!!! #ps4 #farcry4 #gtav #un...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  ['lawofattraction', 'for', 'altwaystoheal', 'is']   \n",
       "1  ['to', 'on', 'simplest', 'moments', 'life', 'b...   \n",
       "2  ['you', 'as', 'as', 'boyfriend', 'be', 'with',...   \n",
       "3  ['do', 'always', 'to', 'me', 'i', 'know', 'to'...   \n",
       "4              ['is', 'here', 'farcry4', 'unchaed4']   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moments', 'life', 'blogger', 'li...   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0                     ['lawofattract', 'altwaystoh']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                          ['alway', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  ['lawofattract', 'resourc', 'heal', 'altwaysto...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                                  ['love', 'devot']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                 lemmatized_hashtags  \n",
       "0  ['lawofattraction', 'resource', 'healing', 'al...  \n",
       "1                        ['blogger', 'blog', 'life']  \n",
       "2                                         ['silvia']  \n",
       "3                               ['love', 'devotion']  \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df.label\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047a24a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTrain-Test-Split (source vs. code here)\\n\\ntrainData --> X_train\\ntestData --> X_test\\ntrainData['Label'] --> y_train\\ntestData['Label'] --> y_test\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source used:\n",
    "# https://medium.com/@vasista/sentiment-analysis-using-svm-338d418e3ff1\n",
    "'''\n",
    "Train-Test-Split (source vs. code here)\n",
    "\n",
    "trainData --> X_train\n",
    "testData --> X_test\n",
    "trainData['Label'] --> y_train\n",
    "testData['Label'] --> y_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab5e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = ms.train_test_split(X, y, test_size=0.2, random_state = 17, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f17f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows dataset:  6393\n",
      "X_train: 5114    X_test: 1279    y_train: 5114    y_test: 1279\n",
      "X_train + X_test = 6393\n"
     ]
    }
   ],
   "source": [
    "print(\"# rows dataset: \", len(df))\n",
    "print(\"X_train:\", len(X_train),  \"   X_test:\", len(X_test), \"   y_train:\",  len(y_train), \"   y_test:\", len(y_test))\n",
    "print(\"X_train + X_test =\", (len(X_train) + len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a952dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fd486",
   "metadata": {},
   "source": [
    "# C-Support Vector Classification\n",
    "\n",
    "Using each column indiviudally from our pre-processed data in order to find best kernel for the SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9d0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d639c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = [\n",
    "\"tweet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b4efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores with  linear kernel \n",
      "\n",
      "tweet :  0.4640000000000001\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  rbf kernel \n",
      "\n",
      "tweet :  0.21359223300970873\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  poly kernel \n",
      "\n",
      "tweet :  0.19607843137254902\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  sigmoid kernel \n",
      "\n",
      "tweet :  0.4166666666666667\n",
      "____________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    print(\"f1 scores with \", kernel, \"kernel \\n\")\n",
    "    for feature in feature_selection:\n",
    "        vectors_train = vectorizer.fit_transform(X_train[feature])\n",
    "        vectors_test = vectorizer.transform(X_test[feature])\n",
    "        # Perform classification with SVM\n",
    "        classifier = svm.SVC(kernel= kernel)\n",
    "        classifier.fit(vectors_train, y_train)\n",
    "\n",
    "        prediction = classifier.predict(vectors_test)\n",
    "\n",
    "        # results\n",
    "        \"\"\"\n",
    "        # alternative: print reports\n",
    "        report = classification_report(y_test, prediction, output_dict=True)\n",
    "        # print(feature,\": \", report['1'])\n",
    "        \"\"\"\n",
    "        f1 = f1_score(y_test, prediction)\n",
    "        print(feature,\": \", f1)\n",
    "    print(\"____________________________________________ \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128351b9",
   "metadata": {},
   "source": [
    "### SCV with balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a932e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores with  linear kernel \n",
      "\n",
      "tweet :  0.632183908045977\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  rbf kernel \n",
      "\n",
      "tweet :  0.4552845528455284\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  poly kernel \n",
      "\n",
      "tweet :  0.2476190476190476\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  sigmoid kernel \n",
      "\n",
      "tweet :  0.608695652173913\n",
      "____________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    print(\"f1 scores with \", kernel, \"kernel \\n\")\n",
    "    for feature in feature_selection:\n",
    "        vectors_train = vectorizer.fit_transform(X_train[feature])\n",
    "        vectors_test = vectorizer.transform(X_test[feature])\n",
    "        # Perform classification with SVM\n",
    "        classifier = svm.SVC(kernel= kernel, class_weight='balanced')\n",
    "        classifier.fit(vectors_train, y_train)\n",
    "\n",
    "        prediction = classifier.predict(vectors_test)\n",
    "\n",
    "        # results\n",
    "        \"\"\"\n",
    "        # alternative: print reports\n",
    "        report = classification_report(y_test, prediction, output_dict=True)\n",
    "        # print(feature,\": \", report['1'])\n",
    "        \"\"\"\n",
    "        f1 = f1_score(y_test, prediction)\n",
    "        print(feature,\": \", f1)\n",
    "    print(\"____________________________________________ \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc92cfc",
   "metadata": {},
   "source": [
    "### SCV with scaled vectors & balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94418b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "vectorizer = TfidfVectorizer()\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "vec_train = vectorizer.fit_transform(X_train[\"tweet\"])\n",
    "vec_test = vectorizer.transform(X_test[\"tweet\"])\n",
    "vectors_train = scaler.fit_transform(vec_train)\n",
    "vectors_test = scaler.transform(vec_test)\n",
    "\n",
    "#vectors_train = scaler.transform(vectors_train)\n",
    "#vectors_test = scaler.transform(vectors_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d270e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores with  linear kernel \n",
      "\n",
      "tweet :  0.45925925925925926\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  rbf kernel \n",
      "\n",
      "tweet :  0.29629629629629634\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  poly kernel \n",
      "\n",
      "tweet :  0.17821782178217824\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  sigmoid kernel \n",
      "\n",
      "tweet :  0.5359477124183005\n",
      "____________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    print(\"f1 scores with \", kernel, \"kernel \\n\")\n",
    "    for feature in feature_selection:\n",
    "        # Perform classification with SVM\n",
    "        classifier = svm.SVC(kernel= kernel, class_weight='balanced')\n",
    "        classifier.fit(vectors_train, y_train)\n",
    "\n",
    "        prediction = classifier.predict(vectors_test)\n",
    "\n",
    "        # results\n",
    "        f1 = f1_score(y_test, prediction)\n",
    "        print(feature,\": \", f1)\n",
    "    print(\"____________________________________________ \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf0b3f",
   "metadata": {},
   "source": [
    "## Using Grid Search for \"tweet\" with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a203b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'C': [0.001,0.01,0.1,1,10,100,1000],  \n",
    "            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'kernel': [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            'class_weight': [\"balanced\"]}\n",
    "start_time = time.time()\n",
    "vectors_train = vectorizer.fit_transform(X_train[\"tweet\"])\n",
    "vectors_test = vectorizer.transform(X_test[\"tweet\"])\n",
    "\n",
    "svm_estimator = svm.SVC()\n",
    "\n",
    "svm_cv = GridSearchCV(svm_estimator, param_grid, cv=3)\n",
    "svm_cv.fit(vectors_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"f1 for \", f1_score(y_test, svm_cv.predict(vectors_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa72185",
   "metadata": {},
   "source": [
    "### Results (run in Colab)\n",
    "--- 905.0211062431335 seconds ---\n",
    "f1 for  0.6206896551724138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992f670e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGridSearchCV(cv=3, estimator=SVC(),\\n             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\\n                         'class_weight': ['balanced'],\\n                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\\n                         'kernel': ['linear', 'rbf', 'poly', 'sigmoid']})\\n--- 905.0211062431335 seconds ---\\nf1 for  0.6206896551724138\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Result:\n",
    "GridSearchCV(cv=3, estimator=SVC(),\n",
    "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                         'class_weight': ['balanced'],\n",
    "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                         'kernel': ['linear', 'rbf', 'poly', 'sigmoid']})\n",
    "--- 905.0211062431335 seconds ---\n",
    "f1 for  0.6206896551724138\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcd499",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\"> to do:</span>\n",
    "\n",
    "- <span style=\"color: blue;\"> Why best result here worse than \"tweet\", with linear kernel?</span>\n",
    "- <span style=\"color: blue;\"> remove stop words</span>\n",
    "- <span style=\"color: blue;\"> Finding out which parameters were most successful</span>\n",
    "- <span style=\"color: blue;\"> add Confusion Matrix</span>\n",
    "\n",
    "## <span style=\"color: blue;\"> ideas:</span>\n",
    "\n",
    "- <span style=\"color: blue;\"> Can we see those examples that were categorized wrong? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd73aa",
   "metadata": {},
   "source": [
    "## keeping track of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b157cd3",
   "metadata": {},
   "source": [
    "- First SVM (linear) with f1 score for the class \"hate-speech\" around 0.3 for \"tweets\" and most pre-preprocessed text. Lemmatization and stemming do not or only slightly improve results. \n",
    "-Testing out different kernels gives worse results for \"rbf\" and \"poly\". \"sigmoid\" produces similar results.\n",
    "- --> !!! vectors not scaled --> bad performance of some of the models?\n",
    "- using weighted classes gives f1 scores up to 0.62\n",
    "- using GridSearch for the least processed column \"tweets\" gives f1 score of 0.59 (parameters C, kernel and class_weight); using more-preprocessing beforehand might improve results\n",
    "- meaning of parameters:\n",
    "    - gamma:tries to fit non-linear data (maybe only useful for some of the \"features\"?\n",
    "    - C: balancing out model complexity and training errors\n",
    "    - weight-class:\n",
    "    - summary of characters of SVM: pp. 496-498 (Tan et. al.), e.g. robust to noise in comparison to decision tree\n",
    "- scaling parameters: f1 down to 50% (for \"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066d183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
