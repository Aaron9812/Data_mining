{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978e6c9a",
   "metadata": {},
   "source": [
    "# Classifing hate speech in tweets using Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cf6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dd58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'label', 'tweet', 'n_mentions', 'hashtags', 'without_puctioation',\n",
      "       'tweet_lower', 'tweet_token', 'clean_token', 'clean_hashtags',\n",
      "       'stemmed_tokens', 'stemmed_hashtags', 'lemmatized_tokens',\n",
      "       'lemmatized_hashtags', 'tfidf_stemmed_tokens', 'tfidf_stemmed_hashtags',\n",
      "       'tfidf_lemmatized_tokens', 'tfidf_lemmatized_hashtags'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Downloading the csv file from GitHub\n",
    "url = \"https://raw.githubusercontent.com/Aaron9812/Data_mining/main/data/220505_train_data_preprocessed.csv\"\n",
    "download = requests.get(url).content\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')), sep=\";\")\n",
    "\n",
    "# Printing out the first row of the dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a45a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "\"tweet\",\n",
    "\"hashtags\",\n",
    "\"without_puctioation\",\n",
    "\"tweet_lower\",\n",
    "\"tweet_token\",\n",
    "\"clean_token\",\n",
    "\"clean_hashtags\",\n",
    "\"stemmed_tokens\",\n",
    "\"stemmed_hashtags\",\n",
    "\"lemmatized_tokens\",\n",
    "\"lemmatized_hashtags\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade0cad",
   "metadata": {},
   "source": [
    "### only text features used (so far); no numerical features included!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00376894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best #lawofattraction #resources for #healing!...</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>['lawofattraction', 'for', 'altwaystoheal', 'is']</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>['lawofattract', 'altwaystoh']</td>\n",
       "      <td>['lawofattract', 'resourc', 'heal', 'altwaysto...</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resource', 'healing', 'al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['to', 'on', 'simplest', 'moments', 'life', 'b...</td>\n",
       "      <td>['simplest', 'moments', 'life', 'blogger', 'li...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['you', 'as', 'as', 'boyfriend', 'be', 'with',...</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why do you always try to make me happy?  i don...</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>['do', 'always', 'to', 'me', 'i', 'know', 'to'...</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>['alway', 'know', 'love']</td>\n",
       "      <td>['love', 'devot']</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg is finally here!!! #ps4 #farcry4 #gtav #un...</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>['is', 'here', 'farcry4', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  best #lawofattraction #resources for #healing!...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy?  i don...   \n",
       "4  omg is finally here!!! #ps4 #farcry4 #gtav #un...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  ['lawofattraction', 'for', 'altwaystoheal', 'is']   \n",
       "1  ['to', 'on', 'simplest', 'moments', 'life', 'b...   \n",
       "2  ['you', 'as', 'as', 'boyfriend', 'be', 'with',...   \n",
       "3  ['do', 'always', 'to', 'me', 'i', 'know', 'to'...   \n",
       "4              ['is', 'here', 'farcry4', 'unchaed4']   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moments', 'life', 'blogger', 'li...   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0                     ['lawofattract', 'altwaystoh']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                          ['alway', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  ['lawofattract', 'resourc', 'heal', 'altwaysto...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                                  ['love', 'devot']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                 lemmatized_hashtags  \n",
       "0  ['lawofattraction', 'resource', 'healing', 'al...  \n",
       "1                        ['blogger', 'blog', 'life']  \n",
       "2                                         ['silvia']  \n",
       "3                               ['love', 'devotion']  \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df.label\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047a24a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTrain-Test-Split (source vs. code here)\\n\\ntrainData --> X_train\\ntestData --> X_test\\ntrainData['Label'] --> y_train\\ntestData['Label'] --> y_test\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source used:\n",
    "# https://medium.com/@vasista/sentiment-analysis-using-svm-338d418e3ff1\n",
    "'''\n",
    "Train-Test-Split (source vs. code here)\n",
    "\n",
    "trainData --> X_train\n",
    "testData --> X_test\n",
    "trainData['Label'] --> y_train\n",
    "testData['Label'] --> y_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab5e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = ms.train_test_split(X, y, test_size=0.2, random_state = 17, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f17f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows dataset:  6393\n",
      "X_train: 5114    X_test: 1279    y_train: 5114    y_test: 1279\n",
      "X_train + X_test = 6393\n"
     ]
    }
   ],
   "source": [
    "print(\"# rows dataset: \", len(df))\n",
    "print(\"X_train:\", len(X_train),  \"   X_test:\", len(X_test), \"   y_train:\",  len(y_train), \"   y_test:\", len(y_test))\n",
    "print(\"X_train + X_test =\", (len(X_train) + len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fd486",
   "metadata": {},
   "source": [
    "# C-Support Vector Classification\n",
    "\n",
    "Using each column indiviudally from our pre-processed data in order to find best kernel for the SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a35d5",
   "metadata": {},
   "source": [
    "### SCV with equal class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a952dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9d0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b4efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores with  linear kernel \n",
      "\n",
      "tweet :  0.4640000000000001\n",
      "hashtags :  0.4406779661016949\n",
      "without_puctioation :  0.4715447154471545\n",
      "tweet_lower :  0.4715447154471545\n",
      "tweet_token :  0.3728813559322034\n",
      "clean_token :  0.35\n",
      "clean_hashtags :  0.4406779661016949\n",
      "stemmed_tokens :  0.3620689655172414\n",
      "stemmed_hashtags :  0.4878048780487805\n",
      "lemmatized_tokens :  0.3529411764705882\n",
      "lemmatized_hashtags :  0.46280991735537186\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  rbf kernel \n",
      "\n",
      "tweet :  0.19607843137254902\n",
      "hashtags :  0.34234234234234234\n",
      "without_puctioation :  0.19607843137254902\n",
      "tweet_lower :  0.19607843137254902\n",
      "tweet_token :  0.2476190476190476\n",
      "clean_token :  0.2452830188679245\n",
      "clean_hashtags :  0.34234234234234234\n",
      "stemmed_tokens :  0.2616822429906542\n",
      "stemmed_hashtags :  0.3571428571428571\n",
      "lemmatized_tokens :  0.27777777777777773\n",
      "lemmatized_hashtags :  0.3571428571428571\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  poly kernel \n",
      "\n",
      "tweet :  0.19607843137254902\n",
      "hashtags :  0.3243243243243243\n",
      "without_puctioation :  0.19607843137254902\n",
      "tweet_lower :  0.19607843137254902\n",
      "tweet_token :  0.16\n",
      "clean_token :  0.21359223300970873\n",
      "clean_hashtags :  0.3243243243243243\n",
      "stemmed_tokens :  0.19607843137254902\n",
      "stemmed_hashtags :  0.3243243243243243\n",
      "lemmatized_tokens :  0.21359223300970873\n",
      "lemmatized_hashtags :  0.3243243243243243\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  sigmoid kernel \n",
      "\n",
      "tweet :  0.4426229508196721\n",
      "hashtags :  0.4426229508196721\n",
      "without_puctioation :  0.4166666666666667\n",
      "tweet_lower :  0.4166666666666667\n",
      "tweet_token :  0.33613445378151263\n",
      "clean_token :  0.35\n",
      "clean_hashtags :  0.4426229508196721\n",
      "stemmed_tokens :  0.3620689655172414\n",
      "stemmed_hashtags :  0.49600000000000005\n",
      "lemmatized_tokens :  0.35\n",
      "lemmatized_hashtags :  0.467741935483871\n",
      "____________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    print(\"f1 scores with \", kernel, \"kernel \\n\")\n",
    "    for feature in features:\n",
    "        vectors_train = vectorizer.fit_transform(X_train[feature])\n",
    "        vectors_test = vectorizer.transform(X_test[feature])\n",
    "        # Perform classification with SVM\n",
    "        classifier = svm.SVC(kernel= kernel)\n",
    "        classifier.fit(vectors_train, y_train)\n",
    "\n",
    "        prediction = classifier.predict(vectors_test)\n",
    "\n",
    "        # results\n",
    "        \"\"\"\n",
    "        # alternative: print reports\n",
    "        report = classification_report(y_test, prediction, output_dict=True)\n",
    "        # print(feature,\": \", report['1'])\n",
    "        \"\"\"\n",
    "        f1 = f1_score(y_test, prediction)\n",
    "        print(feature,\": \", f1)\n",
    "    print(\"____________________________________________ \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2fe60",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\"> to do:</span>\n",
    " <span style=\"color: blue;\"> scaling vectors to improve models for some of the kernels?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128351b9",
   "metadata": {},
   "source": [
    "### SCV with balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a932e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores with  linear kernel \n",
      "\n",
      "tweet :  0.6206896551724138\n",
      "hashtags :  0.48529411764705876\n",
      "without_puctioation :  0.6395348837209303\n",
      "tweet_lower :  0.6395348837209303\n",
      "tweet_token :  0.5053763440860215\n",
      "clean_token :  0.4972375690607735\n",
      "clean_hashtags :  0.48529411764705876\n",
      "stemmed_tokens :  0.5154639175257731\n",
      "stemmed_hashtags :  0.5106382978723404\n",
      "lemmatized_tokens :  0.5212765957446808\n",
      "lemmatized_hashtags :  0.4892086330935252\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  rbf kernel \n",
      "\n",
      "tweet :  0.46031746031746035\n",
      "hashtags :  0.3793103448275862\n",
      "without_puctioation :  0.47244094488188976\n",
      "tweet_lower :  0.47244094488188976\n",
      "tweet_token :  0.3851851851851852\n",
      "clean_token :  0.3664921465968587\n",
      "clean_hashtags :  0.3793103448275862\n",
      "stemmed_tokens :  0.3522727272727273\n",
      "stemmed_hashtags :  0.4067796610169491\n",
      "lemmatized_tokens :  0.37837837837837834\n",
      "lemmatized_hashtags :  0.3931623931623932\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  poly kernel \n",
      "\n",
      "tweet :  0.2476190476190476\n",
      "hashtags :  0.3214285714285714\n",
      "without_puctioation :  0.2476190476190476\n",
      "tweet_lower :  0.2476190476190476\n",
      "tweet_token :  0.27027027027027023\n",
      "clean_token :  0.2786885245901639\n",
      "clean_hashtags :  0.3214285714285714\n",
      "stemmed_tokens :  0.2542372881355932\n",
      "stemmed_hashtags :  0.3214285714285714\n",
      "lemmatized_tokens :  0.28571428571428564\n",
      "lemmatized_hashtags :  0.3214285714285714\n",
      "____________________________________________ \n",
      "\n",
      "f1 scores with  sigmoid kernel \n",
      "\n",
      "tweet :  0.5971563981042655\n",
      "hashtags :  0.47368421052631576\n",
      "without_puctioation :  0.5756097560975609\n",
      "tweet_lower :  0.5756097560975609\n",
      "tweet_token :  0.43824701195219123\n",
      "clean_token :  0.4267782426778243\n",
      "clean_hashtags :  0.47368421052631576\n",
      "stemmed_tokens :  0.43724696356275305\n",
      "stemmed_hashtags :  0.4836601307189543\n",
      "lemmatized_tokens :  0.42448979591836733\n",
      "lemmatized_hashtags :  0.47058823529411764\n",
      "____________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    print(\"f1 scores with \", kernel, \"kernel \\n\")\n",
    "    for feature in features:\n",
    "        vectors_train = vectorizer.fit_transform(X_train[feature])\n",
    "        vectors_test = vectorizer.transform(X_test[feature])\n",
    "        # Perform classification with SVM\n",
    "        classifier = svm.SVC(kernel= kernel, class_weight='balanced')\n",
    "        classifier.fit(vectors_train, y_train)\n",
    "\n",
    "        prediction = classifier.predict(vectors_test)\n",
    "\n",
    "        # results\n",
    "        \"\"\"\n",
    "        # alternative: print reports\n",
    "        report = classification_report(y_test, prediction, output_dict=True)\n",
    "        # print(feature,\": \", report['1'])\n",
    "        \"\"\"\n",
    "        f1 = f1_score(y_test, prediction)\n",
    "        print(feature,\": \", f1)\n",
    "    print(\"____________________________________________ \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf0b3f",
   "metadata": {},
   "source": [
    "## Usin Grid Search for \"tweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39f78fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [1, 10], 'class_weight': (None, 'balanced'),\n",
       "                         'kernel': ('linear', 'rbf', 'poly', 'sigmoid')})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_grid = vectorizer.fit_transform(X_train[\"tweet\"])\n",
    "X_test_grid = vectorizer.transform(X_test[\"tweet\"])\n",
    "\n",
    "svm_estimator = svm.SVC()\n",
    "\n",
    "svm = GridSearchCV(estimator=svm_estimator,\n",
    "             param_grid={'C': [1, 10, ], 'kernel': (\"linear\", \"rbf\", \"poly\", \"sigmoid\"), 'class_weight': (None, \"balanced\")})\n",
    "svm.fit(X_train_grid, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4801905f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5935483870967742"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, svm.predict(X_test_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcd499",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\"> to do:</span>\n",
    "\n",
    "- <span style=\"color: blue;\"> Why best result here worse than \"tweet\", with linear kernel?</span>\n",
    "- <span style=\"color: blue;\"> Finding out which parameters were most successful</span>\n",
    "- <span style=\"color: blue;\"> add Confusion Matrix</span>\n",
    "\n",
    "## <span style=\"color: blue;\"> double check:</span>\n",
    "- <span style=\"color: blue;\"> Does GridSearch use any variables that has been altered by the loops before?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd73aa",
   "metadata": {},
   "source": [
    "## keeping track of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b157cd3",
   "metadata": {},
   "source": [
    "- First SVM (linear) with f1 score for the class \"hate-speech\" around 0.3 for \"tweets\" and most pre-preprocessed text. Lemmatization and stemming do not or only slightly improve results. \n",
    "-Testing out different kernels gives worse results for \"rbf\" and \"poly\". \"sigmoid\" produces similar results.\n",
    "- --> !!! vectors not scaled --> bad performance of some of the models?\n",
    "- using weighted classes gives f1 scores up to 0.62\n",
    "- using GridSearch for the least processed column \"tweets\" gives f1 score of 0.59 (parameters C, kernel and class_weight); using more-preprocessing beforehand might improve results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
