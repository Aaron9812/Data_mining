{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978e6c9a",
   "metadata": {},
   "source": [
    "Classifing hate speech tweets using Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cf6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86fb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources used:\n",
    "# https://medium.com/@vasista/sentiment-analysis-using-svm-338d418e3ff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27dd58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'label', 'tweet', 'n_mentions', 'hashtags', 'without_puctioation',\n",
      "       'tweet_lower', 'tweet_token', 'clean_token', 'clean_hashtags',\n",
      "       'stemmed_tokens', 'stemmed_hashtags', 'lemmatized_tokens',\n",
      "       'lemmatized_hashtags', 'tfidf_stemmed_tokens', 'tfidf_stemmed_hashtags',\n",
      "       'tfidf_lemmatized_tokens', 'tfidf_lemmatized_hashtags'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# import io\n",
    "    \n",
    "# Downloading the csv file from GitHub\n",
    "url = \"https://raw.githubusercontent.com/Aaron9812/Data_mining/main/data/220505_train_data_preprocessed.csv\"\n",
    "download = requests.get(url).content\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')), sep=\";\")\n",
    "\n",
    "# Printing out the first rows of the dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00376894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best #lawofattraction #resources for #healing!...</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>['lawofattraction', 'for', 'altwaystoheal', 'is']</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>['lawofattract', 'altwaystoh']</td>\n",
       "      <td>['lawofattract', 'resourc', 'heal', 'altwaysto...</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resource', 'healing', 'al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['to', 'on', 'simplest', 'moments', 'life', 'b...</td>\n",
       "      <td>['simplest', 'moments', 'life', 'blogger', 'li...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['you', 'as', 'as', 'boyfriend', 'be', 'with',...</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why do you always try to make me happy?  i don...</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>['do', 'always', 'to', 'me', 'i', 'know', 'to'...</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>['alway', 'know', 'love']</td>\n",
       "      <td>['love', 'devot']</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg is finally here!!! #ps4 #farcry4 #gtav #un...</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>['is', 'here', 'farcry4', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  best #lawofattraction #resources for #healing!...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy?  i don...   \n",
       "4  omg is finally here!!! #ps4 #farcry4 #gtav #un...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  best lawofattraction resources for healing    ...   \n",
       "1  remembering to focus on the simplest happy mom...   \n",
       "2  when you get as happy as your boyfriend to be ...   \n",
       "3  why do you always try to make me happy  i dont...   \n",
       "4   omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  ['lawofattraction', 'for', 'altwaystoheal', 'is']   \n",
       "1  ['to', 'on', 'simplest', 'moments', 'life', 'b...   \n",
       "2  ['you', 'as', 'as', 'boyfriend', 'be', 'with',...   \n",
       "3  ['do', 'always', 'to', 'me', 'i', 'know', 'to'...   \n",
       "4              ['is', 'here', 'farcry4', 'unchaed4']   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moments', 'life', 'blogger', 'li...   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                               ['love', 'devotion']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0                     ['lawofattract', 'altwaystoh']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                          ['alway', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  ['lawofattract', 'resourc', 'heal', 'altwaysto...   \n",
       "1                        ['blogger', 'blog', 'life']   \n",
       "2                                         ['silvia']   \n",
       "3                                  ['love', 'devot']   \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0               ['lawofattraction', 'altwaystoheal']   \n",
       "1  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "2                               ['boyfriend', 'car']   \n",
       "3                         ['always', 'know', 'love']   \n",
       "4                            ['farcry4', 'unchaed4']   \n",
       "\n",
       "                                 lemmatized_hashtags  \n",
       "0  ['lawofattraction', 'resource', 'healing', 'al...  \n",
       "1                        ['blogger', 'blog', 'life']  \n",
       "2                                         ['silvia']  \n",
       "3                               ['love', 'devotion']  \n",
       "4             ['ps4', 'farcry4', 'gtav', 'unchaed4']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "\"tweet\",\n",
    "\"hashtags\",\n",
    "\"without_puctioation\",\n",
    "\"tweet_lower\",\n",
    "\"tweet_token\",\n",
    "\"clean_token\",\n",
    "\"clean_hashtags\",\n",
    "\"stemmed_tokens\",\n",
    "\"stemmed_hashtags\",\n",
    "\"lemmatized_tokens\",\n",
    "\"lemmatized_hashtags\"\n",
    "]\n",
    "X = df[features]\n",
    "y = df.label\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047a24a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTrain-Test-Split (source vs. code here)\\n\\ntrainData --> X_train\\ntestData --> X_test\\ntrainData['Label'] --> y_train\\ntestData['Label'] --> y_test\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Train-Test-Split (source vs. code here)\n",
    "\n",
    "trainData --> X_train\n",
    "testData --> X_test\n",
    "trainData['Label'] --> y_train\n",
    "testData['Label'] --> y_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab5e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = ms.train_test_split(X, y, test_size=0.2, random_state = 17, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f17f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows dataset:  6393\n",
      "5114 1279 5114 1279\n",
      "sum:  6393\n"
     ]
    }
   ],
   "source": [
    "print(\"# rows dataset: \", len(df))\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "print(\"sum: \", (len(y_train) + len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a952dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dacc40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet :  {'precision': 0.8181818181818182, 'recall': 0.29347826086956524, 'f1-score': 0.432, 'support': 92}\n",
      "hashtags :  {'precision': 0.9, 'recall': 0.1956521739130435, 'f1-score': 0.3214285714285714, 'support': 92}\n",
      "without_puctioation :  {'precision': 0.84375, 'recall': 0.29347826086956524, 'f1-score': 0.435483870967742, 'support': 92}\n",
      "tweet_lower :  {'precision': 0.84375, 'recall': 0.29347826086956524, 'f1-score': 0.435483870967742, 'support': 92}\n",
      "tweet_token :  {'precision': 0.7727272727272727, 'recall': 0.18478260869565216, 'f1-score': 0.2982456140350877, 'support': 92}\n",
      "clean_token :  {'precision': 0.6956521739130435, 'recall': 0.17391304347826086, 'f1-score': 0.2782608695652174, 'support': 92}\n",
      "clean_hashtags :  {'precision': 0.8571428571428571, 'recall': 0.1956521739130435, 'f1-score': 0.3185840707964602, 'support': 92}\n",
      "stemmed_tokens :  {'precision': 0.7083333333333334, 'recall': 0.18478260869565216, 'f1-score': 0.29310344827586204, 'support': 92}\n",
      "stemmed_hashtags :  {'precision': 0.8636363636363636, 'recall': 0.20652173913043478, 'f1-score': 0.33333333333333337, 'support': 92}\n",
      "lemmatized_tokens :  {'precision': 0.6666666666666666, 'recall': 0.17391304347826086, 'f1-score': 0.27586206896551724, 'support': 92}\n",
      "lemmatized_hashtags :  {'precision': 0.9090909090909091, 'recall': 0.21739130434782608, 'f1-score': 0.3508771929824562, 'support': 92}\n"
     ]
    }
   ],
   "source": [
    "for feature in features:\n",
    "    vectors_train = vectorizer.fit_transform(X_train[feature])\n",
    "    vectors_test = vectorizer.transform(X_test[feature])\n",
    "    # Perform classification with SVM, kernel=linear\n",
    "    classifier_linear = svm.SVC(kernel='linear')\n",
    "    classifier_linear.fit(vectors_train, y_train)\n",
    "\n",
    "    prediction_linear = classifier_linear.predict(vectors_test)\n",
    "\n",
    "    # results\n",
    "    report = classification_report(y_test, prediction_linear, output_dict=True)\n",
    "    print(feature,\": \", report['1'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
