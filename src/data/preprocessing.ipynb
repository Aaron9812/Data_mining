{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq646iXZxVbH",
        "outputId": "16e71bf1-6405-4847-a853-3454f9b1ac41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Wsk40Gx35D",
        "outputId": "3b61d87b-6a88-4ec7-faa3-7035fe043253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DataMining/Data_mining/src/data\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/DataMining/Data_mining/src/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHAr19h1xQgn"
      },
      "source": [
        "# Preproccesing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6trqFrZixQg1"
      },
      "source": [
        "## Importing needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9x1R9W9xQg3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDSIpni2xQg9"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVg3ddLpxQg-"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"../../data/train_tweet.csv\")\n",
        "test_data = pd.read_csv(\"../../data/test_tweets.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3jgrWtqxQhB"
      },
      "source": [
        "Take a first look at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "982-_-dQxQhD"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsMipIvRxQhH"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3fp1U0AxQhJ"
      },
      "source": [
        "## Deal with user mentions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSp0NV4txQhM"
      },
      "outputs": [],
      "source": [
        "def count_user_mentions(text:str) ->int:\n",
        "    return text.count(\"@user\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iUWPC51xQhP"
      },
      "outputs": [],
      "source": [
        "test_data[\"n_mentions\"] = test_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
        "train_data[\"n_mentions\"] = train_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD0cibTixQhR"
      },
      "source": [
        "## Deal with hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfD7xYZDxQhT"
      },
      "outputs": [],
      "source": [
        "def identify_hashtags(text:str) -> list:\n",
        "    pattern = re.compile(r\"#(\\w+)\")\n",
        "    return pattern.findall(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbu8dHAgxQhV"
      },
      "outputs": [],
      "source": [
        "test_data[\"hashtags\"] = test_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
        "train_data[\"hashtags\"] = train_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPaOhm2lxQhX"
      },
      "source": [
        "## Punctuation Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfZQIjwdxQhY"
      },
      "source": [
        "Create helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFYglYuQxQhZ"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text:str) -> str:\n",
        "    return \"\".join([i for i in text if i not in punctuation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D3dA36-xQha"
      },
      "outputs": [],
      "source": [
        "test_data[\"without_punctuation\"] = test_data[\"tweet\"].apply(lambda x: remove_punctuation(x))\n",
        "train_data[\"without_punctuation\"] = train_data[\"tweet\"].apply(lambda x: remove_punctuation(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeV3Ir5ExQhc"
      },
      "outputs": [],
      "source": [
        "train_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgDDGxTCxQhd"
      },
      "source": [
        "## Lowering text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sASiJLEOxQhf"
      },
      "outputs": [],
      "source": [
        "test_data[\"tweet_lower\"] = test_data[\"without_punctuation\"].apply(lambda x: x.lower())\n",
        "train_data[\"tweet_lower\"] = train_data[\"without_punctuation\"].apply(lambda x: x.lower())\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tx4IBjzxQhh"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVg49DvUzuhY"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu11J8NWxQhh"
      },
      "outputs": [],
      "source": [
        "def tokenization(text:str) -> list:\n",
        "    return nltk.word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9ptoOj7xQhj"
      },
      "outputs": [],
      "source": [
        "test_data[\"tweet_token\"] = test_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
        "train_data[\"tweet_token\"] = train_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSC4DCYDxQhk"
      },
      "source": [
        "## Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i91SIAplz9JQ"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcRjbmKdxQhl"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens) ->list:\n",
        "    stopwords_list = stopwords.words(\"english\")\n",
        "    return [token for token in tokens if token not in stopwords_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckw3hwngxQhm"
      },
      "outputs": [],
      "source": [
        "test_data[\"clean_token\"] = test_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
        "train_data[\"clean_token\"] = train_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
        "test_data[\"clean_hashtags\"] = test_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
        "train_data[\"clean_hashtags\"] = train_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKW9vlXKxQho"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK6vz_J2xQhp"
      },
      "outputs": [],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(text:list) -> list:\n",
        "    return [porter_stemmer.stem(word) for word in text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdU25XTMxQhq"
      },
      "outputs": [],
      "source": [
        "test_data[\"stemmed_tokens\"] = test_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
        "train_data[\"stemmed_tokens\"] = train_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
        "test_data[\"stemmed_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
        "train_data[\"stemmed_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGOW_qTqxQhs"
      },
      "source": [
        "Result does not look great (e.g. movie -> movi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT29MhFuxQht"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTj48kNd0Or9"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbYBgb2ExQht"
      },
      "outputs": [],
      "source": [
        "word_lemmatizer = WordNetLemmatizer()\n",
        "def lemmatizer(text: list) -> list:\n",
        "    return [word_lemmatizer.lemmatize(word) for word in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbKAW17l1GXG"
      },
      "outputs": [],
      "source": [
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5caf3YwxQhu"
      },
      "outputs": [],
      "source": [
        "test_data[\"lemmatized_tokens\"] = test_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
        "train_data[\"lemmatized_tokens\"] = train_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
        "test_data[\"lemmatized_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
        "train_data[\"lemmatized_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n77DbcoxQhw"
      },
      "source": [
        "## Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1-NGbkmxQhw"
      },
      "outputs": [],
      "source": [
        "test_data.to_csv(\"../../data/220502_test_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
        "train_data.to_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBHdSxnVxQhy"
      },
      "source": [
        "## Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HQKvjv9V0cpE",
        "outputId": "49682c53-d937-4690-87c2-d57652df607a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting texthero\n",
            "  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting nltk>=3.3\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 21.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.0.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n",
            "Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.64.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.0->texthero) (4.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2022.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.1.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.9.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n",
            "Installing collected packages: regex, unidecode, nltk, texthero\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.7 regex-2022.4.24 texthero-1.1.0 unidecode-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install texthero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gANrTs-xxQhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82681452-252f-4b12-daf9-5088d87d79cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import texthero as hero\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=';')\n",
        "train_data[\"tfidf_stemmed_tokens\"] = (hero.tfidf(train_data[\"stemmed_tokens\"], max_features=8000))\n",
        "train_data[\"tfidf_stemmed_tokens\"] = (hero.pca(train_data[\"tfidf_stemmed_tokens\"], n_components=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BuV0sAAxQh0"
      },
      "outputs": [],
      "source": [
        "train_data[\"tfidf_stemmed_hashtags\"] = (hero.tfidf(train_data[\"stemmed_hashtags\"], max_features=8000))\n",
        "train_data[\"tfidf_stemmed_hashtags\"] = (hero.pca(train_data[\"tfidf_stemmed_hashtags\"], n_components=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4OJX1fMxQh0"
      },
      "outputs": [],
      "source": [
        "train_data[\"tfidf_lemmatized_tokens\"] = (hero.tfidf(train_data[\"lemmatized_tokens\"], max_features=8000))\n",
        "train_data[\"tfidf_lemmatized_tokens\"] = (hero.pca(train_data[\"tfidf_lemmatized_tokens\"], n_components=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv3CTQNmxQh1"
      },
      "outputs": [],
      "source": [
        "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.tfidf(train_data[\"lemmatized_hashtags\"], max_features=8000))\n",
        "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.pca(train_data[\"tfidf_lemmatized_hashtags\"], n_components=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MwAXtKqxQh3"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFkG0AdjxQh4"
      },
      "outputs": [],
      "source": [
        "X = train_data.loc[:, train_data.columns != \"label\"]\n",
        "Y = train_data.loc[train_data.label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw6vYXgzxQh5"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_data, Y, test_size=0.2, random_state=55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNwBcQqoxQh6"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PRdzEerxQh8"
      },
      "outputs": [],
      "source": [
        "X_test.to_csv(\"../../data/220505_test_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
        "X_train.to_csv(\"../../data/220505_train_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
        "X_val.to_csv(\"../../data/220505_validation_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVSJHA6AxQh9"
      },
      "source": [
        "# Work in progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYWUXpT1xQh-"
      },
      "source": [
        "## Work on emojis \n",
        "Convert emojis to their corresponding text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMkxb8h0xQh-"
      },
      "outputs": [],
      "source": [
        "text = \"#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦  \"\n",
        "test = b'#model   i love u take with u all the time in ur\\xc3\\xb0\\xc2\\x9f\\xc2\\x93\\xc2\\xb1!!! \\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x99\\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x8e\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x84\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x85\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6  '\n",
        "\n",
        "test.decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7Ug2eCHxQiA"
      },
      "outputs": [],
      "source": [
        "test = \"#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦  \"\n",
        "print(emot.emoji(test))\n",
        "\n",
        "print(test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kgDDGxTCxQhd",
        "9Tx4IBjzxQhh",
        "DSC4DCYDxQhk",
        "JKW9vlXKxQho",
        "tT29MhFuxQht",
        "5n77DbcoxQhw",
        "SBHdSxnVxQhy",
        "-MwAXtKqxQh3",
        "VYWUXpT1xQh-"
      ],
      "machine_shape": "hm",
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "947cd5ef72fa4485f7ecf5a654ef12bcb7ac0faec018370a70389fc4010d0179"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}