{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccesing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../data/train_tweet.csv\")\n",
    "test_data = pd.read_csv(\"../../data/test_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with user mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_user_mentions(text:str) ->int:\n",
    "    return text.count(\"@user\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0\n",
       "1  31964   @user #white #supremacists want everyone to s...           1\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0\n",
       "3  31966  is the hp and the cursed child book up for res...           0\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"n_mentions\"] = test_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
    "train_data[\"n_mentions\"] = train_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_hashtags(text:str) -> list:\n",
    "    pattern = re.compile(r\"#(\\w+)\")\n",
    "    return pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1               [white, supremacists, birdsâ, movie]  \n",
       "2            [acne, altwaystoheal, healthy, healing]  \n",
       "3                [harrypotter, pottermore, favorite]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"hashtags\"] = test_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
    "train_data[\"hashtags\"] = train_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctioation(text:str) -> str:\n",
    "    return \"\".join([i for i in text if i not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birdsâ, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \n",
       "0  studiolife aislife requires passion dedication...  \n",
       "1   user white supremacists want everyone to see ...  \n",
       "2  safe ways to heal your acne    altwaystoheal h...  \n",
       "3  is the hp and the cursed child book up for res...  \n",
       "4    3rd bihday to my amazing hilarious nephew el...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"without_puctioation\"] = test_data[\"tweet\"].apply(lambda x: remove_punctioation(x))\n",
    "train_data[\"without_puctioation\"] = train_data[\"tweet\"].apply(lambda x: remove_punctioation(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[run]</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lyft, disapointed, getthanked]</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[model]</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>[motivation]</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[allshowandnogo]</td>\n",
       "      <td>22 huge fan fare and big talking before they l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>user camping tomorrow user user user user use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "      <td>0</td>\n",
       "      <td>[school, exams, hate, imagine, actorslife, rev...</td>\n",
       "      <td>the next school year is the year for examsð¯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[allin, cavs, champions, cleveland, clevelandc...</td>\n",
       "      <td>we won love the land allin cavs champions clev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>[gr8]</td>\n",
       "      <td>user user welcome here   im   its so gr8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  n_mentions  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...           1   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...           2   \n",
       "2   3      0                                bihday your majesty           0   \n",
       "3   4      0  #model   i love u take with u all the time in ...           0   \n",
       "4   5      0             factsguide: society now    #motivation           0   \n",
       "5   6      0  [2/2] huge fan fare and big talking before the...           0   \n",
       "6   7      0   @user camping tomorrow @user @user @user @use...           8   \n",
       "7   8      0  the next school year is the year for exams.ð...           0   \n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...           0   \n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr...           2   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                                              [run]   \n",
       "1                    [lyft, disapointed, getthanked]   \n",
       "2                                                 []   \n",
       "3                                            [model]   \n",
       "4                                       [motivation]   \n",
       "5                                   [allshowandnogo]   \n",
       "6                                                 []   \n",
       "7  [school, exams, hate, imagine, actorslife, rev...   \n",
       "8  [allin, cavs, champions, cleveland, clevelandc...   \n",
       "9                                              [gr8]   \n",
       "\n",
       "                                 without_puctioation  \n",
       "0   user when a father is dysfunctional and is so...  \n",
       "1  user user thanks for lyft credit i cant use ca...  \n",
       "2                                bihday your majesty  \n",
       "3  model   i love u take with u all the time in u...  \n",
       "4               factsguide society now    motivation  \n",
       "5  22 huge fan fare and big talking before they l...  \n",
       "6   user camping tomorrow user user user user use...  \n",
       "7  the next school year is the year for examsð¯...  \n",
       "8  we won love the land allin cavs champions clev...  \n",
       "9         user user welcome here   im   its so gr8    "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowering text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[run]</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lyft, disapointed, getthanked]</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[model]</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>[motivation]</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  n_mentions  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...           1   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...           2   \n",
       "2   3      0                                bihday your majesty           0   \n",
       "3   4      0  #model   i love u take with u all the time in ...           0   \n",
       "4   5      0             factsguide: society now    #motivation           0   \n",
       "\n",
       "                          hashtags  \\\n",
       "0                            [run]   \n",
       "1  [lyft, disapointed, getthanked]   \n",
       "2                               []   \n",
       "3                          [model]   \n",
       "4                     [motivation]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0   user when a father is dysfunctional and is so...   \n",
       "1  user user thanks for lyft credit i cant use ca...   \n",
       "2                                bihday your majesty   \n",
       "3  model   i love u take with u all the time in u...   \n",
       "4               factsguide society now    motivation   \n",
       "\n",
       "                                         tweet_lower  \n",
       "0   user when a father is dysfunctional and is so...  \n",
       "1  user user thanks for lyft credit i cant use ca...  \n",
       "2                                bihday your majesty  \n",
       "3  model   i love u take with u all the time in u...  \n",
       "4               factsguide society now    motivation  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"tweet_lower\"] = test_data[\"without_puctioation\"].apply(lambda x: x.lower())\n",
    "train_data[\"tweet_lower\"] = train_data[\"without_puctioation\"].apply(lambda x: x.lower())\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text:str) -> list:\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birdsâ, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1  [user, white, supremacists, want, everyone, to...  \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...  \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...  \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"tweet_token\"] = test_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
    "train_data[\"tweet_token\"] = train_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens) ->list:\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    return [token for token in tokens if token not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, se...</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "      <td>[safe, ways, heal, acne, altwaystoheal, health...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "      <td>[hp, cursed, child, book, reservations, alread...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birdsâ, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, to...   \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...   \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...   \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, se...   \n",
       "2  [safe, ways, heal, acne, altwaystoheal, health...   \n",
       "3  [hp, cursed, child, book, reservations, alread...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                      clean_hashtags  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1               [white, supremacists, birdsâ, movie]  \n",
       "2            [acne, altwaystoheal, healthy, healing]  \n",
       "3                [harrypotter, pottermore, favorite]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"clean_token\"] = test_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data[\"clean_token\"] = train_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
    "test_data[\"clean_hashtags\"] = test_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data[\"clean_hashtags\"] = train_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(text:list) -> list:\n",
    "    return [porter_stemmer.stem(word) for word in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, se...</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "      <td>[user, white, supremacist, want, everyon, see,...</td>\n",
       "      <td>[white, supremacist, birdsâ, movi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "      <td>[safe, ways, heal, acne, altwaystoheal, health...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>[safe, way, heal, acn, altwaystoh, healthi, heal]</td>\n",
       "      <td>[acn, altwaystoh, healthi, heal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "      <td>[hp, cursed, child, book, reservations, alread...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>[hp, curs, child, book, reserv, alreadi, ye, ð...</td>\n",
       "      <td>[harrypott, pottermor, favorit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>[3rd, bihday, amaz, hilari, nephew, eli, ahmir...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birdsâ, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, to...   \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...   \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...   \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, se...   \n",
       "2  [safe, ways, heal, acne, altwaystoheal, health...   \n",
       "3  [hp, cursed, child, book, reservations, alread...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birdsâ, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...   \n",
       "1  [user, white, supremacist, want, everyon, see,...   \n",
       "2  [safe, way, heal, acn, altwaystoh, healthi, heal]   \n",
       "3  [hp, curs, child, book, reserv, alreadi, ye, ð...   \n",
       "4  [3rd, bihday, amaz, hilari, nephew, eli, ahmir...   \n",
       "\n",
       "                                    stemmed_hashtags  \n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...  \n",
       "1                 [white, supremacist, birdsâ, movi]  \n",
       "2                   [acn, altwaystoh, healthi, heal]  \n",
       "3                    [harrypott, pottermor, favorit]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"stemmed_tokens\"] = test_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
    "train_data[\"stemmed_tokens\"] = train_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
    "test_data[\"stemmed_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
    "train_data[\"stemmed_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result does not look great (e.g. movie -> movi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\D073999\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizer(text: list) -> list:\n",
    "    return [word_lemmatizer.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, se...</td>\n",
       "      <td>[white, supremacists, birdsâ, movie]</td>\n",
       "      <td>[user, white, supremacist, want, everyon, see,...</td>\n",
       "      <td>[white, supremacist, birdsâ, movi]</td>\n",
       "      <td>[user, white, supremacist, want, everyone, see...</td>\n",
       "      <td>[white, supremacist, birdsâ, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "      <td>[safe, ways, heal, acne, altwaystoheal, health...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>[safe, way, heal, acn, altwaystoh, healthi, heal]</td>\n",
       "      <td>[acn, altwaystoh, healthi, heal]</td>\n",
       "      <td>[safe, way, heal, acne, altwaystoheal, healthy...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "      <td>[hp, cursed, child, book, reservations, alread...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>[hp, curs, child, book, reserv, alreadi, ye, ð...</td>\n",
       "      <td>[harrypott, pottermor, favorit]</td>\n",
       "      <td>[hp, cursed, child, book, reservation, already...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>[3rd, bihday, amaz, hilari, nephew, eli, ahmir...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birdsâ, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, to...   \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...   \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...   \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, se...   \n",
       "2  [safe, ways, heal, acne, altwaystoheal, health...   \n",
       "3  [hp, cursed, child, book, reservations, alread...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birdsâ, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...   \n",
       "1  [user, white, supremacist, want, everyon, see,...   \n",
       "2  [safe, way, heal, acn, altwaystoh, healthi, heal]   \n",
       "3  [hp, curs, child, book, reserv, alreadi, ye, ð...   \n",
       "4  [3rd, bihday, amaz, hilari, nephew, eli, ahmir...   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...   \n",
       "1                 [white, supremacist, birdsâ, movi]   \n",
       "2                   [acn, altwaystoh, healthi, heal]   \n",
       "3                    [harrypott, pottermor, favorit]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacist, want, everyone, see...   \n",
       "2  [safe, way, heal, acne, altwaystoheal, healthy...   \n",
       "3  [hp, cursed, child, book, reservation, already...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                 lemmatized_hashtags  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1                [white, supremacist, birdsâ, movie]  \n",
       "2            [acne, altwaystoheal, healthy, healing]  \n",
       "3                [harrypotter, pottermore, favorite]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"lemmatized_tokens\"] = test_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
    "train_data[\"lemmatized_tokens\"] = train_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
    "test_data[\"lemmatized_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
    "train_data[\"lemmatized_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"../../data/220502_test_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
    "train_data.to_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://int.repositories.cloud.sap/artifactory/api/pypi/build-releases-pypi/simple, https://int.repositories.cloud.sap/artifactory/api/pypi/build-milestones-pypi/simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000017342771E50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A0A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A5B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A7C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279AAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279ACA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279AE50>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173427A7040>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "ERROR: Could not find a version that satisfies the requirement texthero (from versions: none)\n",
      "ERROR: No matching distribution found for texthero\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\d073999\\miniconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install texthero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'texthero'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5552/2869611230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtexthero\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../data/220502_train_data_preprocessed.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tfidf_stemmed_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhero\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"stemmed_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'texthero'"
     ]
    }
   ],
   "source": [
    "import texthero as hero\n",
    "\n",
    "train_data = pd.read_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=';')\n",
    "train_data.head()\n",
    "train_data[\"tfidf_stemmed_tokens\"] = (hero.tfidf(train_data[\"stemmed_tokens\"], max_features=8000))\n",
    "train_data[\"tfidf_stemmed_tokens\"] = (hero.tsne(train_data[\"tfidf_stemmed_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tfidf_stemmed_hashtags\"] = (hero.tfidf(train_data[\"stemmed_hashtags\"], max_features=8000))\n",
    "train_data[\"tfidf_stemmed_hashtags\"] = (hero.tsne(train_data[\"tfidf_stemmed_hashtags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tfidf_lemmatized_tokens\"] = (hero.tfidf(train_data[\"lemmatized_tokens\"], max_features=8000))\n",
    "train_data[\"tfidf_lemmatized_tokens\"] = (hero.tsne(train_data[\"tfidf_lemmatized_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.tfidf(train_data[\"lemmatized_hashtags\"], max_features=8000))\n",
    "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.tsne(train_data[\"tfidf_lemmatized_hashtags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>tfidf_stemmed_tokens</th>\n",
       "      <th>tfidf_stemmed_hashtags</th>\n",
       "      <th>tfidf_lemmatized_tokens</th>\n",
       "      <th>tfidf_lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['run']</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>['when', 'father', 'dysfunctional', 'is', 'sel...</td>\n",
       "      <td>['father', 'dysfunctional', 'selfish', 'drags'...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>['father', 'dysfunct', 'selfish', 'drag', 'kid...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>['father', 'dysfunctional', 'selfish', 'drag',...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>[46.213226318359375, 40.86911392211914]</td>\n",
       "      <td>[14.551663398742676, -24.269515991210938]</td>\n",
       "      <td>[-78.26954650878906, 28.642547607421875]</td>\n",
       "      <td>[9.827388763427734, -1.2510194778442383]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>2</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>['user', 'for', 'credit', 'cant', 'cause', 'do...</td>\n",
       "      <td>['user', 'credit', 'cant', 'cause', 'dont', 'w...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>['user', 'credit', 'cant', 'caus', 'dont', 'wh...</td>\n",
       "      <td>['lyft', 'disapoint', 'getthank']</td>\n",
       "      <td>['user', 'credit', 'cant', 'cause', 'dont', 'w...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>[-55.80070495605469, -38.61825942993164]</td>\n",
       "      <td>[2.428388833999634, -6.956750869750977]</td>\n",
       "      <td>[-23.937767028808594, -82.70685577392578]</td>\n",
       "      <td>[-2.348551034927368, -3.923227071762085]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>['your']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-26.814523696899414, -47.92416763305664]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[-16.838232040405273, -55.85787582397461]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['model']</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>['i', 'with', 'u', 'all', 'time', 'urð\\x9f\\x93...</td>\n",
       "      <td>['u', 'time', 'urð\\x9f\\x93±', 'ð\\x9f\\x92¦ð\\x9f...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['u', 'time', 'urð\\x9f\\x93±', 'ð\\x9f\\x92¦ð\\x9f...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['u', 'time', 'urð\\x9f\\x93±', 'ð\\x9f\\x92¦ð\\x9f...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>[18.343618392944336, 52.06433868408203]</td>\n",
       "      <td>[-44.3891716003418, -0.5640219449996948]</td>\n",
       "      <td>[21.397464752197266, -56.25738525390625]</td>\n",
       "      <td>[-2.5464980602264404, 29.163206100463867]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>['society', 'motivation']</td>\n",
       "      <td>['society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>['societi', 'motiv']</td>\n",
       "      <td>['motiv']</td>\n",
       "      <td>['society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>[18.323801040649414, -18.519189834594727]</td>\n",
       "      <td>[-35.41141128540039, 38.93525314331055]</td>\n",
       "      <td>[11.912168502807617, 15.544480323791504]</td>\n",
       "      <td>[22.646465301513672, 20.2937068939209]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>ate user isz that youuuðððððð...</td>\n",
       "      <td>ate user isz that youuuðððððð...</td>\n",
       "      <td>['user', 'that']</td>\n",
       "      <td>['user']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['user']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['user']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[58.34431838989258, 15.119406700134277]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[52.27455520629883, -32.16963577270508]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>0</td>\n",
       "      <td>['shame', 'imwithher']</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>['see', 'turner', 'trying', 'wrap', 'in', 'the...</td>\n",
       "      <td>['see', 'turner', 'trying', 'wrap', 'mantle', ...</td>\n",
       "      <td>['shame', 'imwithher']</td>\n",
       "      <td>['see', 'turner', 'tri', 'wrap', 'mantl', 'her...</td>\n",
       "      <td>['shame', 'imwithh']</td>\n",
       "      <td>['see', 'turner', 'trying', 'wrap', 'mantle', ...</td>\n",
       "      <td>['shame', 'imwithher']</td>\n",
       "      <td>[-76.02751922607422, 11.04662799835205]</td>\n",
       "      <td>[5.823639392852783, -36.403419494628906]</td>\n",
       "      <td>[22.713640213012695, -4.954850196838379]</td>\n",
       "      <td>[9.027178764343262, 5.0646653175354]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>['to', 'songs', 'a', 'morning', 'to', 'is']</td>\n",
       "      <td>['songs', 'morning']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song', 'morn']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song', 'morning']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-5.372385501861572, -38.51631164550781]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[-8.667880058288574, 29.219118118286133]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>1</td>\n",
       "      <td>['sikh', 'temple', 'calgary', 'wso']</td>\n",
       "      <td>user sikh temple vandalised in in calgary wso ...</td>\n",
       "      <td>user sikh temple vandalised in in calgary wso ...</td>\n",
       "      <td>['sikh', 'vandalised', 'in', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'vandalised', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'temple', 'calgary', 'wso']</td>\n",
       "      <td>['sikh', 'vandalis', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'templ', 'calgari', 'wso']</td>\n",
       "      <td>['sikh', 'vandalised', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'temple', 'calgary', 'wso']</td>\n",
       "      <td>[-34.529258728027344, -22.818838119506836]</td>\n",
       "      <td>[-34.69069290161133, -22.710052490234375]</td>\n",
       "      <td>[-23.177236557006836, -36.8807258605957]</td>\n",
       "      <td>[-24.578514099121094, -8.35556411743164]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>thank you user for you follow</td>\n",
       "      <td>thank you user for you follow</td>\n",
       "      <td>['for', 'you', 'follow']</td>\n",
       "      <td>['follow']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['follow']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['follow']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-33.226036071777344, 39.90363693237305]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[16.156993865966797, 14.71973705291748]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  n_mentions  \\\n",
       "0          1   @user when a father is dysfunctional and is s...           1   \n",
       "1          2  @user @user thanks for #lyft credit i can't us...           2   \n",
       "2          3                                bihday your majesty           0   \n",
       "3          4  #model   i love u take with u all the time in ...           0   \n",
       "4          5             factsguide: society now    #motivation           0   \n",
       "...      ...                                                ...         ...   \n",
       "31957  31958  ate @user isz that youuu?ðððððð...           1   \n",
       "31958  31959    to see nina turner on the airwaves trying to...           0   \n",
       "31959  31960  listening to sad songs on a monday morning otw...           0   \n",
       "31960  31961  @user #sikh #temple vandalised in in #calgary,...           1   \n",
       "31961  31962                   thank you @user for you follow             1   \n",
       "\n",
       "                                    hashtags  \\\n",
       "0                                    ['run']   \n",
       "1      ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                         []   \n",
       "3                                  ['model']   \n",
       "4                             ['motivation']   \n",
       "...                                      ...   \n",
       "31957                                     []   \n",
       "31958                 ['shame', 'imwithher']   \n",
       "31959                                     []   \n",
       "31960   ['sikh', 'temple', 'calgary', 'wso']   \n",
       "31961                                     []   \n",
       "\n",
       "                                     without_puctioation  \\\n",
       "0       user when a father is dysfunctional and is so...   \n",
       "1      user user thanks for lyft credit i cant use ca...   \n",
       "2                                    bihday your majesty   \n",
       "3      model   i love u take with u all the time in u...   \n",
       "4                   factsguide society now    motivation   \n",
       "...                                                  ...   \n",
       "31957  ate user isz that youuuðððððð...   \n",
       "31958    to see nina turner on the airwaves trying to...   \n",
       "31959  listening to sad songs on a monday morning otw...   \n",
       "31960  user sikh temple vandalised in in calgary wso ...   \n",
       "31961                    thank you user for you follow     \n",
       "\n",
       "                                             tweet_lower  \\\n",
       "0       user when a father is dysfunctional and is so...   \n",
       "1      user user thanks for lyft credit i cant use ca...   \n",
       "2                                    bihday your majesty   \n",
       "3      model   i love u take with u all the time in u...   \n",
       "4                   factsguide society now    motivation   \n",
       "...                                                  ...   \n",
       "31957  ate user isz that youuuðððððð...   \n",
       "31958    to see nina turner on the airwaves trying to...   \n",
       "31959  listening to sad songs on a monday morning otw...   \n",
       "31960  user sikh temple vandalised in in calgary wso ...   \n",
       "31961                    thank you user for you follow     \n",
       "\n",
       "                                             tweet_token  \\\n",
       "0      ['when', 'father', 'dysfunctional', 'is', 'sel...   \n",
       "1      ['user', 'for', 'credit', 'cant', 'cause', 'do...   \n",
       "2                                               ['your']   \n",
       "3      ['i', 'with', 'u', 'all', 'time', 'urð\\x9f\\x93...   \n",
       "4                              ['society', 'motivation']   \n",
       "...                                                  ...   \n",
       "31957                                   ['user', 'that']   \n",
       "31958  ['see', 'turner', 'trying', 'wrap', 'in', 'the...   \n",
       "31959        ['to', 'songs', 'a', 'morning', 'to', 'is']   \n",
       "31960         ['sikh', 'vandalised', 'in', 'wso', 'act']   \n",
       "31961                           ['for', 'you', 'follow']   \n",
       "\n",
       "                                             clean_token  \\\n",
       "0      ['father', 'dysfunctional', 'selfish', 'drags'...   \n",
       "1      ['user', 'credit', 'cant', 'cause', 'dont', 'w...   \n",
       "2                                                     []   \n",
       "3      ['u', 'time', 'urð\\x9f\\x93±', 'ð\\x9f\\x92¦ð\\x9f...   \n",
       "4                              ['society', 'motivation']   \n",
       "...                                                  ...   \n",
       "31957                                           ['user']   \n",
       "31958  ['see', 'turner', 'trying', 'wrap', 'mantle', ...   \n",
       "31959                               ['songs', 'morning']   \n",
       "31960               ['sikh', 'vandalised', 'wso', 'act']   \n",
       "31961                                         ['follow']   \n",
       "\n",
       "                              clean_hashtags  \\\n",
       "0                                    ['run']   \n",
       "1      ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                         []   \n",
       "3                                  ['model']   \n",
       "4                             ['motivation']   \n",
       "...                                      ...   \n",
       "31957                                     []   \n",
       "31958                 ['shame', 'imwithher']   \n",
       "31959                                     []   \n",
       "31960   ['sikh', 'temple', 'calgary', 'wso']   \n",
       "31961                                     []   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "0      ['father', 'dysfunct', 'selfish', 'drag', 'kid...   \n",
       "1      ['user', 'credit', 'cant', 'caus', 'dont', 'wh...   \n",
       "2                                                     []   \n",
       "3      ['u', 'time', 'urð\\x9f\\x93±', 'ð\\x9f\\x92¦ð\\x9f...   \n",
       "4                                   ['societi', 'motiv']   \n",
       "...                                                  ...   \n",
       "31957                                           ['user']   \n",
       "31958  ['see', 'turner', 'tri', 'wrap', 'mantl', 'her...   \n",
       "31959                                   ['song', 'morn']   \n",
       "31960                 ['sikh', 'vandalis', 'wso', 'act']   \n",
       "31961                                         ['follow']   \n",
       "\n",
       "                          stemmed_hashtags  \\\n",
       "0                                  ['run']   \n",
       "1        ['lyft', 'disapoint', 'getthank']   \n",
       "2                                       []   \n",
       "3                                ['model']   \n",
       "4                                ['motiv']   \n",
       "...                                    ...   \n",
       "31957                                   []   \n",
       "31958                 ['shame', 'imwithh']   \n",
       "31959                                   []   \n",
       "31960  ['sikh', 'templ', 'calgari', 'wso']   \n",
       "31961                                   []   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "0      ['father', 'dysfunctional', 'selfish', 'drag',...   \n",
       "1      ['user', 'credit', 'cant', 'cause', 'dont', 'w...   \n",
       "2                                                     []   \n",
       "3      ['u', 'time', 'urð\\x9f\\x93±', 'ð\\x9f\\x92¦ð\\x9f...   \n",
       "4                              ['society', 'motivation']   \n",
       "...                                                  ...   \n",
       "31957                                           ['user']   \n",
       "31958  ['see', 'turner', 'trying', 'wrap', 'mantle', ...   \n",
       "31959                                ['song', 'morning']   \n",
       "31960               ['sikh', 'vandalised', 'wso', 'act']   \n",
       "31961                                         ['follow']   \n",
       "\n",
       "                         lemmatized_hashtags  \\\n",
       "0                                    ['run']   \n",
       "1      ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                         []   \n",
       "3                                  ['model']   \n",
       "4                             ['motivation']   \n",
       "...                                      ...   \n",
       "31957                                     []   \n",
       "31958                 ['shame', 'imwithher']   \n",
       "31959                                     []   \n",
       "31960   ['sikh', 'temple', 'calgary', 'wso']   \n",
       "31961                                     []   \n",
       "\n",
       "                             tfidf_stemmed_tokens  \\\n",
       "0         [46.213226318359375, 40.86911392211914]   \n",
       "1        [-55.80070495605469, -38.61825942993164]   \n",
       "2       [-26.814523696899414, -47.92416763305664]   \n",
       "3         [18.343618392944336, 52.06433868408203]   \n",
       "4       [18.323801040649414, -18.519189834594727]   \n",
       "...                                           ...   \n",
       "31957     [58.34431838989258, 15.119406700134277]   \n",
       "31958     [-76.02751922607422, 11.04662799835205]   \n",
       "31959    [-5.372385501861572, -38.51631164550781]   \n",
       "31960  [-34.529258728027344, -22.818838119506836]   \n",
       "31961    [-33.226036071777344, 39.90363693237305]   \n",
       "\n",
       "                          tfidf_stemmed_hashtags  \\\n",
       "0      [14.551663398742676, -24.269515991210938]   \n",
       "1        [2.428388833999634, -6.956750869750977]   \n",
       "2      [-18.723024368286133, 16.274951934814453]   \n",
       "3       [-44.3891716003418, -0.5640219449996948]   \n",
       "4        [-35.41141128540039, 38.93525314331055]   \n",
       "...                                          ...   \n",
       "31957  [-18.723024368286133, 16.274951934814453]   \n",
       "31958   [5.823639392852783, -36.403419494628906]   \n",
       "31959  [-18.723024368286133, 16.274951934814453]   \n",
       "31960  [-34.69069290161133, -22.710052490234375]   \n",
       "31961  [-18.723024368286133, 16.274951934814453]   \n",
       "\n",
       "                         tfidf_lemmatized_tokens  \\\n",
       "0       [-78.26954650878906, 28.642547607421875]   \n",
       "1      [-23.937767028808594, -82.70685577392578]   \n",
       "2      [-16.838232040405273, -55.85787582397461]   \n",
       "3       [21.397464752197266, -56.25738525390625]   \n",
       "4       [11.912168502807617, 15.544480323791504]   \n",
       "...                                          ...   \n",
       "31957    [52.27455520629883, -32.16963577270508]   \n",
       "31958   [22.713640213012695, -4.954850196838379]   \n",
       "31959   [-8.667880058288574, 29.219118118286133]   \n",
       "31960   [-23.177236557006836, -36.8807258605957]   \n",
       "31961    [16.156993865966797, 14.71973705291748]   \n",
       "\n",
       "                        tfidf_lemmatized_hashtags  \n",
       "0        [9.827388763427734, -1.2510194778442383]  \n",
       "1        [-2.348551034927368, -3.923227071762085]  \n",
       "2      [-0.12042795121669769, 11.407605171203613]  \n",
       "3       [-2.5464980602264404, 29.163206100463867]  \n",
       "4          [22.646465301513672, 20.2937068939209]  \n",
       "...                                           ...  \n",
       "31957  [-0.12042795121669769, 11.407605171203613]  \n",
       "31958        [9.027178764343262, 5.0646653175354]  \n",
       "31959  [-0.12042795121669769, 11.407605171203613]  \n",
       "31960    [-24.578514099121094, -8.35556411743164]  \n",
       "31961  [-0.12042795121669769, 11.407605171203613]  \n",
       "\n",
       "[31962 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data.loc[:, train_data.columns != \"label\"]\n",
    "Y = train_data.loc[train_data.label]\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, Y, test_size=0.2, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/8f0zskws27x_14w3qmfndjfw0000gn/T/ipykernel_2814/3172956890.py:1: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  X_train[\"label\"] = y_train.label\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb#ch0000051?line=0'>1</a>\u001b[0m X_train[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39mlabel\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb#ch0000051?line=1'>2</a>\u001b[0m X_test[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39mlabel\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb#ch0000051?line=2'>3</a>\u001b[0m X_val[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m y_val\u001b[39m.\u001b[39mlabel\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3651'>3652</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3652'>3653</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3653'>3654</a>\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3654'>3655</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3821'>3822</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3822'>3823</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3823'>3824</a>\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3824'>3825</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3829'>3830</a>\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3830'>3831</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3831'>3832</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3833'>3834</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3834'>3835</a>\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3835'>3836</a>\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3836'>3837</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3837'>3838</a>\u001b[0m     ):\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3838'>3839</a>\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=3839'>3840</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py:4532\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=4529'>4530</a>\u001b[0m \u001b[39m# We should never get here with DataFrame value\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=4530'>4531</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Series):\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=4531'>4532</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=4533'>4534</a>\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=4534'>4535</a>\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py:10999\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10994'>10995</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10995'>10996</a>\u001b[0m     \u001b[39m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10996'>10997</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10997'>10998</a>\u001b[0m         \u001b[39m# duplicate axis\u001b[39;00m\n\u001b[0;32m> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10998'>10999</a>\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=11000'>11001</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=11001'>11002</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mincompatible index of inserted column with frame index\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=11002'>11003</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=11003'>11004</a>\u001b[0m \u001b[39mreturn\u001b[39;00m reindexed_value\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py:10994\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10991'>10992</a>\u001b[0m \u001b[39m# GH#4107\u001b[39;00m\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10992'>10993</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10993'>10994</a>\u001b[0m     reindexed_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mreindex(index)\u001b[39m.\u001b[39m_values\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10994'>10995</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10995'>10996</a>\u001b[0m     \u001b[39m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10996'>10997</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m  <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/frame.py?line=10997'>10998</a>\u001b[0m         \u001b[39m# duplicate axis\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/series.py:4672\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/series.py?line=4667'>4668</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/series.py?line=4668'>4669</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m passed as both positional and keyword argument\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/series.py?line=4669'>4670</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/series.py?line=4670'>4671</a>\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index})\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/series.py?line=4671'>4672</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mreindex(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py:4966\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4962'>4963</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4964'>4965</a>\u001b[0m \u001b[39m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4965'>4966</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reindex_axes(\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4966'>4967</a>\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4967'>4968</a>\u001b[0m )\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py:4986\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4980'>4981</a>\u001b[0m new_index, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mreindex(\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4981'>4982</a>\u001b[0m     labels, level\u001b[39m=\u001b[39mlevel, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance, method\u001b[39m=\u001b[39mmethod\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4982'>4983</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4984'>4985</a>\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(a)\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4985'>4986</a>\u001b[0m obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_reindex_with_indexers(\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4986'>4987</a>\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4987'>4988</a>\u001b[0m     fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4988'>4989</a>\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4989'>4990</a>\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4990'>4991</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4991'>4992</a>\u001b[0m \u001b[39m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=4992'>4993</a>\u001b[0m copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py:5032\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5028'>5029</a>\u001b[0m     indexer \u001b[39m=\u001b[39m ensure_platform_int(indexer)\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5030'>5031</a>\u001b[0m \u001b[39m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5031'>5032</a>\u001b[0m new_data \u001b[39m=\u001b[39m new_data\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5032'>5033</a>\u001b[0m     index,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5033'>5034</a>\u001b[0m     indexer,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5034'>5035</a>\u001b[0m     axis\u001b[39m=\u001b[39;49mbaxis,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5035'>5036</a>\u001b[0m     fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5036'>5037</a>\u001b[0m     allow_dups\u001b[39m=\u001b[39;49mallow_dups,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5037'>5038</a>\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5038'>5039</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5039'>5040</a>\u001b[0m \u001b[39m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/generic.py?line=5040'>5041</a>\u001b[0m copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/internals/managers.py:679\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=676'>677</a>\u001b[0m \u001b[39m# some axes don't allow reindexing with dups\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=677'>678</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_dups:\n\u001b[0;32m--> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=678'>679</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxes[axis]\u001b[39m.\u001b[39;49m_validate_can_reindex(indexer)\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=680'>681</a>\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim:\n\u001b[1;32m    <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=681'>682</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRequested axis not found in manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:4107\u001b[0m, in \u001b[0;36mIndex._validate_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=4104'>4105</a>\u001b[0m \u001b[39m# trying to reindex on an axis with duplicates\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=4105'>4106</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(indexer):\n\u001b[0;32m-> <a href='file:///Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=4106'>4107</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "X_train[\"label\"] = y_train.label\n",
    "X_test[\"label\"] = y_test.label\n",
    "X_val[\"label\"] = y_val.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>tfidf_stemmed_tokens</th>\n",
       "      <th>tfidf_stemmed_hashtags</th>\n",
       "      <th>tfidf_lemmatized_tokens</th>\n",
       "      <th>tfidf_lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24089</th>\n",
       "      <td>24090</td>\n",
       "      <td>0</td>\n",
       "      <td>best #lawofattraction #resources for #healing!...</td>\n",
       "      <td>0</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>best lawofattraction resources for healing    ...</td>\n",
       "      <td>['lawofattraction', 'for', 'altwaystoheal', 'is']</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resources', 'healing', 'a...</td>\n",
       "      <td>['lawofattract', 'altwaystoh']</td>\n",
       "      <td>['lawofattract', 'resourc', 'heal', 'altwaysto...</td>\n",
       "      <td>['lawofattraction', 'altwaystoheal']</td>\n",
       "      <td>['lawofattraction', 'resource', 'healing', 'al...</td>\n",
       "      <td>[-54.797821044921875, 19.326093673706055]</td>\n",
       "      <td>[50.279502868652344, -0.368119478225708]</td>\n",
       "      <td>[30.39127540588379, -49.107627868652344]</td>\n",
       "      <td>[8.619098663330078, -19.02434539794922]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15263</th>\n",
       "      <td>15264</td>\n",
       "      <td>0</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>0</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>remembering to focus on the simplest happy mom...</td>\n",
       "      <td>['to', 'on', 'simplest', 'moments', 'life', 'b...</td>\n",
       "      <td>['simplest', 'moments', 'life', 'blogger', 'li...</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>['simplest', 'moment', 'life', 'blogger', 'life']</td>\n",
       "      <td>['blogger', 'blog', 'life']</td>\n",
       "      <td>[12.769611358642578, 38.40631866455078]</td>\n",
       "      <td>[-0.20247356593608856, -12.449955940246582]</td>\n",
       "      <td>[-25.304161071777344, -11.912176132202148]</td>\n",
       "      <td>[-5.459761619567871, -11.88219928741455]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19309</th>\n",
       "      <td>19310</td>\n",
       "      <td>0</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>when you get as happy as your boyfriend to be ...</td>\n",
       "      <td>['you', 'as', 'as', 'boyfriend', 'be', 'with',...</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>['boyfriend', 'car']</td>\n",
       "      <td>['silvia']</td>\n",
       "      <td>[15.18979263305664, -9.596672058105469]</td>\n",
       "      <td>[10.67345905303955, -4.716570854187012]</td>\n",
       "      <td>[80.47900390625, 3.222534656524658]</td>\n",
       "      <td>[-2.534292459487915, -6.132740020751953]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27243</th>\n",
       "      <td>27244</td>\n",
       "      <td>0</td>\n",
       "      <td>why do you always try to make me happy?  i don...</td>\n",
       "      <td>0</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>why do you always try to make me happy  i dont...</td>\n",
       "      <td>['do', 'always', 'to', 'me', 'i', 'know', 'to'...</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>['alway', 'know', 'love']</td>\n",
       "      <td>['love', 'devot']</td>\n",
       "      <td>['always', 'know', 'love']</td>\n",
       "      <td>['love', 'devotion']</td>\n",
       "      <td>[-46.13848876953125, -18.032955169677734]</td>\n",
       "      <td>[11.31847858428955, -16.98657989501953]</td>\n",
       "      <td>[40.34811019897461, -24.527305603027344]</td>\n",
       "      <td>[8.227179527282715, -13.818502426147461]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>6633</td>\n",
       "      <td>0</td>\n",
       "      <td>omg is finally here!!! #ps4 #farcry4 #gtav #un...</td>\n",
       "      <td>0</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>omg is finally here ps4 farcry4 gtav unchaed4</td>\n",
       "      <td>['is', 'here', 'farcry4', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>['farcry4', 'unchaed4']</td>\n",
       "      <td>['ps4', 'farcry4', 'gtav', 'unchaed4']</td>\n",
       "      <td>[3.866750478744507, -5.09706449508667]</td>\n",
       "      <td>[21.51004409790039, 15.360187530517578]</td>\n",
       "      <td>[8.554491996765137, -20.749971389770508]</td>\n",
       "      <td>[-6.128843307495117, -11.832077980041504]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9039</th>\n",
       "      <td>9040</td>\n",
       "      <td>0</td>\n",
       "      <td>#cat #kitty   select knowledge:</td>\n",
       "      <td>0</td>\n",
       "      <td>['cat', 'kitty']</td>\n",
       "      <td>cat kitty   select knowledge</td>\n",
       "      <td>cat kitty   select knowledge</td>\n",
       "      <td>['kitty', 'knowledge']</td>\n",
       "      <td>['kitty', 'knowledge']</td>\n",
       "      <td>['cat', 'kitty']</td>\n",
       "      <td>['kitti', 'knowledg']</td>\n",
       "      <td>['cat', 'kitti']</td>\n",
       "      <td>['kitty', 'knowledge']</td>\n",
       "      <td>['cat', 'kitty']</td>\n",
       "      <td>[-48.76616668701172, 1.9193642139434814]</td>\n",
       "      <td>[8.789885520935059, 21.543291091918945]</td>\n",
       "      <td>[17.61883544921875, -40.230289459228516]</td>\n",
       "      <td>[3.1132328510284424, -5.80885124206543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10615</th>\n",
       "      <td>10616</td>\n",
       "      <td>0</td>\n",
       "      <td>@user shays first camp @user   #clawosseum #c...</td>\n",
       "      <td>2</td>\n",
       "      <td>['clawosseum', 'clawsout']</td>\n",
       "      <td>user shays first camp user   clawosseum claws...</td>\n",
       "      <td>user shays first camp user   clawosseum claws...</td>\n",
       "      <td>['shays', 'camp', 'clawosseum']</td>\n",
       "      <td>['shays', 'camp', 'clawosseum']</td>\n",
       "      <td>['clawosseum', 'clawsout']</td>\n",
       "      <td>['shay', 'camp', 'clawosseum']</td>\n",
       "      <td>['clawosseum', 'clawsout']</td>\n",
       "      <td>['shay', 'camp', 'clawosseum']</td>\n",
       "      <td>['clawosseum', 'clawsout']</td>\n",
       "      <td>[22.903425216674805, -5.662053108215332]</td>\n",
       "      <td>[-0.957913339138031, 13.914173126220703]</td>\n",
       "      <td>[-13.517252922058105, 6.790152072906494]</td>\n",
       "      <td>[1.1501950025558472, 0.8506528735160828]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12034</th>\n",
       "      <td>12035</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user yes yes yes!</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>user user yes yes yes</td>\n",
       "      <td>user user yes yes yes</td>\n",
       "      <td>['user', 'yes']</td>\n",
       "      <td>['user', 'yes']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['user', 'ye']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['user', 'yes']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-11.248979568481445, 19.911727905273438]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[12.155923843383789, -3.0542140007019043]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>10801</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm so   and #grateful now that - #affirmations</td>\n",
       "      <td>0</td>\n",
       "      <td>['grateful', 'affirmations']</td>\n",
       "      <td>im so   and grateful now that  affirmations</td>\n",
       "      <td>im so   and grateful now that  affirmations</td>\n",
       "      <td>['so', 'grateful', 'that']</td>\n",
       "      <td>['grateful']</td>\n",
       "      <td>['grateful', 'affirmations']</td>\n",
       "      <td>['grate']</td>\n",
       "      <td>['grate', 'affirm']</td>\n",
       "      <td>['grateful']</td>\n",
       "      <td>['grateful', 'affirmation']</td>\n",
       "      <td>[16.90058708190918, -54.99842834472656]</td>\n",
       "      <td>[-27.510984420776367, -36.945865631103516]</td>\n",
       "      <td>[56.24428939819336, 16.980188369750977]</td>\n",
       "      <td>[-10.509261131286621, 5.6819562911987305]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23848</th>\n",
       "      <td>23849</td>\n",
       "      <td>0</td>\n",
       "      <td>cbr at ubc:</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>cbr at ubc</td>\n",
       "      <td>cbr at ubc</td>\n",
       "      <td>['at']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-26.814523696899414, -47.92416763305664]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[-16.838232040405273, -55.85787582397461]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6393 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "24089  24090      0  best #lawofattraction #resources for #healing!...   \n",
       "15263  15264      0  remembering to focus on the simplest happy mom...   \n",
       "19309  19310      0  when you get as happy as your boyfriend to be ...   \n",
       "27243  27244      0  why do you always try to make me happy?  i don...   \n",
       "6632    6633      0  omg is finally here!!! #ps4 #farcry4 #gtav #un...   \n",
       "...      ...    ...                                                ...   \n",
       "9039    9040      0                    #cat #kitty   select knowledge:   \n",
       "10615  10616      0   @user shays first camp @user   #clawosseum #c...   \n",
       "12034  12035      0                         @user @user yes yes yes!     \n",
       "10800  10801      0   i'm so   and #grateful now that - #affirmations    \n",
       "23848  23849      0                                        cbr at ubc:   \n",
       "\n",
       "       n_mentions                                           hashtags  \\\n",
       "24089           0  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "15263           0                        ['blogger', 'blog', 'life']   \n",
       "19309           0                                         ['silvia']   \n",
       "27243           0                               ['love', 'devotion']   \n",
       "6632            0             ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "...           ...                                                ...   \n",
       "9039            0                                   ['cat', 'kitty']   \n",
       "10615           2                         ['clawosseum', 'clawsout']   \n",
       "12034           2                                                 []   \n",
       "10800           0                       ['grateful', 'affirmations']   \n",
       "23848           0                                                 []   \n",
       "\n",
       "                                     without_puctioation  \\\n",
       "24089  best lawofattraction resources for healing    ...   \n",
       "15263  remembering to focus on the simplest happy mom...   \n",
       "19309  when you get as happy as your boyfriend to be ...   \n",
       "27243  why do you always try to make me happy  i dont...   \n",
       "6632    omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "...                                                  ...   \n",
       "9039                        cat kitty   select knowledge   \n",
       "10615   user shays first camp user   clawosseum claws...   \n",
       "12034                            user user yes yes yes     \n",
       "10800       im so   and grateful now that  affirmations    \n",
       "23848                                         cbr at ubc   \n",
       "\n",
       "                                             tweet_lower  \\\n",
       "24089  best lawofattraction resources for healing    ...   \n",
       "15263  remembering to focus on the simplest happy mom...   \n",
       "19309  when you get as happy as your boyfriend to be ...   \n",
       "27243  why do you always try to make me happy  i dont...   \n",
       "6632    omg is finally here ps4 farcry4 gtav unchaed4      \n",
       "...                                                  ...   \n",
       "9039                        cat kitty   select knowledge   \n",
       "10615   user shays first camp user   clawosseum claws...   \n",
       "12034                            user user yes yes yes     \n",
       "10800       im so   and grateful now that  affirmations    \n",
       "23848                                         cbr at ubc   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "24089  ['lawofattraction', 'for', 'altwaystoheal', 'is']   \n",
       "15263  ['to', 'on', 'simplest', 'moments', 'life', 'b...   \n",
       "19309  ['you', 'as', 'as', 'boyfriend', 'be', 'with',...   \n",
       "27243  ['do', 'always', 'to', 'me', 'i', 'know', 'to'...   \n",
       "6632               ['is', 'here', 'farcry4', 'unchaed4']   \n",
       "...                                                  ...   \n",
       "9039                              ['kitty', 'knowledge']   \n",
       "10615                    ['shays', 'camp', 'clawosseum']   \n",
       "12034                                    ['user', 'yes']   \n",
       "10800                         ['so', 'grateful', 'that']   \n",
       "23848                                             ['at']   \n",
       "\n",
       "                                             clean_token  \\\n",
       "24089               ['lawofattraction', 'altwaystoheal']   \n",
       "15263  ['simplest', 'moments', 'life', 'blogger', 'li...   \n",
       "19309                               ['boyfriend', 'car']   \n",
       "27243                         ['always', 'know', 'love']   \n",
       "6632                             ['farcry4', 'unchaed4']   \n",
       "...                                                  ...   \n",
       "9039                              ['kitty', 'knowledge']   \n",
       "10615                    ['shays', 'camp', 'clawosseum']   \n",
       "12034                                    ['user', 'yes']   \n",
       "10800                                       ['grateful']   \n",
       "23848                                                 []   \n",
       "\n",
       "                                          clean_hashtags  \\\n",
       "24089  ['lawofattraction', 'resources', 'healing', 'a...   \n",
       "15263                        ['blogger', 'blog', 'life']   \n",
       "19309                                         ['silvia']   \n",
       "27243                               ['love', 'devotion']   \n",
       "6632              ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "...                                                  ...   \n",
       "9039                                    ['cat', 'kitty']   \n",
       "10615                         ['clawosseum', 'clawsout']   \n",
       "12034                                                 []   \n",
       "10800                       ['grateful', 'affirmations']   \n",
       "23848                                                 []   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "24089                     ['lawofattract', 'altwaystoh']   \n",
       "15263  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "19309                               ['boyfriend', 'car']   \n",
       "27243                          ['alway', 'know', 'love']   \n",
       "6632                             ['farcry4', 'unchaed4']   \n",
       "...                                                  ...   \n",
       "9039                               ['kitti', 'knowledg']   \n",
       "10615                     ['shay', 'camp', 'clawosseum']   \n",
       "12034                                     ['user', 'ye']   \n",
       "10800                                          ['grate']   \n",
       "23848                                                 []   \n",
       "\n",
       "                                        stemmed_hashtags  \\\n",
       "24089  ['lawofattract', 'resourc', 'heal', 'altwaysto...   \n",
       "15263                        ['blogger', 'blog', 'life']   \n",
       "19309                                         ['silvia']   \n",
       "27243                                  ['love', 'devot']   \n",
       "6632              ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "...                                                  ...   \n",
       "9039                                    ['cat', 'kitti']   \n",
       "10615                         ['clawosseum', 'clawsout']   \n",
       "12034                                                 []   \n",
       "10800                                ['grate', 'affirm']   \n",
       "23848                                                 []   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "24089               ['lawofattraction', 'altwaystoheal']   \n",
       "15263  ['simplest', 'moment', 'life', 'blogger', 'life']   \n",
       "19309                               ['boyfriend', 'car']   \n",
       "27243                         ['always', 'know', 'love']   \n",
       "6632                             ['farcry4', 'unchaed4']   \n",
       "...                                                  ...   \n",
       "9039                              ['kitty', 'knowledge']   \n",
       "10615                     ['shay', 'camp', 'clawosseum']   \n",
       "12034                                    ['user', 'yes']   \n",
       "10800                                       ['grateful']   \n",
       "23848                                                 []   \n",
       "\n",
       "                                     lemmatized_hashtags  \\\n",
       "24089  ['lawofattraction', 'resource', 'healing', 'al...   \n",
       "15263                        ['blogger', 'blog', 'life']   \n",
       "19309                                         ['silvia']   \n",
       "27243                               ['love', 'devotion']   \n",
       "6632              ['ps4', 'farcry4', 'gtav', 'unchaed4']   \n",
       "...                                                  ...   \n",
       "9039                                    ['cat', 'kitty']   \n",
       "10615                         ['clawosseum', 'clawsout']   \n",
       "12034                                                 []   \n",
       "10800                        ['grateful', 'affirmation']   \n",
       "23848                                                 []   \n",
       "\n",
       "                            tfidf_stemmed_tokens  \\\n",
       "24089  [-54.797821044921875, 19.326093673706055]   \n",
       "15263    [12.769611358642578, 38.40631866455078]   \n",
       "19309    [15.18979263305664, -9.596672058105469]   \n",
       "27243  [-46.13848876953125, -18.032955169677734]   \n",
       "6632      [3.866750478744507, -5.09706449508667]   \n",
       "...                                          ...   \n",
       "9039    [-48.76616668701172, 1.9193642139434814]   \n",
       "10615   [22.903425216674805, -5.662053108215332]   \n",
       "12034  [-11.248979568481445, 19.911727905273438]   \n",
       "10800    [16.90058708190918, -54.99842834472656]   \n",
       "23848  [-26.814523696899414, -47.92416763305664]   \n",
       "\n",
       "                            tfidf_stemmed_hashtags  \\\n",
       "24089     [50.279502868652344, -0.368119478225708]   \n",
       "15263  [-0.20247356593608856, -12.449955940246582]   \n",
       "19309      [10.67345905303955, -4.716570854187012]   \n",
       "27243      [11.31847858428955, -16.98657989501953]   \n",
       "6632       [21.51004409790039, 15.360187530517578]   \n",
       "...                                            ...   \n",
       "9039       [8.789885520935059, 21.543291091918945]   \n",
       "10615     [-0.957913339138031, 13.914173126220703]   \n",
       "12034    [-18.723024368286133, 16.274951934814453]   \n",
       "10800   [-27.510984420776367, -36.945865631103516]   \n",
       "23848    [-18.723024368286133, 16.274951934814453]   \n",
       "\n",
       "                          tfidf_lemmatized_tokens  \\\n",
       "24089    [30.39127540588379, -49.107627868652344]   \n",
       "15263  [-25.304161071777344, -11.912176132202148]   \n",
       "19309         [80.47900390625, 3.222534656524658]   \n",
       "27243    [40.34811019897461, -24.527305603027344]   \n",
       "6632     [8.554491996765137, -20.749971389770508]   \n",
       "...                                           ...   \n",
       "9039     [17.61883544921875, -40.230289459228516]   \n",
       "10615    [-13.517252922058105, 6.790152072906494]   \n",
       "12034   [12.155923843383789, -3.0542140007019043]   \n",
       "10800     [56.24428939819336, 16.980188369750977]   \n",
       "23848   [-16.838232040405273, -55.85787582397461]   \n",
       "\n",
       "                        tfidf_lemmatized_hashtags  \n",
       "24089     [8.619098663330078, -19.02434539794922]  \n",
       "15263    [-5.459761619567871, -11.88219928741455]  \n",
       "19309    [-2.534292459487915, -6.132740020751953]  \n",
       "27243    [8.227179527282715, -13.818502426147461]  \n",
       "6632    [-6.128843307495117, -11.832077980041504]  \n",
       "...                                           ...  \n",
       "9039      [3.1132328510284424, -5.80885124206543]  \n",
       "10615    [1.1501950025558472, 0.8506528735160828]  \n",
       "12034  [-0.12042795121669769, 11.407605171203613]  \n",
       "10800   [-10.509261131286621, 5.6819562911987305]  \n",
       "23848  [-0.12042795121669769, 11.407605171203613]  \n",
       "\n",
       "[6393 rows x 18 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../../data/220505_test_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
    "X_test.to_csv(\"../../data/220505_train_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
    "X_val.to_csv(\"../../data/220505_validation_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work on emojis \n",
    "Convert emojis to their corresponding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/IPython/core/inputtransformer2.py:595: UserWarning: `make_tokens_by_line` received a list of lines which do not have lineending markers ('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c'), behavior will be unspecified\n",
      "  tokens_by_line = make_tokens_by_line(lines)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#model   i love u take with u all the time in urð\\x9f\\x93±!!! ð\\x9f\\x98\\x99ð\\x9f\\x98\\x8eð\\x9f\\x91\\x84ð\\x9f\\x91\\x85ð\\x9f\\x92¦ð\\x9f\\x92¦ð\\x9f\\x92¦  '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"#model   i love u take with u all the time in urð±!!! ðððð",
    "ð¦ð¦ð¦  \"\n",
    "test = b'#model   i love u take with u all the time in ur\\xc3\\xb0\\xc2\\x9f\\xc2\\x93\\xc2\\xb1!!! \\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x99\\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x8e\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x84\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x85\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6  '\n",
    "\n",
    "test.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/IPython/core/inputtransformer2.py:595: UserWarning: `make_tokens_by_line` received a list of lines which do not have lineending markers ('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c'), behavior will be unspecified\n",
      "  tokens_by_line = make_tokens_by_line(lines)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'emot' has no attribute 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb#ch0000013?line=0'>1</a>\u001b[0m test = \"#model   i love u take with u all the time in urð±!!! ðððð\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb#ch0000013?line=1'>2</a>\u001b[0m ð¦ð¦ð¦  \"\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb#ch0000013?line=2'>3</a>\u001b[0m print(emot.emoji(test))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaronsteiner/Documents/GitHub/Data_mining/src/data/preprocessing.ipynb#ch0000013?line=3'>4</a>\u001b[0m \n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'emot' has no attribute 'emoji'"
     ]
    }
   ],
   "source": [
    "test = \"#model   i love u take with u all the time in urð±!!! ðððð",
    "ð¦ð¦ð¦  \"\n",
    "print(emot.emoji(test))\n",
    "\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947cd5ef72fa4485f7ecf5a654ef12bcb7ac0faec018370a70389fc4010d0179"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
