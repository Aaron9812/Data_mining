{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccesing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../data/train_tweet.csv\")\n",
    "test_data = pd.read_csv(\"../../data/test_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
=======
  "cells": [
>>>>>>> f63486bf3514bae7f29337e27359da41af91e3d9
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq646iXZxVbH",
        "outputId": "16e71bf1-6405-4847-a853-3454f9b1ac41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
<<<<<<< HEAD
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with user mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_user_mentions(text:str) ->int:\n",
    "    return text.count(\"@user\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0\n",
       "1  31964   @user #white #supremacists want everyone to s...           1\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0\n",
       "3  31966  is the hp and the cursed child book up for res...           0\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"n_mentions\"] = test_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
    "train_data[\"n_mentions\"] = train_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_hashtags(text:str) -> list:\n",
    "    pattern = re.compile(r\"#(\\w+)\")\n",
    "    return pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1               [white, supremacists, birds√¢, movie]  \n",
       "2            [acne, altwaystoheal, healthy, healing]  \n",
       "3                [harrypotter, pottermore, favorite]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"hashtags\"] = test_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
    "train_data[\"hashtags\"] = train_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctioation(text:str) -> str:\n",
    "    return \"\".join([i for i in text if i not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birds√¢, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \n",
       "0  studiolife aislife requires passion dedication...  \n",
       "1   user white supremacists want everyone to see ...  \n",
       "2  safe ways to heal your acne    altwaystoheal h...  \n",
       "3  is the hp and the cursed child book up for res...  \n",
       "4    3rd bihday to my amazing hilarious nephew el...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"without_puctioation\"] = test_data[\"tweet\"].apply(lambda x: remove_punctioation(x))\n",
    "train_data[\"without_puctioation\"] = train_data[\"tweet\"].apply(lambda x: remove_punctioation(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[run]</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lyft, disapointed, getthanked]</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[model]</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>[motivation]</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[allshowandnogo]</td>\n",
       "      <td>22 huge fan fare and big talking before they l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>user camping tomorrow user user user user use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.√∞¬ü¬ò...</td>\n",
       "      <td>0</td>\n",
       "      <td>[school, exams, hate, imagine, actorslife, rev...</td>\n",
       "      <td>the next school year is the year for exams√∞¬ü¬ò¬Ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[allin, cavs, champions, cleveland, clevelandc...</td>\n",
       "      <td>we won love the land allin cavs champions clev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>[gr8]</td>\n",
       "      <td>user user welcome here   im   its so gr8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  n_mentions  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...           1   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...           2   \n",
       "2   3      0                                bihday your majesty           0   \n",
       "3   4      0  #model   i love u take with u all the time in ...           0   \n",
       "4   5      0             factsguide: society now    #motivation           0   \n",
       "5   6      0  [2/2] huge fan fare and big talking before the...           0   \n",
       "6   7      0   @user camping tomorrow @user @user @user @use...           8   \n",
       "7   8      0  the next school year is the year for exams.√∞¬ü¬ò...           0   \n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...           0   \n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr...           2   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                                              [run]   \n",
       "1                    [lyft, disapointed, getthanked]   \n",
       "2                                                 []   \n",
       "3                                            [model]   \n",
       "4                                       [motivation]   \n",
       "5                                   [allshowandnogo]   \n",
       "6                                                 []   \n",
       "7  [school, exams, hate, imagine, actorslife, rev...   \n",
       "8  [allin, cavs, champions, cleveland, clevelandc...   \n",
       "9                                              [gr8]   \n",
       "\n",
       "                                 without_puctioation  \n",
       "0   user when a father is dysfunctional and is so...  \n",
       "1  user user thanks for lyft credit i cant use ca...  \n",
       "2                                bihday your majesty  \n",
       "3  model   i love u take with u all the time in u...  \n",
       "4               factsguide society now    motivation  \n",
       "5  22 huge fan fare and big talking before they l...  \n",
       "6   user camping tomorrow user user user user use...  \n",
       "7  the next school year is the year for exams√∞¬ü¬ò¬Ø...  \n",
       "8  we won love the land allin cavs champions clev...  \n",
       "9         user user welcome here   im   its so gr8    "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowering text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[run]</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lyft, disapointed, getthanked]</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[model]</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>[motivation]</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  n_mentions  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...           1   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...           2   \n",
       "2   3      0                                bihday your majesty           0   \n",
       "3   4      0  #model   i love u take with u all the time in ...           0   \n",
       "4   5      0             factsguide: society now    #motivation           0   \n",
       "\n",
       "                          hashtags  \\\n",
       "0                            [run]   \n",
       "1  [lyft, disapointed, getthanked]   \n",
       "2                               []   \n",
       "3                          [model]   \n",
       "4                     [motivation]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0   user when a father is dysfunctional and is so...   \n",
       "1  user user thanks for lyft credit i cant use ca...   \n",
       "2                                bihday your majesty   \n",
       "3  model   i love u take with u all the time in u...   \n",
       "4               factsguide society now    motivation   \n",
       "\n",
       "                                         tweet_lower  \n",
       "0   user when a father is dysfunctional and is so...  \n",
       "1  user user thanks for lyft credit i cant use ca...  \n",
       "2                                bihday your majesty  \n",
       "3  model   i love u take with u all the time in u...  \n",
       "4               factsguide society now    motivation  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"tweet_lower\"] = test_data[\"without_puctioation\"].apply(lambda x: x.lower())\n",
    "train_data[\"tweet_lower\"] = train_data[\"without_puctioation\"].apply(lambda x: x.lower())\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text:str) -> list:\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birds√¢, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1  [user, white, supremacists, want, everyone, to...  \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...  \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...  \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"tweet_token\"] = test_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
    "train_data[\"tweet_token\"] = train_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens) ->list:\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    return [token for token in tokens if token not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, se...</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "      <td>[safe, ways, heal, acne, altwaystoheal, health...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "      <td>[hp, cursed, child, book, reservations, alread...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birds√¢, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, to...   \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...   \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...   \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, se...   \n",
       "2  [safe, ways, heal, acne, altwaystoheal, health...   \n",
       "3  [hp, cursed, child, book, reservations, alread...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                      clean_hashtags  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1               [white, supremacists, birds√¢, movie]  \n",
       "2            [acne, altwaystoheal, healthy, healing]  \n",
       "3                [harrypotter, pottermore, favorite]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"clean_token\"] = test_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data[\"clean_token\"] = train_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
    "test_data[\"clean_hashtags\"] = test_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data[\"clean_hashtags\"] = train_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(text:list) -> list:\n",
    "    return [porter_stemmer.stem(word) for word in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, se...</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "      <td>[user, white, supremacist, want, everyon, see,...</td>\n",
       "      <td>[white, supremacist, birds√¢, movi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "      <td>[safe, ways, heal, acne, altwaystoheal, health...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>[safe, way, heal, acn, altwaystoh, healthi, heal]</td>\n",
       "      <td>[acn, altwaystoh, healthi, heal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "      <td>[hp, cursed, child, book, reservations, alread...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>[hp, curs, child, book, reserv, alreadi, ye, √∞...</td>\n",
       "      <td>[harrypott, pottermor, favorit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>[3rd, bihday, amaz, hilari, nephew, eli, ahmir...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birds√¢, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, to...   \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...   \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...   \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, se...   \n",
       "2  [safe, ways, heal, acne, altwaystoheal, health...   \n",
       "3  [hp, cursed, child, book, reservations, alread...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birds√¢, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...   \n",
       "1  [user, white, supremacist, want, everyon, see,...   \n",
       "2  [safe, way, heal, acn, altwaystoh, healthi, heal]   \n",
       "3  [hp, curs, child, book, reserv, alreadi, ye, √∞...   \n",
       "4  [3rd, bihday, amaz, hilari, nephew, eli, ahmir...   \n",
       "\n",
       "                                    stemmed_hashtags  \n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...  \n",
       "1                 [white, supremacist, birds√¢, movi]  \n",
       "2                   [acn, altwaystoh, healthi, heal]  \n",
       "3                    [harrypott, pottermor, favorit]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"stemmed_tokens\"] = test_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
    "train_data[\"stemmed_tokens\"] = train_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
    "test_data[\"stemmed_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
    "train_data[\"stemmed_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result does not look great (e.g. movie -> movi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\D073999\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizer(text: list) -> list:\n",
    "    return [word_lemmatizer.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>user white supremacists want everyone to see ...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, to...</td>\n",
       "      <td>[user, white, supremacists, want, everyone, se...</td>\n",
       "      <td>[white, supremacists, birds√¢, movie]</td>\n",
       "      <td>[user, white, supremacist, want, everyon, see,...</td>\n",
       "      <td>[white, supremacist, birds√¢, movi]</td>\n",
       "      <td>[user, white, supremacist, want, everyone, see...</td>\n",
       "      <td>[white, supremacist, birds√¢, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>safe ways to heal your acne    altwaystoheal h...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "      <td>[safe, ways, heal, acne, altwaystoheal, health...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "      <td>[safe, way, heal, acn, altwaystoh, healthi, heal]</td>\n",
       "      <td>[acn, altwaystoh, healthi, heal]</td>\n",
       "      <td>[safe, way, heal, acne, altwaystoheal, healthy...</td>\n",
       "      <td>[acne, altwaystoheal, healthy, healing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>0</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "      <td>[hp, cursed, child, book, reservations, alread...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "      <td>[hp, curs, child, book, reserv, alreadi, ye, √∞...</td>\n",
       "      <td>[harrypott, pottermor, favorit]</td>\n",
       "      <td>[hp, cursed, child, book, reservation, already...</td>\n",
       "      <td>[harrypotter, pottermore, favorite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>3rd bihday to my amazing hilarious nephew el...</td>\n",
       "      <td>[3rd, bihday, to, my, amazing, hilarious, neph...</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>[3rd, bihday, amaz, hilari, nephew, eli, ahmir...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "      <td>[3rd, bihday, amazing, hilarious, nephew, eli,...</td>\n",
       "      <td>[bihday, nephew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  n_mentions  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...           0   \n",
       "1  31964   @user #white #supremacists want everyone to s...           1   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...           0   \n",
       "3  31966  is the hp and the cursed child book up for res...           0   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...           0   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birds√¢, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                 without_puctioation  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_lower  \\\n",
       "0  studiolife aislife requires passion dedication...   \n",
       "1   user white supremacists want everyone to see ...   \n",
       "2  safe ways to heal your acne    altwaystoheal h...   \n",
       "3  is the hp and the cursed child book up for res...   \n",
       "4    3rd bihday to my amazing hilarious nephew el...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, to...   \n",
       "2  [safe, ways, to, heal, your, acne, altwaystohe...   \n",
       "3  [is, the, hp, and, the, cursed, child, book, u...   \n",
       "4  [3rd, bihday, to, my, amazing, hilarious, neph...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacists, want, everyone, se...   \n",
       "2  [safe, ways, heal, acne, altwaystoheal, health...   \n",
       "3  [hp, cursed, child, book, reservations, alread...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                      clean_hashtags  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1               [white, supremacists, birds√¢, movie]   \n",
       "2            [acne, altwaystoheal, healthy, healing]   \n",
       "3                [harrypotter, pottermore, favorite]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...   \n",
       "1  [user, white, supremacist, want, everyon, see,...   \n",
       "2  [safe, way, heal, acn, altwaystoh, healthi, heal]   \n",
       "3  [hp, curs, child, book, reserv, alreadi, ye, √∞...   \n",
       "4  [3rd, bihday, amaz, hilari, nephew, eli, ahmir...   \n",
       "\n",
       "                                    stemmed_hashtags  \\\n",
       "0  [studiolif, aislif, requir, passion, dedic, wi...   \n",
       "1                 [white, supremacist, birds√¢, movi]   \n",
       "2                   [acn, altwaystoh, healthi, heal]   \n",
       "3                    [harrypott, pottermor, favorit]   \n",
       "4                                   [bihday, nephew]   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [studiolife, aislife, requires, passion, dedic...   \n",
       "1  [user, white, supremacist, want, everyone, see...   \n",
       "2  [safe, way, heal, acne, altwaystoheal, healthy...   \n",
       "3  [hp, cursed, child, book, reservation, already...   \n",
       "4  [3rd, bihday, amazing, hilarious, nephew, eli,...   \n",
       "\n",
       "                                 lemmatized_hashtags  \n",
       "0  [studiolife, aislife, requires, passion, dedic...  \n",
       "1                [white, supremacist, birds√¢, movie]  \n",
       "2            [acne, altwaystoheal, healthy, healing]  \n",
       "3                [harrypotter, pottermore, favorite]  \n",
       "4                                   [bihday, nephew]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"lemmatized_tokens\"] = test_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
    "train_data[\"lemmatized_tokens\"] = train_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
    "test_data[\"lemmatized_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
    "train_data[\"lemmatized_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"../../data/220502_test_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
    "train_data.to_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://int.repositories.cloud.sap/artifactory/api/pypi/build-releases-pypi/simple, https://int.repositories.cloud.sap/artifactory/api/pypi/build-milestones-pypi/simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000017342771E50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A0A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A5B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-releases-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279A7C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279AAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279ACA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001734279AE50>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000173427A7040>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /artifactory/api/pypi/build-milestones-pypi/simple/texthero/\n",
      "ERROR: Could not find a version that satisfies the requirement texthero (from versions: none)\n",
      "ERROR: No matching distribution found for texthero\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\d073999\\miniconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install texthero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'texthero'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5552/2869611230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtexthero\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../data/220502_train_data_preprocessed.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tfidf_stemmed_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhero\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"stemmed_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'texthero'"
     ]
    }
   ],
   "source": [
    "import texthero as hero\n",
    "\n",
    "train_data = pd.read_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=';')\n",
    "train_data.head()\n",
    "train_data[\"tfidf_stemmed_tokens\"] = (hero.tfidf(train_data[\"stemmed_tokens\"], max_features=8000))\n",
    "train_data[\"tfidf_stemmed_tokens\"] = (hero.tsne(train_data[\"tfidf_stemmed_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tfidf_stemmed_hashtags\"] = (hero.tfidf(train_data[\"stemmed_hashtags\"], max_features=8000))\n",
    "train_data[\"tfidf_stemmed_hashtags\"] = (hero.tsne(train_data[\"tfidf_stemmed_hashtags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tfidf_lemmatized_tokens\"] = (hero.tfidf(train_data[\"lemmatized_tokens\"], max_features=8000))\n",
    "train_data[\"tfidf_lemmatized_tokens\"] = (hero.tsne(train_data[\"tfidf_lemmatized_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.tfidf(train_data[\"lemmatized_hashtags\"], max_features=8000))\n",
    "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.tsne(train_data[\"tfidf_lemmatized_hashtags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>without_puctioation</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_hashtags</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>stemmed_hashtags</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lemmatized_hashtags</th>\n",
       "      <th>tfidf_stemmed_tokens</th>\n",
       "      <th>tfidf_stemmed_hashtags</th>\n",
       "      <th>tfidf_lemmatized_tokens</th>\n",
       "      <th>tfidf_lemmatized_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['run']</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "      <td>['when', 'father', 'dysfunctional', 'is', 'sel...</td>\n",
       "      <td>['father', 'dysfunctional', 'selfish', 'drags'...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>['father', 'dysfunct', 'selfish', 'drag', 'kid...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>['father', 'dysfunctional', 'selfish', 'drag',...</td>\n",
       "      <td>['run']</td>\n",
       "      <td>[46.213226318359375, 40.86911392211914]</td>\n",
       "      <td>[14.551663398742676, -24.269515991210938]</td>\n",
       "      <td>[-78.26954650878906, 28.642547607421875]</td>\n",
       "      <td>[9.827388763427734, -1.2510194778442383]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>2</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "      <td>['user', 'for', 'credit', 'cant', 'cause', 'do...</td>\n",
       "      <td>['user', 'credit', 'cant', 'cause', 'dont', 'w...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>['user', 'credit', 'cant', 'caus', 'dont', 'wh...</td>\n",
       "      <td>['lyft', 'disapoint', 'getthank']</td>\n",
       "      <td>['user', 'credit', 'cant', 'cause', 'dont', 'w...</td>\n",
       "      <td>['lyft', 'disapointed', 'getthanked']</td>\n",
       "      <td>[-55.80070495605469, -38.61825942993164]</td>\n",
       "      <td>[2.428388833999634, -6.956750869750977]</td>\n",
       "      <td>[-23.937767028808594, -82.70685577392578]</td>\n",
       "      <td>[-2.348551034927368, -3.923227071762085]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>['your']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-26.814523696899414, -47.92416763305664]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[-16.838232040405273, -55.85787582397461]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['model']</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "      <td>['i', 'with', 'u', 'all', 'time', 'ur√∞\\x9f\\x93...</td>\n",
       "      <td>['u', 'time', 'ur√∞\\x9f\\x93¬±', '√∞\\x9f\\x92¬¶√∞\\x9f...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['u', 'time', 'ur√∞\\x9f\\x93¬±', '√∞\\x9f\\x92¬¶√∞\\x9f...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['u', 'time', 'ur√∞\\x9f\\x93¬±', '√∞\\x9f\\x92¬¶√∞\\x9f...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>[18.343618392944336, 52.06433868408203]</td>\n",
       "      <td>[-44.3891716003418, -0.5640219449996948]</td>\n",
       "      <td>[21.397464752197266, -56.25738525390625]</td>\n",
       "      <td>[-2.5464980602264404, 29.163206100463867]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "      <td>['society', 'motivation']</td>\n",
       "      <td>['society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>['societi', 'motiv']</td>\n",
       "      <td>['motiv']</td>\n",
       "      <td>['society', 'motivation']</td>\n",
       "      <td>['motivation']</td>\n",
       "      <td>[18.323801040649414, -18.519189834594727]</td>\n",
       "      <td>[-35.41141128540039, 38.93525314331055]</td>\n",
       "      <td>[11.912168502807617, 15.544480323791504]</td>\n",
       "      <td>[22.646465301513672, 20.2937068939209]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>ate @user isz that youuu?√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>ate user isz that youuu√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò...</td>\n",
       "      <td>ate user isz that youuu√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò...</td>\n",
       "      <td>['user', 'that']</td>\n",
       "      <td>['user']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['user']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['user']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[58.34431838989258, 15.119406700134277]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[52.27455520629883, -32.16963577270508]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>0</td>\n",
       "      <td>['shame', 'imwithher']</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>['see', 'turner', 'trying', 'wrap', 'in', 'the...</td>\n",
       "      <td>['see', 'turner', 'trying', 'wrap', 'mantle', ...</td>\n",
       "      <td>['shame', 'imwithher']</td>\n",
       "      <td>['see', 'turner', 'tri', 'wrap', 'mantl', 'her...</td>\n",
       "      <td>['shame', 'imwithh']</td>\n",
       "      <td>['see', 'turner', 'trying', 'wrap', 'mantle', ...</td>\n",
       "      <td>['shame', 'imwithher']</td>\n",
       "      <td>[-76.02751922607422, 11.04662799835205]</td>\n",
       "      <td>[5.823639392852783, -36.403419494628906]</td>\n",
       "      <td>[22.713640213012695, -4.954850196838379]</td>\n",
       "      <td>[9.027178764343262, 5.0646653175354]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>['to', 'songs', 'a', 'morning', 'to', 'is']</td>\n",
       "      <td>['songs', 'morning']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song', 'morn']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song', 'morning']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-5.372385501861572, -38.51631164550781]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[-8.667880058288574, 29.219118118286133]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>1</td>\n",
       "      <td>['sikh', 'temple', 'calgary', 'wso']</td>\n",
       "      <td>user sikh temple vandalised in in calgary wso ...</td>\n",
       "      <td>user sikh temple vandalised in in calgary wso ...</td>\n",
       "      <td>['sikh', 'vandalised', 'in', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'vandalised', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'temple', 'calgary', 'wso']</td>\n",
       "      <td>['sikh', 'vandalis', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'templ', 'calgari', 'wso']</td>\n",
       "      <td>['sikh', 'vandalised', 'wso', 'act']</td>\n",
       "      <td>['sikh', 'temple', 'calgary', 'wso']</td>\n",
       "      <td>[-34.529258728027344, -22.818838119506836]</td>\n",
       "      <td>[-34.69069290161133, -22.710052490234375]</td>\n",
       "      <td>[-23.177236557006836, -36.8807258605957]</td>\n",
       "      <td>[-24.578514099121094, -8.35556411743164]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>thank you user for you follow</td>\n",
       "      <td>thank you user for you follow</td>\n",
       "      <td>['for', 'you', 'follow']</td>\n",
       "      <td>['follow']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['follow']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['follow']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-33.226036071777344, 39.90363693237305]</td>\n",
       "      <td>[-18.723024368286133, 16.274951934814453]</td>\n",
       "      <td>[16.156993865966797, 14.71973705291748]</td>\n",
       "      <td>[-0.12042795121669769, 11.407605171203613]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  n_mentions  \\\n",
       "0          1   @user when a father is dysfunctional and is s...           1   \n",
       "1          2  @user @user thanks for #lyft credit i can't us...           2   \n",
       "2          3                                bihday your majesty           0   \n",
       "3          4  #model   i love u take with u all the time in ...           0   \n",
       "4          5             factsguide: society now    #motivation           0   \n",
       "...      ...                                                ...         ...   \n",
       "31957  31958  ate @user isz that youuu?√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞...           1   \n",
       "31958  31959    to see nina turner on the airwaves trying to...           0   \n",
       "31959  31960  listening to sad songs on a monday morning otw...           0   \n",
       "31960  31961  @user #sikh #temple vandalised in in #calgary,...           1   \n",
       "31961  31962                   thank you @user for you follow             1   \n",
       "\n",
       "                                    hashtags  \\\n",
       "0                                    ['run']   \n",
       "1      ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                         []   \n",
       "3                                  ['model']   \n",
       "4                             ['motivation']   \n",
       "...                                      ...   \n",
       "31957                                     []   \n",
       "31958                 ['shame', 'imwithher']   \n",
       "31959                                     []   \n",
       "31960   ['sikh', 'temple', 'calgary', 'wso']   \n",
       "31961                                     []   \n",
       "\n",
       "                                     without_puctioation  \\\n",
       "0       user when a father is dysfunctional and is so...   \n",
       "1      user user thanks for lyft credit i cant use ca...   \n",
       "2                                    bihday your majesty   \n",
       "3      model   i love u take with u all the time in u...   \n",
       "4                   factsguide society now    motivation   \n",
       "...                                                  ...   \n",
       "31957  ate user isz that youuu√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò...   \n",
       "31958    to see nina turner on the airwaves trying to...   \n",
       "31959  listening to sad songs on a monday morning otw...   \n",
       "31960  user sikh temple vandalised in in calgary wso ...   \n",
       "31961                    thank you user for you follow     \n",
       "\n",
       "                                             tweet_lower  \\\n",
       "0       user when a father is dysfunctional and is so...   \n",
       "1      user user thanks for lyft credit i cant use ca...   \n",
       "2                                    bihday your majesty   \n",
       "3      model   i love u take with u all the time in u...   \n",
       "4                   factsguide society now    motivation   \n",
       "...                                                  ...   \n",
       "31957  ate user isz that youuu√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò...   \n",
       "31958    to see nina turner on the airwaves trying to...   \n",
       "31959  listening to sad songs on a monday morning otw...   \n",
       "31960  user sikh temple vandalised in in calgary wso ...   \n",
       "31961                    thank you user for you follow     \n",
       "\n",
       "                                             tweet_token  \\\n",
       "0      ['when', 'father', 'dysfunctional', 'is', 'sel...   \n",
       "1      ['user', 'for', 'credit', 'cant', 'cause', 'do...   \n",
       "2                                               ['your']   \n",
       "3      ['i', 'with', 'u', 'all', 'time', 'ur√∞\\x9f\\x93...   \n",
       "4                              ['society', 'motivation']   \n",
       "...                                                  ...   \n",
       "31957                                   ['user', 'that']   \n",
       "31958  ['see', 'turner', 'trying', 'wrap', 'in', 'the...   \n",
       "31959        ['to', 'songs', 'a', 'morning', 'to', 'is']   \n",
       "31960         ['sikh', 'vandalised', 'in', 'wso', 'act']   \n",
       "31961                           ['for', 'you', 'follow']   \n",
       "\n",
       "                                             clean_token  \\\n",
       "0      ['father', 'dysfunctional', 'selfish', 'drags'...   \n",
       "1      ['user', 'credit', 'cant', 'cause', 'dont', 'w...   \n",
       "2                                                     []   \n",
       "3      ['u', 'time', 'ur√∞\\x9f\\x93¬±', '√∞\\x9f\\x92¬¶√∞\\x9f...   \n",
       "4                              ['society', 'motivation']   \n",
       "...                                                  ...   \n",
       "31957                                           ['user']   \n",
       "31958  ['see', 'turner', 'trying', 'wrap', 'mantle', ...   \n",
       "31959                               ['songs', 'morning']   \n",
       "31960               ['sikh', 'vandalised', 'wso', 'act']   \n",
       "31961                                         ['follow']   \n",
       "\n",
       "                              clean_hashtags  \\\n",
       "0                                    ['run']   \n",
       "1      ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                         []   \n",
       "3                                  ['model']   \n",
       "4                             ['motivation']   \n",
       "...                                      ...   \n",
       "31957                                     []   \n",
       "31958                 ['shame', 'imwithher']   \n",
       "31959                                     []   \n",
       "31960   ['sikh', 'temple', 'calgary', 'wso']   \n",
       "31961                                     []   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "0      ['father', 'dysfunct', 'selfish', 'drag', 'kid...   \n",
       "1      ['user', 'credit', 'cant', 'caus', 'dont', 'wh...   \n",
       "2                                                     []   \n",
       "3      ['u', 'time', 'ur√∞\\x9f\\x93¬±', '√∞\\x9f\\x92¬¶√∞\\x9f...   \n",
       "4                                   ['societi', 'motiv']   \n",
       "...                                                  ...   \n",
       "31957                                           ['user']   \n",
       "31958  ['see', 'turner', 'tri', 'wrap', 'mantl', 'her...   \n",
       "31959                                   ['song', 'morn']   \n",
       "31960                 ['sikh', 'vandalis', 'wso', 'act']   \n",
       "31961                                         ['follow']   \n",
       "\n",
       "                          stemmed_hashtags  \\\n",
       "0                                  ['run']   \n",
       "1        ['lyft', 'disapoint', 'getthank']   \n",
       "2                                       []   \n",
       "3                                ['model']   \n",
       "4                                ['motiv']   \n",
       "...                                    ...   \n",
       "31957                                   []   \n",
       "31958                 ['shame', 'imwithh']   \n",
       "31959                                   []   \n",
       "31960  ['sikh', 'templ', 'calgari', 'wso']   \n",
       "31961                                   []   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "0      ['father', 'dysfunctional', 'selfish', 'drag',...   \n",
       "1      ['user', 'credit', 'cant', 'cause', 'dont', 'w...   \n",
       "2                                                     []   \n",
       "3      ['u', 'time', 'ur√∞\\x9f\\x93¬±', '√∞\\x9f\\x92¬¶√∞\\x9f...   \n",
       "4                              ['society', 'motivation']   \n",
       "...                                                  ...   \n",
       "31957                                           ['user']   \n",
       "31958  ['see', 'turner', 'trying', 'wrap', 'mantle', ...   \n",
       "31959                                ['song', 'morning']   \n",
       "31960               ['sikh', 'vandalised', 'wso', 'act']   \n",
       "31961                                         ['follow']   \n",
       "\n",
       "                         lemmatized_hashtags  \\\n",
       "0                                    ['run']   \n",
       "1      ['lyft', 'disapointed', 'getthanked']   \n",
       "2                                         []   \n",
       "3                                  ['model']   \n",
       "4                             ['motivation']   \n",
       "...                                      ...   \n",
       "31957                                     []   \n",
       "31958                 ['shame', 'imwithher']   \n",
       "31959                                     []   \n",
       "31960   ['sikh', 'temple', 'calgary', 'wso']   \n",
       "31961                                     []   \n",
       "\n",
       "                             tfidf_stemmed_tokens  \\\n",
       "0         [46.213226318359375, 40.86911392211914]   \n",
       "1        [-55.80070495605469, -38.61825942993164]   \n",
       "2       [-26.814523696899414, -47.92416763305664]   \n",
       "3         [18.343618392944336, 52.06433868408203]   \n",
       "4       [18.323801040649414, -18.519189834594727]   \n",
       "...                                           ...   \n",
       "31957     [58.34431838989258, 15.119406700134277]   \n",
       "31958     [-76.02751922607422, 11.04662799835205]   \n",
       "31959    [-5.372385501861572, -38.51631164550781]   \n",
       "31960  [-34.529258728027344, -22.818838119506836]   \n",
       "31961    [-33.226036071777344, 39.90363693237305]   \n",
       "\n",
       "                          tfidf_stemmed_hashtags  \\\n",
       "0      [14.551663398742676, -24.269515991210938]   \n",
       "1        [2.428388833999634, -6.956750869750977]   \n",
       "2      [-18.723024368286133, 16.274951934814453]   \n",
       "3       [-44.3891716003418, -0.5640219449996948]   \n",
       "4        [-35.41141128540039, 38.93525314331055]   \n",
       "...                                          ...   \n",
       "31957  [-18.723024368286133, 16.274951934814453]   \n",
       "31958   [5.823639392852783, -36.403419494628906]   \n",
       "31959  [-18.723024368286133, 16.274951934814453]   \n",
       "31960  [-34.69069290161133, -22.710052490234375]   \n",
       "31961  [-18.723024368286133, 16.274951934814453]   \n",
       "\n",
       "                         tfidf_lemmatized_tokens  \\\n",
       "0       [-78.26954650878906, 28.642547607421875]   \n",
       "1      [-23.937767028808594, -82.70685577392578]   \n",
       "2      [-16.838232040405273, -55.85787582397461]   \n",
       "3       [21.397464752197266, -56.25738525390625]   \n",
       "4       [11.912168502807617, 15.544480323791504]   \n",
       "...                                          ...   \n",
       "31957    [52.27455520629883, -32.16963577270508]   \n",
       "31958   [22.713640213012695, -4.954850196838379]   \n",
       "31959   [-8.667880058288574, 29.219118118286133]   \n",
       "31960   [-23.177236557006836, -36.8807258605957]   \n",
       "31961    [16.156993865966797, 14.71973705291748]   \n",
       "\n",
       "                        tfidf_lemmatized_hashtags  \n",
       "0        [9.827388763427734, -1.2510194778442383]  \n",
       "1        [-2.348551034927368, -3.923227071762085]  \n",
       "2      [-0.12042795121669769, 11.407605171203613]  \n",
       "3       [-2.5464980602264404, 29.163206100463867]  \n",
       "4          [22.646465301513672, 20.2937068939209]  \n",
       "...                                           ...  \n",
       "31957  [-0.12042795121669769, 11.407605171203613]  \n",
       "31958        [9.027178764343262, 5.0646653175354]  \n",
       "31959  [-0.12042795121669769, 11.407605171203613]  \n",
       "31960    [-24.578514099121094, -8.35556411743164]  \n",
       "31961  [-0.12042795121669769, 11.407605171203613]  \n",
       "\n",
       "[31962 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data.loc[:, train_data.columns != \"label\"]\n",
    "Y = train_data.loc[train_data.label]\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, Y, test_size=0.2, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/8f0zskws27x_14w3qmfndjfw0000gn/T/ipykernel_2814/3172956890.py:1: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  X_train[\"label\"] = y_train.label\n"
     ]
=======
>>>>>>> f63486bf3514bae7f29337e27359da41af91e3d9
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Wsk40Gx35D",
        "outputId": "3b61d87b-6a88-4ec7-faa3-7035fe043253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DataMining/Data_mining/src/data\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/DataMining/Data_mining/src/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHAr19h1xQgn"
      },
      "source": [
        "# Preproccesing the data"
      ]
<<<<<<< HEAD
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"#model   i love u take with u all the time in ur√∞¬ü¬ì¬±!!! √∞¬ü¬ò¬ô√∞¬ü¬ò¬é√∞¬ü¬ë¬Ñ√∞¬ü¬ë¬Ö",
    "√∞¬ü¬í¬¶√∞¬ü¬í¬¶√∞¬ü¬í¬¶  \"\n",
    "test = b'#model   i love u take with u all the time in ur\\xc3\\xb0\\xc2\\x9f\\xc2\\x93\\xc2\\xb1!!! \\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x99\\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x8e\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x84\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x85\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6  '\n",
    "\n",
    "test.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronsteiner/Documents/GitHub/Data_mining/env/lib/python3.9/site-packages/IPython/core/inputtransformer2.py:595: UserWarning: `make_tokens_by_line` received a list of lines which do not have lineending markers ('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c'), behavior will be unspecified\n",
      "  tokens_by_line = make_tokens_by_line(lines)\n"
     ]
=======
>>>>>>> f63486bf3514bae7f29337e27359da41af91e3d9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6trqFrZixQg1"
      },
      "source": [
        "## Importing needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9x1R9W9xQg3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDSIpni2xQg9"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVg3ddLpxQg-"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"../../data/train_tweet.csv\")\n",
        "test_data = pd.read_csv(\"../../data/test_tweets.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3jgrWtqxQhB"
      },
      "source": [
        "Take a first look at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "982-_-dQxQhD"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsMipIvRxQhH"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3fp1U0AxQhJ"
      },
      "source": [
        "## Deal with user mentions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSp0NV4txQhM"
      },
      "outputs": [],
      "source": [
        "def count_user_mentions(text:str) ->int:\n",
        "    return text.count(\"@user\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iUWPC51xQhP"
      },
      "outputs": [],
      "source": [
        "test_data[\"n_mentions\"] = test_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
        "train_data[\"n_mentions\"] = train_data[\"tweet\"].apply(lambda x: count_user_mentions(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD0cibTixQhR"
      },
      "source": [
        "## Deal with hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfD7xYZDxQhT"
      },
      "outputs": [],
      "source": [
        "def identify_hashtags(text:str) -> list:\n",
        "    pattern = re.compile(r\"#(\\w+)\")\n",
        "    return pattern.findall(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbu8dHAgxQhV"
      },
      "outputs": [],
      "source": [
        "test_data[\"hashtags\"] = test_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
        "train_data[\"hashtags\"] = train_data[\"tweet\"].apply(lambda x: identify_hashtags(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPaOhm2lxQhX"
      },
      "source": [
        "## Punctuation Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfZQIjwdxQhY"
      },
      "source": [
        "Create helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFYglYuQxQhZ"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text:str) -> str:\n",
        "    return \"\".join([i for i in text if i not in punctuation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D3dA36-xQha"
      },
      "outputs": [],
      "source": [
        "test_data[\"without_punctuation\"] = test_data[\"tweet\"].apply(lambda x: remove_punctuation(x))\n",
        "train_data[\"without_punctuation\"] = train_data[\"tweet\"].apply(lambda x: remove_punctuation(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeV3Ir5ExQhc"
      },
      "outputs": [],
      "source": [
        "train_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgDDGxTCxQhd"
      },
      "source": [
        "## Lowering text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sASiJLEOxQhf"
      },
      "outputs": [],
      "source": [
        "test_data[\"tweet_lower\"] = test_data[\"without_punctuation\"].apply(lambda x: x.lower())\n",
        "train_data[\"tweet_lower\"] = train_data[\"without_punctuation\"].apply(lambda x: x.lower())\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tx4IBjzxQhh"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVg49DvUzuhY"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu11J8NWxQhh"
      },
      "outputs": [],
      "source": [
        "def tokenization(text:str) -> list:\n",
        "    return nltk.word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9ptoOj7xQhj"
      },
      "outputs": [],
      "source": [
        "test_data[\"tweet_token\"] = test_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
        "train_data[\"tweet_token\"] = train_data[\"tweet_lower\"].apply(lambda x: tokenization(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSC4DCYDxQhk"
      },
      "source": [
        "## Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i91SIAplz9JQ"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcRjbmKdxQhl"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens) ->list:\n",
        "    stopwords_list = stopwords.words(\"english\")\n",
        "    return [token for token in tokens if token not in stopwords_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckw3hwngxQhm"
      },
      "outputs": [],
      "source": [
        "test_data[\"clean_token\"] = test_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
        "train_data[\"clean_token\"] = train_data[\"tweet_token\"].apply(lambda x: remove_stopwords(x))\n",
        "test_data[\"clean_hashtags\"] = test_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
        "train_data[\"clean_hashtags\"] = train_data[\"hashtags\"].apply(lambda x: remove_stopwords(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKW9vlXKxQho"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK6vz_J2xQhp"
      },
      "outputs": [],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(text:list) -> list:\n",
        "    return [porter_stemmer.stem(word) for word in text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdU25XTMxQhq"
      },
      "outputs": [],
      "source": [
        "test_data[\"stemmed_tokens\"] = test_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
        "train_data[\"stemmed_tokens\"] = train_data[\"clean_token\"].apply(lambda x: stemming(x))\n",
        "test_data[\"stemmed_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
        "train_data[\"stemmed_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: stemming(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGOW_qTqxQhs"
      },
      "source": [
        "Result does not look great (e.g. movie -> movi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT29MhFuxQht"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTj48kNd0Or9"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbYBgb2ExQht"
      },
      "outputs": [],
      "source": [
        "word_lemmatizer = WordNetLemmatizer()\n",
        "def lemmatizer(text: list) -> list:\n",
        "    return [word_lemmatizer.lemmatize(word) for word in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbKAW17l1GXG"
      },
      "outputs": [],
      "source": [
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5caf3YwxQhu"
      },
      "outputs": [],
      "source": [
        "test_data[\"lemmatized_tokens\"] = test_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
        "train_data[\"lemmatized_tokens\"] = train_data[\"clean_token\"].apply(lambda x: lemmatizer(x))\n",
        "test_data[\"lemmatized_hashtags\"] = test_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
        "train_data[\"lemmatized_hashtags\"] = train_data[\"clean_hashtags\"].apply(lambda x: lemmatizer(x))\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n77DbcoxQhw"
      },
      "source": [
        "## Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1-NGbkmxQhw"
      },
      "outputs": [],
      "source": [
        "test_data.to_csv(\"../../data/220502_test_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
        "train_data.to_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBHdSxnVxQhy"
      },
      "source": [
        "## Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HQKvjv9V0cpE",
        "outputId": "49682c53-d937-4690-87c2-d57652df607a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting texthero\n",
            "  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 235 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting nltk>=3.3\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.5 MB 21.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.0.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n",
            "Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.64.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.0->texthero) (4.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 749 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2022.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.1.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.9.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n",
            "Installing collected packages: regex, unidecode, nltk, texthero\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.7 regex-2022.4.24 texthero-1.1.0 unidecode-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install texthero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gANrTs-xxQhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82681452-252f-4b12-daf9-5088d87d79cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import texthero as hero\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"../../data/220502_train_data_preprocessed.csv\", sep=';')\n",
        "train_data[\"tfidf_stemmed_tokens\"] = (hero.tfidf(train_data[\"stemmed_tokens\"], max_features=8000))\n",
        "train_data[\"tfidf_stemmed_tokens\"] = (hero.pca(train_data[\"tfidf_stemmed_tokens\"], n_components=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BuV0sAAxQh0"
      },
      "outputs": [],
      "source": [
        "train_data[\"tfidf_stemmed_hashtags\"] = (hero.tfidf(train_data[\"stemmed_hashtags\"], max_features=8000))\n",
        "train_data[\"tfidf_stemmed_hashtags\"] = (hero.pca(train_data[\"tfidf_stemmed_hashtags\"], n_components=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4OJX1fMxQh0"
      },
      "outputs": [],
      "source": [
        "train_data[\"tfidf_lemmatized_tokens\"] = (hero.tfidf(train_data[\"lemmatized_tokens\"], max_features=8000))\n",
        "train_data[\"tfidf_lemmatized_tokens\"] = (hero.pca(train_data[\"tfidf_lemmatized_tokens\"], n_components=500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv3CTQNmxQh1"
      },
      "outputs": [],
      "source": [
        "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.tfidf(train_data[\"lemmatized_hashtags\"], max_features=8000))\n",
        "train_data[\"tfidf_lemmatized_hashtags\"] = (hero.pca(train_data[\"tfidf_lemmatized_hashtags\"], n_components=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MwAXtKqxQh3"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFkG0AdjxQh4"
      },
      "outputs": [],
      "source": [
        "X = train_data.loc[:, train_data.columns != \"label\"]\n",
        "Y = train_data.loc[train_data.label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw6vYXgzxQh5"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_data, Y, test_size=0.2, random_state=55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNwBcQqoxQh6"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PRdzEerxQh8"
      },
      "outputs": [],
      "source": [
        "X_test.to_csv(\"../../data/220505_test_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
        "X_train.to_csv(\"../../data/220505_train_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
        "X_val.to_csv(\"../../data/220505_validation_data_preprocessed.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVSJHA6AxQh9"
      },
      "source": [
        "# Work in progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYWUXpT1xQh-"
      },
      "source": [
        "## Work on emojis \n",
        "Convert emojis to their corresponding text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMkxb8h0xQh-"
      },
      "outputs": [],
      "source": [
        "text = \"#model   i love u take with u all the time in ur√∞¬ü¬ì¬±!!! √∞¬ü¬ò¬ô√∞¬ü¬ò¬é√∞¬ü¬ë¬Ñ√∞¬ü¬ë¬Ö√∞¬ü¬í¬¶√∞¬ü¬í¬¶√∞¬ü¬í¬¶  \"\n",
        "test = b'#model   i love u take with u all the time in ur\\xc3\\xb0\\xc2\\x9f\\xc2\\x93\\xc2\\xb1!!! \\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x99\\xc3\\xb0\\xc2\\x9f\\xc2\\x98\\xc2\\x8e\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x84\\xc3\\xb0\\xc2\\x9f\\xc2\\x91\\xc2\\x85\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6\\xc3\\xb0\\xc2\\x9f\\xc2\\x92\\xc2\\xa6  '\n",
        "\n",
        "test.decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7Ug2eCHxQiA"
      },
      "outputs": [],
      "source": [
        "test = \"#model   i love u take with u all the time in ur√∞¬ü¬ì¬±!!! √∞¬ü¬ò¬ô√∞¬ü¬ò¬é√∞¬ü¬ë¬Ñ√∞¬ü¬ë¬Ö√∞¬ü¬í¬¶√∞¬ü¬í¬¶√∞¬ü¬í¬¶  \"\n",
        "print(emot.emoji(test))\n",
        "\n",
        "print(test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kgDDGxTCxQhd",
        "9Tx4IBjzxQhh",
        "DSC4DCYDxQhk",
        "JKW9vlXKxQho",
        "tT29MhFuxQht",
        "5n77DbcoxQhw",
        "SBHdSxnVxQhy",
        "-MwAXtKqxQh3",
        "VYWUXpT1xQh-"
      ],
      "machine_shape": "hm",
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "947cd5ef72fa4485f7ecf5a654ef12bcb7ac0faec018370a70389fc4010d0179"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
<<<<<<< HEAD
   ],
   "source": [
    "test = \"#model   i love u take with u all the time in ur√∞¬ü¬ì¬±!!! √∞¬ü¬ò¬ô√∞¬ü¬ò¬é√∞¬ü¬ë¬Ñ√∞¬ü¬ë¬Ö",
    "√∞¬ü¬í¬¶√∞¬ü¬í¬¶√∞¬ü¬í¬¶  \"\n",
    "print(emot.emoji(test))\n",
    "\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947cd5ef72fa4485f7ecf5a654ef12bcb7ac0faec018370a70389fc4010d0179"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> f63486bf3514bae7f29337e27359da41af91e3d9
